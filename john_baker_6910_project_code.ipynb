{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Research Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Educational researchers and data scientists face significant challenges in identifying struggling students early enough to provide timely interventions in online learning environments. While there are extensive data available, extracting meaningful predictive signals from student interactions remains difficult, particularly within the first portion of assignments when intervention would be most valuable. [The ASSISTments dataset](https://osf.io/59shv/files/osfstorage) provides a unique opportunity to address this challenge through its comprehensive data from 88 distinct assignment-level randomized controlled experiments conducted within [the ASSISTments platform](https://www.assistments.org/). This collection, analyzed initially in [Prihar et al.'s 2022 paper *Exploring Common Trends in Online Educational Experiments*](https://osf.io/f58dz), includes detailed clickstream data that captures temporal aspects of student engagement across diverse educational interventions. The rich multi-level student interaction data enables the development and evaluation of early warning systems that could identify struggling students before they fall significantly behind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Research Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How can temporal engagement features derived from clickstream data in the ASSISTments experimental dataset predict student performance drops across different intervention types, and which feature selection methods most effectively identify at-risk students within the first 25% of an assignment? This research will leverage the dataset's granular student interaction logs to extract time-based engagement patterns, analyze how these patterns correlate with performance outcomes, and determine which combinations of features provide the earliest reliable signals of academic struggle across different intervention conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ9f09tc34r0"
      },
      "source": [
        "# Load, Merge, and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polars version: 1.29.0\n",
            "Base data path: /Users/john/Downloads/osfstorage-archive\n",
            "Experiment IDs path: /Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23\n",
            "Global String Cache enabled: True\n"
          ]
        }
      ],
      "source": [
        "# Setup and Configuration\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "import gc\n",
        "\n",
        "# --- Enable Global String Cache for Categoricals ---\n",
        "pl.enable_string_cache()\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_DATA_PATH = Path('/Users/john/Downloads/osfstorage-archive')\n",
        "EXPERIMENT_IDS_PATH = BASE_DATA_PATH / 'experiment_dataset_2021-09-23'\n",
        "\n",
        "# Output path for the cleaned data\n",
        "SAVE_CLEANED_PATH_POLARS_PARQUET = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.parquet'\n",
        "SAVE_CLEANED_PATH_POLARS_CSV = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.csv'\n",
        "\n",
        "print(f\"Polars version: {pl.__version__}\")\n",
        "print(f\"Base data path: {BASE_DATA_PATH}\")\n",
        "print(f\"Experiment IDs path: {EXPERIMENT_IDS_PATH}\")\n",
        "print(f\"Global String Cache enabled: {pl.using_string_cache()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 88 experiment ID directories.\n",
            "Sample performance file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_alogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_alogs.csv']\n",
            "Sample problems file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_plogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_plogs.csv']\n",
            "Sample actions file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_slogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_slogs.csv']\n",
            "Sample metrics file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/priors.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/priors.csv']\n"
          ]
        }
      ],
      "source": [
        "# Generate File Paths\n",
        "try:\n",
        "    if not EXPERIMENT_IDS_PATH.is_dir():\n",
        "        raise FileNotFoundError(f\"Error: Directory not found at {EXPERIMENT_IDS_PATH}\")\n",
        "    experiment_ids = [d.name for d in EXPERIMENT_IDS_PATH.iterdir() if d.is_dir()]\n",
        "    print(f\"Found {len(experiment_ids)} experiment ID directories.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    experiment_ids = []\n",
        "\n",
        "performance_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_alogs.csv') for exp_id in experiment_ids]\n",
        "problems_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_plogs.csv') for exp_id in experiment_ids]\n",
        "actions_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_slogs.csv') for exp_id in experiment_ids]\n",
        "metrics_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'priors.csv') for exp_id in experiment_ids]\n",
        "\n",
        "print(\"Sample performance file paths:\", performance_file_paths[:2])\n",
        "print(\"Sample problems file paths:\", problems_file_paths[:2])\n",
        "print(\"Sample actions file paths:\", actions_file_paths[:2])\n",
        "print(\"Sample metrics file paths:\", metrics_file_paths[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Schemas and Date Parsing Information\n",
        "\n",
        "actions_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'problem_id': pl.Categorical,\n",
        "    'problem_part': pl.Categorical, \n",
        "    'scaffold_id': pl.Categorical,  \n",
        "    'experiment_tag_path': pl.Utf8,\n",
        "    'action': pl.Categorical,\n",
        "    'timestamp': pl.Utf8,\n",
        "    'assistments_reference_action_log_id': pl.UInt64\n",
        "}\n",
        "actions_parse_dates = ['timestamp']\n",
        "\n",
        "problems_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'problem_id': pl.Categorical,\n",
        "    'problem_part': pl.Categorical, \n",
        "    'scaffold_id': pl.Categorical,  \n",
        "    'problem_condition': pl.Categorical,\n",
        "    'start_time': pl.Utf8,\n",
        "    'end_time': pl.Utf8,\n",
        "    'session_count': pl.UInt16,\n",
        "    'time_on_task': pl.Float32,\n",
        "    'first_response_or_request_time': pl.Float32,\n",
        "    'first_answer': pl.Utf8,\n",
        "    'correct': pl.Boolean,\n",
        "    'reported_score': pl.Float32,\n",
        "    'answer_before_tutoring': pl.Boolean,\n",
        "    'attempt_count': pl.UInt16,\n",
        "    'hints_available': pl.UInt16,\n",
        "    'hints_given': pl.UInt16,\n",
        "    'scaffold_problems_available': pl.UInt16,\n",
        "    'scaffold_problems_given': pl.UInt16,\n",
        "    'explanation_available': pl.Boolean,\n",
        "    'explanation_given': pl.Boolean,\n",
        "    'answer_given': pl.Boolean,\n",
        "    'assistments_reference_problem_log_id': pl.UInt64\n",
        "}\n",
        "problems_parse_dates = ['start_time', 'end_time']\n",
        "\n",
        "performance_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'release_date': pl.Utf8,\n",
        "    'due_date': pl.Utf8,\n",
        "    'start_time': pl.Utf8,\n",
        "    'end_time': pl.Utf8,\n",
        "    'assignment_session_count': pl.Float32,\n",
        "    'pretest_problem_count': pl.Float32,\n",
        "    'pretest_correct': pl.Float32,\n",
        "    'pretest_time_on_task': pl.Float32,\n",
        "    'pretest_average_first_response_time': pl.Float32,\n",
        "    'pretest_session_count': pl.Float32,\n",
        "    'assigned_condition': pl.Categorical,\n",
        "    'condition_time_on_task': pl.Float32,\n",
        "    'condition_average_first_response_or_request_time': pl.Float32,\n",
        "    'condition_problem_count': pl.Float32,\n",
        "    'condition_total_correct': pl.Float32,\n",
        "    'condition_total_correct_after_wrong_response': pl.Float32,\n",
        "    'condition_total_correct_after_tutoring': pl.Float32,\n",
        "    'condition_total_answers_before_tutoring': pl.Float32,\n",
        "    'condition_total_attempt_count': pl.Float32,\n",
        "    'condition_total_hints_available': pl.Float32,\n",
        "    'condition_total_hints_given': pl.Float32,\n",
        "    'condition_total_scaffold_problems_available': pl.Float32,\n",
        "    'condition_total_scaffold_problems_given': pl.Float32,\n",
        "    'condition_total_explanations_available': pl.Float32,\n",
        "    'condition_total_explanations_given': pl.Float32,\n",
        "    'condition_total_answers_given': pl.Float32,\n",
        "    'condition_session_count': pl.Float32,\n",
        "    'posttest_problem_count': pl.Float32,\n",
        "    'posttest_correct': pl.Float32,\n",
        "    'posttest_time_on_task': pl.Float32,\n",
        "    'posttest_average_first_response_time': pl.Float32,\n",
        "    'posttest_session_count': pl.Float32,\n",
        "    'assistments_reference_assignment_log_id': pl.UInt64\n",
        "}\n",
        "performance_parse_dates = ['release_date', 'due_date', 'start_time', 'end_time']\n",
        "\n",
        "metrics_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'student_prior_started_skill_builder_count': pl.UInt32,\n",
        "    'student_prior_completed_skill_builder_count': pl.UInt32,\n",
        "    'student_prior_started_problem_set_count': pl.UInt32,\n",
        "    'student_prior_completed_problem_set_count': pl.UInt32,\n",
        "    'student_prior_completed_problem_count': pl.UInt32,\n",
        "    'student_prior_median_first_response_time': pl.Float32,\n",
        "    'student_prior_median_time_on_task': pl.Float32,\n",
        "    'student_prior_average_correctness': pl.Float32,\n",
        "    'student_prior_average_attempt_count': pl.Float32,\n",
        "    'class_id': pl.Categorical,\n",
        "    'class_creation_date': pl.Utf8,\n",
        "    'class_student_count': pl.UInt16,\n",
        "    'class_prior_skill_builder_count': pl.UInt32,\n",
        "    'class_prior_problem_set_count': pl.UInt32,\n",
        "    'class_prior_skill_builder_percent_started': pl.Float32,\n",
        "    'class_prior_skill_builder_percent_completed': pl.Float32,\n",
        "    'class_prior_problem_set_percent_started': pl.Float32,\n",
        "    'class_prior_problem_set_percent_completed': pl.Float32,\n",
        "    'class_prior_completed_problem_count': pl.UInt32,\n",
        "    'class_prior_median_time_on_task': pl.Float32,\n",
        "    'class_prior_median_first_response_time': pl.Float32,\n",
        "    'class_prior_average_correctness': pl.Float32,\n",
        "    'class_prior_average_attempt_count': pl.Float32,\n",
        "    'teacher id': pl.Categorical, \n",
        "    'teacher_account_creation_date': pl.Utf8,\n",
        "    'district_id': pl.Categorical,\n",
        "    'location': pl.Categorical,\n",
        "    'opportunity_zone': pl.Categorical,\n",
        "    'locale_description': pl.Categorical\n",
        "}\n",
        "metrics_parse_dates = ['class_creation_date', 'teacher_account_creation_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper Function for Memory-Efficient CSV Concatenation\n",
        "\n",
        "def combine_polars_csvs(file_paths, schema=None, parse_dates_list=None,\n",
        "                        known_date_format_str: str = None,\n",
        "                        date_time_unit='us'):\n",
        "    lazy_frames = []\n",
        "    print(f\"\\nScanning {len(file_paths)} files...\")\n",
        "\n",
        "    common_columns_from_first_file = None\n",
        "    if file_paths and schema:\n",
        "        try:\n",
        "            common_columns_from_first_file = pl.scan_csv(\n",
        "                file_paths[0], infer_schema_length=100, n_rows=10\n",
        "            ).collect_schema().names()\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not determine common columns from first file {file_paths[0]}: {e}\")\n",
        "            common_columns_from_first_file = list(schema.keys())\n",
        "\n",
        "    problematic_file_for_date_parse = None\n",
        "    current_col_for_date_parse = \"unknown\"\n",
        "\n",
        "    for i, file_path_str in enumerate(file_paths):\n",
        "        file_path = Path(file_path_str)\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Scanning file {i+1}/{len(file_paths)}: {file_path.parent.name}/{file_path.name}\")\n",
        "\n",
        "        try:\n",
        "            lf = pl.scan_csv(file_path,\n",
        "                             schema=schema,\n",
        "                             infer_schema_length=100,\n",
        "                             null_values=[\"\", \"NA\", \"NaN\", \"null\"])\n",
        "\n",
        "            if parse_dates_list:\n",
        "                date_parsing_expressions = []\n",
        "                columns_to_check_for_dates = common_columns_from_first_file if common_columns_from_first_file else lf.collect_schema().names()\n",
        "\n",
        "                for col_name in parse_dates_list:\n",
        "                    current_col_for_date_parse = col_name\n",
        "                    if col_name in columns_to_check_for_dates:\n",
        "                        if col_name not in lf.collect_schema().names():\n",
        "                            continue\n",
        "\n",
        "                        date_expr = pl.col(col_name).cast(pl.Utf8, strict=False)\n",
        "\n",
        "                        if known_date_format_str:\n",
        "                            date_expr = date_expr.str.to_datetime(\n",
        "                                format=known_date_format_str,\n",
        "                                strict=False,\n",
        "                                time_unit=date_time_unit\n",
        "                            )\n",
        "                        else:\n",
        "                            date_expr = date_expr.str.to_datetime(\n",
        "                                strict=False,\n",
        "                                time_unit=date_time_unit\n",
        "                            )\n",
        "                        date_parsing_expressions.append(\n",
        "                            date_expr.dt.convert_time_zone(\"UTC\").alias(col_name)\n",
        "                        )\n",
        "                if date_parsing_expressions:\n",
        "                    lf = lf.with_columns(date_parsing_expressions)\n",
        "\n",
        "            lazy_frames.append(lf)\n",
        "            problematic_file_for_date_parse = None\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  Warning: File not found, skipping: {file_path}\")\n",
        "        except pl.exceptions.NoDataError:\n",
        "             print(f\"  Warning: File is empty, skipping: {file_path}\")\n",
        "        except Exception as e:\n",
        "            problematic_file_for_date_parse = file_path\n",
        "            if \"strptime\" in str(e).lower() or \"conversion\" in str(e).lower() or \"datetime\" in str(e).lower():\n",
        "                 print(f\"  Potential date parsing error for {problematic_file_for_date_parse} (column likely '{current_col_for_date_parse}'): {e}\")\n",
        "            else:\n",
        "                print(f\"  Error scanning {file_path} or applying initial transforms: {e}\")\n",
        "\n",
        "\n",
        "    if not lazy_frames:\n",
        "        print(\"  No lazy frames were created from scanning files.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Concatenating {len(lazy_frames)} lazy frames...\")\n",
        "    try:\n",
        "        combined_lf = pl.concat(lazy_frames, how=\"vertical_relaxed\")\n",
        "        print(\"Collecting data into DataFrame (streaming enabled)...\")\n",
        "        # Reverted to engine=\"streaming\" as per deprecation warning for Polars 1.29.0\n",
        "        collected_df = combined_lf.collect(engine=\"streaming\")\n",
        "        print(\"Concatenation and collection complete.\")\n",
        "        return collected_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error during lazy concatenation or collection: {e}\")\n",
        "        if problematic_file_for_date_parse:\n",
        "            print(f\"  This might be related to an earlier issue in file: {problematic_file_for_date_parse}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combining Actions Data (exp_slogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_slogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_slogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_slogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_slogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_slogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_slogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_slogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_slogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_slogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Actions DataFrame shape: (3708299, 9)\n",
            "\n",
            "Combining Problems Data (exp_plogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_plogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_plogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_plogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_plogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_plogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_plogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_plogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_plogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_plogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Problems DataFrame shape: (771386, 24)\n",
            "\n",
            "Combining Performance Data (exp_alogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_alogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_alogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_alogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_alogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_alogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_alogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_alogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_alogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_alogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Performance DataFrame shape: (95990, 35)\n",
            "\n",
            "Combining Metrics Data (priors)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/priors.csv\n",
            "  Scanning file 11/88: PSAYCFH/priors.csv\n",
            "  Scanning file 21/88: PSAZ2G4/priors.csv\n",
            "  Scanning file 31/88: PSAQJFP/priors.csv\n",
            "  Scanning file 41/88: PSAJVP8/priors.csv\n",
            "  Scanning file 51/88: PSA9XWV/priors.csv\n",
            "  Scanning file 61/88: PSAM4NK/priors.csv\n",
            "  Scanning file 71/88: PSATP2Z/priors.csv\n",
            "  Scanning file 81/88: PSASDZY/priors.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Metrics DataFrame shape: (95979, 31)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load DataFrames\n",
        "\n",
        "COMMON_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S%.f%z\" \n",
        "\n",
        "print(\"Combining Actions Data (exp_slogs)...\")\n",
        "actions_df = combine_polars_csvs(\n",
        "    actions_file_paths, \n",
        "    schema=actions_schema, \n",
        "    parse_dates_list=actions_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT \n",
        ")\n",
        "if actions_df is not None:\n",
        "    print(f\"Actions DataFrame shape: {actions_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Problems Data (exp_plogs)...\")\n",
        "problems_df = combine_polars_csvs(\n",
        "    problems_file_paths, \n",
        "    schema=problems_schema, \n",
        "    parse_dates_list=problems_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT \n",
        ")\n",
        "if problems_df is not None:\n",
        "    print(f\"Problems DataFrame shape: {problems_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Performance Data (exp_alogs)...\")\n",
        "performance_df = combine_polars_csvs(\n",
        "    performance_file_paths, \n",
        "    schema=performance_schema, \n",
        "    parse_dates_list=performance_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT\n",
        ")\n",
        "if performance_df is not None:\n",
        "    print(f\"Performance DataFrame shape: {performance_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Metrics Data (priors)...\")\n",
        "if 'teacher id' in metrics_schema: \n",
        "    metrics_schema_corrected = metrics_schema.copy()\n",
        "    metrics_schema_corrected['teacher_id'] = metrics_schema_corrected.pop('teacher id')\n",
        "else:\n",
        "    metrics_schema_corrected = metrics_schema\n",
        "\n",
        "metrics_df = combine_polars_csvs(\n",
        "    metrics_file_paths, \n",
        "    schema=metrics_schema_corrected, \n",
        "    parse_dates_list=metrics_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT\n",
        ")\n",
        "if metrics_df is not None:\n",
        "    if 'teacher id' in metrics_df.columns: \n",
        "        metrics_df = metrics_df.rename({'teacher id': 'teacher_id'})\n",
        "    print(f\"Metrics DataFrame shape: {metrics_df.shape}\")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Merge Operations ---\n",
            "Starting with actions_df: (3708299, 9)\n",
            "Merging problems_df...\n",
            "After merging problems_df: (3708299, 28)\n",
            "Merging performance_df...\n",
            "After merging performance_df: (3711215, 61)\n",
            "Merging metrics_df...\n",
            "After merging metrics_df: (3711215, 90)\n",
            "\n",
            "--- Merge Complete ---\n",
            "Final Merged DataFrame Info:\n",
            "Shape: (3711215, 90)\n",
            "Columns in merged_df: ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id', 'experiment_tag_path', 'action', 'timestamp', 'assistments_reference_action_log_id', 'problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task', 'first_response_or_request_time', 'first_answer', 'correct', 'reported_score', 'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given', 'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available', 'explanation_given', 'answer_given', 'assistments_reference_problem_log_id', 'release_date', 'due_date', 'start_time_perf', 'end_time_perf', 'assignment_session_count', 'pretest_problem_count', 'pretest_correct', 'pretest_time_on_task', 'pretest_average_first_response_time', 'pretest_session_count', 'assigned_condition', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_problem_count', 'condition_total_correct', 'condition_total_correct_after_wrong_response', 'condition_total_correct_after_tutoring', 'condition_total_answers_before_tutoring', 'condition_total_attempt_count', 'condition_total_hints_available', 'condition_total_hints_given', 'condition_total_scaffold_problems_available', 'condition_total_scaffold_problems_given', 'condition_total_explanations_available', 'condition_total_explanations_given', 'condition_total_answers_given', 'condition_session_count', 'posttest_problem_count', 'posttest_correct', 'posttest_time_on_task', 'posttest_average_first_response_time', 'posttest_session_count', 'assistments_reference_assignment_log_id', 'student_prior_started_skill_builder_count', 'student_prior_completed_skill_builder_count', 'student_prior_started_problem_set_count', 'student_prior_completed_problem_set_count', 'student_prior_completed_problem_count', 'student_prior_median_first_response_time', 'student_prior_median_time_on_task', 'student_prior_average_correctness', 'student_prior_average_attempt_count', 'class_id', 'class_creation_date', 'class_student_count', 'class_prior_skill_builder_count', 'class_prior_problem_set_count', 'class_prior_skill_builder_percent_started', 'class_prior_skill_builder_percent_completed', 'class_prior_problem_set_percent_started', 'class_prior_problem_set_percent_completed', 'class_prior_completed_problem_count', 'class_prior_median_time_on_task', 'class_prior_median_first_response_time', 'class_prior_average_correctness', 'class_prior_average_attempt_count', 'teacher_account_creation_date', 'district_id', 'location', 'opportunity_zone', 'locale_description', 'teacher_id']\n",
            "<bound method DataFrame.head of shape: (3_711_215, 90)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
            "│ experimen ┆ student_i ┆ problem_i ┆ problem_p ┆ … ┆ location ┆ opportuni ┆ locale_de ┆ teacher_i │\n",
            "│ t_id      ┆ d         ┆ d         ┆ art       ┆   ┆ ---      ┆ ty_zone   ┆ scription ┆ d         │\n",
            "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ cat      ┆ ---       ┆ ---       ┆ ---       │\n",
            "│ cat       ┆ cat       ┆ cat       ┆ cat       ┆   ┆          ┆ cat       ┆ cat       ┆ cat       │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ null      ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 1.0       ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 1.0       ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 1.0       ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ null      ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …        ┆ …         ┆ …         ┆ …         │\n",
            "│ PSA2KP9   ┆ 992517    ┆ PRABAWMQ  ┆ 1.0       ┆ … ┆ 1335.0   ┆ North     ┆ No        ┆ None      │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆ Carolina  ┆           ┆           │\n",
            "│ PSA2KP9   ┆ 992517    ┆ PRABAWMQ  ┆ 1.0       ┆ … ┆ 1335.0   ┆ North     ┆ No        ┆ None      │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆ Carolina  ┆           ┆           │\n",
            "│ PSA2KP9   ┆ 992517    ┆ PRABAWMQ  ┆ 1.0       ┆ … ┆ 1335.0   ┆ North     ┆ No        ┆ None      │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆ Carolina  ┆           ┆           │\n",
            "│ PSA2KP9   ┆ 992517    ┆ null      ┆ null      ┆ … ┆ 1335.0   ┆ North     ┆ No        ┆ None      │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆ Carolina  ┆           ┆           │\n",
            "│ PSA2KP9   ┆ 992517    ┆ PRAJDU2   ┆ 1.0       ┆ … ┆ 1335.0   ┆ North     ┆ No        ┆ None      │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆ Carolina  ┆           ┆           │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴──────────┴───────────┴───────────┴───────────┘>\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'problem_id': Categorical(ordering='physical'), 'problem_part': Categorical(ordering='physical'), 'scaffold_id': Categorical(ordering='physical'), 'experiment_tag_path': String, 'action': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'assistments_reference_action_log_id': UInt64, 'problem_condition': Categorical(ordering='physical'), 'start_time': Datetime(time_unit='us', time_zone='UTC'), 'end_time': Datetime(time_unit='us', time_zone='UTC'), 'session_count': UInt16, 'time_on_task': Float32, 'first_response_or_request_time': Float32, 'first_answer': String, 'correct': Boolean, 'reported_score': Float32, 'answer_before_tutoring': Boolean, 'attempt_count': UInt16, 'hints_available': UInt16, 'hints_given': UInt16, 'scaffold_problems_available': UInt16, 'scaffold_problems_given': UInt16, 'explanation_available': Boolean, 'explanation_given': Boolean, 'answer_given': Boolean, 'assistments_reference_problem_log_id': UInt64, 'release_date': Datetime(time_unit='us', time_zone='UTC'), 'due_date': Datetime(time_unit='us', time_zone='UTC'), 'start_time_perf': Datetime(time_unit='us', time_zone='UTC'), 'end_time_perf': Datetime(time_unit='us', time_zone='UTC'), 'assignment_session_count': Float32, 'pretest_problem_count': Float32, 'pretest_correct': Float32, 'pretest_time_on_task': Float32, 'pretest_average_first_response_time': Float32, 'pretest_session_count': Float32, 'assigned_condition': Categorical(ordering='physical'), 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_problem_count': Float32, 'condition_total_correct': Float32, 'condition_total_correct_after_wrong_response': Float32, 'condition_total_correct_after_tutoring': Float32, 'condition_total_answers_before_tutoring': Float32, 'condition_total_attempt_count': Float32, 'condition_total_hints_available': Float32, 'condition_total_hints_given': Float32, 'condition_total_scaffold_problems_available': Float32, 'condition_total_scaffold_problems_given': Float32, 'condition_total_explanations_available': Float32, 'condition_total_explanations_given': Float32, 'condition_total_answers_given': Float32, 'condition_session_count': Float32, 'posttest_problem_count': Float32, 'posttest_correct': Float32, 'posttest_time_on_task': Float32, 'posttest_average_first_response_time': Float32, 'posttest_session_count': Float32, 'assistments_reference_assignment_log_id': UInt64, 'student_prior_started_skill_builder_count': UInt32, 'student_prior_completed_skill_builder_count': UInt32, 'student_prior_started_problem_set_count': UInt32, 'student_prior_completed_problem_set_count': UInt32, 'student_prior_completed_problem_count': UInt32, 'student_prior_median_first_response_time': Float32, 'student_prior_median_time_on_task': Float32, 'student_prior_average_correctness': Float32, 'student_prior_average_attempt_count': Float32, 'class_id': Categorical(ordering='physical'), 'class_creation_date': Datetime(time_unit='us', time_zone='UTC'), 'class_student_count': UInt16, 'class_prior_skill_builder_count': UInt32, 'class_prior_problem_set_count': UInt32, 'class_prior_skill_builder_percent_started': Float32, 'class_prior_skill_builder_percent_completed': Float32, 'class_prior_problem_set_percent_started': Float32, 'class_prior_problem_set_percent_completed': Float32, 'class_prior_completed_problem_count': UInt32, 'class_prior_median_time_on_task': Float32, 'class_prior_median_first_response_time': Float32, 'class_prior_average_correctness': Float32, 'class_prior_average_attempt_count': Float32, 'teacher_account_creation_date': Datetime(time_unit='us', time_zone='UTC'), 'district_id': Categorical(ordering='physical'), 'location': Categorical(ordering='physical'), 'opportunity_zone': Categorical(ordering='physical'), 'locale_description': Categorical(ordering='physical'), 'teacher_id': Categorical(ordering='physical')})\n"
          ]
        }
      ],
      "source": [
        "# Merge DataFrames into One\n",
        "\n",
        "merged_df = None\n",
        "merge_successful = True\n",
        "\n",
        "print(\"\\n--- Starting Merge Operations ---\")\n",
        "\n",
        "if actions_df is None or actions_df.is_empty():\n",
        "    print(\"Actions DataFrame is empty or None. Cannot proceed with merge.\")\n",
        "    merge_successful = False\n",
        "else:\n",
        "    merged_df = actions_df.clone()\n",
        "    print(f\"Starting with actions_df: {merged_df.shape}\")\n",
        "\n",
        "    # Merge Problems Data\n",
        "    if problems_df is not None and not problems_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging problems_df...\")\n",
        "            problem_keys = ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id']\n",
        "            merged_df = merged_df.join(problems_df, on=problem_keys, how=\"left\", suffix=\"_problem\")\n",
        "            print(f\"After merging problems_df: {merged_df.shape}\")\n",
        "            del problems_df \n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging problems_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful: \n",
        "        print(\"Skipping problems_df merge (not loaded or empty).\")\n",
        "\n",
        "    # Merge Performance Data\n",
        "    if performance_df is not None and not performance_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging performance_df...\")\n",
        "            perf_keys = ['experiment_id', 'student_id']\n",
        "            merged_df = merged_df.join(performance_df, on=perf_keys, how=\"left\", suffix=\"_perf\")\n",
        "            print(f\"After merging performance_df: {merged_df.shape}\")\n",
        "            del performance_df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging performance_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful:\n",
        "        print(\"Skipping performance_df merge (not loaded or empty).\")\n",
        "\n",
        "    # Merge Metrics Data\n",
        "    if metrics_df is not None and not metrics_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging metrics_df...\")\n",
        "            metrics_keys = ['experiment_id', 'student_id']\n",
        "            merged_df = merged_df.join(metrics_df, on=metrics_keys, how=\"left\", suffix=\"_metrics\")\n",
        "            print(f\"After merging metrics_df: {merged_df.shape}\")\n",
        "            del metrics_df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging metrics_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful:\n",
        "        print(\"Skipping metrics_df merge (not loaded or empty).\")\n",
        "\n",
        "    if merged_df is not None and merge_successful:\n",
        "        print(\"\\n--- Merge Complete ---\")\n",
        "        print(\"Final Merged DataFrame Info:\")\n",
        "        print(f\"Shape: {merged_df.shape}\")\n",
        "        print(\"Columns in merged_df:\", merged_df.columns)\n",
        "        print(merged_df.head)\n",
        "        print(merged_df.schema)\n",
        "        \n",
        "        if 'actions_df' in locals() and actions_df is not merged_df: \n",
        "            del actions_df\n",
        "            gc.collect()\n",
        "            \n",
        "    elif merged_df is not None: \n",
        "        print(\"\\n--- Merge Partially Complete or Some DataFrames Skipped ---\")\n",
        "        print(\"Columns in partially merged_df:\", merged_df.columns)\n",
        "    else: \n",
        "        print(\"\\n--- Merge Failed or Base DataFrame (actions_df) was not suitable ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8NxlbW3ynlT"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Data Cleaning ---\n",
            "Initial merged_df shape for cleaning: (3711215, 90)\n",
            "\n",
            "--- Renaming Columns ---\n",
            "Applying renames: {'assistments_reference_action_log_id': 'action_log_id', 'start_time_perf': 'assignment_start_time', 'end_time_perf': 'assignment_end_time', 'assistments_reference_assignment_log_id': 'assignment_log_id'}\n",
            "Scheduled for categorical conversion: experiment_tag_path\n",
            "Scheduled 'assignment_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct_after_wrong_response' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct_after_tutoring' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_answers_before_tutoring' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_attempt_count' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_hints_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_hints_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_scaffold_problems_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_scaffold_problems_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_explanations_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_explanations_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_answers_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_session_count' for Float32 to UInt16 conversion.\n",
            "\n",
            "Applying column type transformations...\n",
            "Type transformations applied.\n",
            "\n",
            "--- Specific Value Cleaning ---\n",
            "Scheduled 'opportunity_zone' to boolean 'opportunity_zone_bool' conversion.\n",
            "Scheduled fill_null for categorical district_id with 'Unknown_District'.\n",
            "Scheduled fill_null for categorical location with 'Unknown_Location'.\n",
            "Scheduled fill_null for categorical locale_description with 'Unknown_Locale'.\n",
            "\n",
            "Applying specific value cleaning expressions...\n",
            "Specific value cleaning applied.\n",
            "\n",
            "Dropping fully empty columns: ['problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task', 'first_response_or_request_time', 'first_answer', 'correct', 'reported_score', 'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given', 'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available', 'explanation_given', 'answer_given', 'assistments_reference_problem_log_id']\n",
            "Dropping original opportunity zone column: 'opportunity_zone'\n",
            "\n",
            "Shape after Cleaning: (3711215, 71)\n",
            "Columns after cleaning: ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id', 'experiment_tag_path', 'action', 'timestamp', 'action_log_id', 'release_date', 'due_date', 'assignment_start_time', 'assignment_end_time', 'assignment_session_count', 'pretest_problem_count', 'pretest_correct', 'pretest_time_on_task', 'pretest_average_first_response_time', 'pretest_session_count', 'assigned_condition', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_problem_count', 'condition_total_correct', 'condition_total_correct_after_wrong_response', 'condition_total_correct_after_tutoring', 'condition_total_answers_before_tutoring', 'condition_total_attempt_count', 'condition_total_hints_available', 'condition_total_hints_given', 'condition_total_scaffold_problems_available', 'condition_total_scaffold_problems_given', 'condition_total_explanations_available', 'condition_total_explanations_given', 'condition_total_answers_given', 'condition_session_count', 'posttest_problem_count', 'posttest_correct', 'posttest_time_on_task', 'posttest_average_first_response_time', 'posttest_session_count', 'assignment_log_id', 'student_prior_started_skill_builder_count', 'student_prior_completed_skill_builder_count', 'student_prior_started_problem_set_count', 'student_prior_completed_problem_set_count', 'student_prior_completed_problem_count', 'student_prior_median_first_response_time', 'student_prior_median_time_on_task', 'student_prior_average_correctness', 'student_prior_average_attempt_count', 'class_id', 'class_creation_date', 'class_student_count', 'class_prior_skill_builder_count', 'class_prior_problem_set_count', 'class_prior_skill_builder_percent_started', 'class_prior_skill_builder_percent_completed', 'class_prior_problem_set_percent_started', 'class_prior_problem_set_percent_completed', 'class_prior_completed_problem_count', 'class_prior_median_time_on_task', 'class_prior_median_first_response_time', 'class_prior_average_correctness', 'class_prior_average_attempt_count', 'teacher_account_creation_date', 'district_id', 'location', 'locale_description', 'teacher_id', 'opportunity_zone_bool']\n"
          ]
        }
      ],
      "source": [
        "# Data Cleaning\n",
        "\n",
        "if 'merged_df' in locals() and merged_df is not None and merge_successful: \n",
        "    print(\"\\n--- Starting Data Cleaning ---\")\n",
        "    print(f\"Initial merged_df shape for cleaning: {merged_df.shape}\")\n",
        "\n",
        "    print(\"\\n--- Renaming Columns ---\")\n",
        "    rename_map = {\n",
        "        'assistments_reference_action_log_id': 'action_log_id',\n",
        "        'start_time_perf': 'assignment_start_time',\n",
        "        'end_time_perf': 'assignment_end_time',\n",
        "        'assistments_reference_assignment_log_id': 'assignment_log_id'\n",
        "    }\n",
        "    actual_renames = {k: v for k, v in rename_map.items() if k in merged_df.columns}\n",
        "    if actual_renames:\n",
        "        print(f\"Applying renames: {actual_renames}\")\n",
        "        merged_df = merged_df.rename(actual_renames)\n",
        "    else:\n",
        "        print(\"No columns matched for renaming based on the current rename_map.\")\n",
        "\n",
        "    column_transformations = []\n",
        "\n",
        "    datetime_cols_final_check = [\n",
        "        'timestamp', 'start_time', 'end_time', 'release_date', 'due_date',\n",
        "        'assignment_start_time', 'assignment_end_time',\n",
        "        'class_creation_date', 'teacher_account_creation_date'\n",
        "    ]\n",
        "    for col_name in datetime_cols_final_check:\n",
        "        if col_name in merged_df.columns:\n",
        "            current_dtype = merged_df[col_name].dtype\n",
        "            if current_dtype == pl.Utf8:\n",
        "                print(f\"Scheduled for datetime re-parsing (UTF8 found): {col_name}\")\n",
        "                column_transformations.append(\n",
        "                    pl.col(col_name).str.to_datetime(format=COMMON_DATETIME_FORMAT, strict=False, time_unit='us')\n",
        "                    .dt.convert_time_zone(\"UTC\")\n",
        "                    .alias(col_name)\n",
        "                )\n",
        "            elif isinstance(current_dtype, pl.Datetime):\n",
        "                current_tz = current_dtype.time_zone\n",
        "                if current_tz is None:\n",
        "                    print(f\"Info: Datetime column '{col_name}' is naive. Localizing to UTC.\")\n",
        "                    column_transformations.append(\n",
        "                        pl.col(col_name).dt.replace_time_zone(\"UTC\", ambiguous='earliest').alias(col_name)\n",
        "                    )\n",
        "                elif current_tz != \"UTC\":\n",
        "                    print(f\"Scheduled for UTC conversion (already datetime, was '{current_tz}'): {col_name}\")\n",
        "                    column_transformations.append(\n",
        "                        pl.col(col_name).dt.convert_time_zone(\"UTC\").alias(col_name)\n",
        "                    )\n",
        "\n",
        "    cols_to_category_polars = [\n",
        "        'experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id',\n",
        "        'experiment_tag_path', 'action', 'problem_condition', 'assigned_condition',\n",
        "        'class_id', 'district_id', 'location', 'opportunity_zone',\n",
        "        'locale_description', 'teacher_id'\n",
        "    ]\n",
        "    for col_name in cols_to_category_polars:\n",
        "        if col_name in merged_df.columns and merged_df[col_name].dtype != pl.Categorical:\n",
        "             column_transformations.append(pl.col(col_name).cast(pl.Categorical).alias(col_name))\n",
        "             print(f\"Scheduled for categorical conversion: {col_name}\")\n",
        "\n",
        "    float_to_int_casts = {\n",
        "        'assignment_session_count': pl.UInt16, 'pretest_problem_count': pl.UInt16,\n",
        "        'pretest_correct': pl.UInt16, 'pretest_session_count': pl.UInt16,\n",
        "        'condition_problem_count': pl.UInt16, 'condition_total_correct': pl.UInt16,\n",
        "        'condition_total_correct_after_wrong_response': pl.UInt16,\n",
        "        'condition_total_correct_after_tutoring': pl.UInt16,\n",
        "        'condition_total_answers_before_tutoring': pl.UInt16,\n",
        "        'condition_total_attempt_count': pl.UInt32,\n",
        "        'condition_total_hints_available': pl.UInt32, 'condition_total_hints_given': pl.UInt32,\n",
        "        'condition_total_scaffold_problems_available': pl.UInt32,\n",
        "        'condition_total_scaffold_problems_given': pl.UInt32,\n",
        "        'condition_total_explanations_available': pl.UInt32,\n",
        "        'condition_total_explanations_given': pl.UInt32,\n",
        "        'condition_total_answers_given': pl.UInt32,\n",
        "        'condition_session_count': pl.UInt16, 'posttest_problem_count': pl.UInt16,\n",
        "        'posttest_correct': pl.UInt16, 'posttest_session_count': pl.UInt16,\n",
        "    }\n",
        "    for col_name, target_int_type in float_to_int_casts.items():\n",
        "        if col_name in merged_df.columns:\n",
        "            if merged_df[col_name].dtype == pl.Float32:\n",
        "                column_transformations.append(\n",
        "                    pl.col(col_name)\n",
        "                      .fill_null(0)\n",
        "                      .cast(target_int_type, strict=False)\n",
        "                      .alias(col_name)\n",
        "                )\n",
        "                print(f\"Scheduled '{col_name}' for Float32 to {target_int_type} conversion.\")\n",
        "            elif merged_df[col_name].dtype != target_int_type:\n",
        "                print(f\"Warning: Column '{col_name}' was expected to be Float32 for int conversion, but found {merged_df[col_name].dtype}. Skipping specific int cast.\")\n",
        "\n",
        "    # General Float64 to Float32 pass\n",
        "    float64_cols = [col_name for col_name, dtype in merged_df.schema.items() if dtype == pl.Float64]\n",
        "    for col_name in float64_cols:\n",
        "        if col_name in merged_df.columns:\n",
        "            column_transformations.append(pl.col(col_name).cast(pl.Float32).alias(col_name))\n",
        "            print(f\"Scheduled for Float64 to Float32 conversion: {col_name}\")\n",
        "\n",
        "    if column_transformations:\n",
        "        print(\"\\nApplying column type transformations...\")\n",
        "        merged_df = merged_df.with_columns(column_transformations)\n",
        "        print(\"Type transformations applied.\")\n",
        "\n",
        "    print(\"\\n--- Specific Value Cleaning ---\")\n",
        "    specific_value_cleaning_expressions = []\n",
        "\n",
        "    if 'opportunity_zone' in merged_df.columns:\n",
        "        if merged_df['opportunity_zone'].dtype != pl.Categorical:\n",
        "             merged_df = merged_df.with_columns(pl.col('opportunity_zone').cast(pl.Categorical))\n",
        "        specific_value_cleaning_expressions.append(\n",
        "            pl.when(pl.col('opportunity_zone').cast(pl.Utf8) == \"Yes\").then(True)\n",
        "              .when(pl.col('opportunity_zone').cast(pl.Utf8) == \"No\").then(False)\n",
        "              .otherwise(None)\n",
        "              .cast(pl.Boolean)\n",
        "              .alias('opportunity_zone_bool')\n",
        "        )\n",
        "        print(\"Scheduled 'opportunity_zone' to boolean 'opportunity_zone_bool' conversion.\")\n",
        "\n",
        "    cat_cols_to_fill_info = {\n",
        "        'district_id': 'Unknown_District',\n",
        "        'location': 'Unknown_Location',\n",
        "        'locale_description': 'Unknown_Locale'\n",
        "    }\n",
        "    for col_name, fill_val in cat_cols_to_fill_info.items():\n",
        "        if col_name in merged_df.columns:\n",
        "            if merged_df[col_name].dtype != pl.Categorical:\n",
        "                merged_df = merged_df.with_columns(pl.col(col_name).cast(pl.Categorical))\n",
        "                print(f\"Casted '{col_name}' to Categorical before fill_null.\")\n",
        "            specific_value_cleaning_expressions.append(pl.col(col_name).fill_null(pl.lit(fill_val).cast(pl.Categorical)).alias(col_name))\n",
        "            print(f\"Scheduled fill_null for categorical {col_name} with '{fill_val}'.\")\n",
        "\n",
        "    if specific_value_cleaning_expressions:\n",
        "        print(\"\\nApplying specific value cleaning expressions...\")\n",
        "        merged_df = merged_df.with_columns(specific_value_cleaning_expressions)\n",
        "        print(\"Specific value cleaning applied.\")\n",
        "\n",
        "    pandas_identified_empty_cols = [\n",
        "         'problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task',\n",
        "         'first_response_or_request_time', 'first_answer', 'correct', 'reported_score',\n",
        "         'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given',\n",
        "         'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available',\n",
        "         'explanation_given', 'answer_given',\n",
        "         'assistments_reference_problem_log_id'\n",
        "    ]\n",
        "    actual_empty_cols_to_drop = []\n",
        "    if not merged_df.is_empty():\n",
        "        for col_name in pandas_identified_empty_cols:\n",
        "            if col_name in merged_df.columns and merged_df[col_name].is_null().all():\n",
        "                actual_empty_cols_to_drop.append(col_name)\n",
        "            elif col_name in merged_df.columns:\n",
        "                null_count = merged_df[col_name].is_null().sum()\n",
        "                if null_count > 0 :\n",
        "                    print(f\"Info: Column '{col_name}' (candidate for empty drop) was not fully null. Nulls: {null_count}/{merged_df.height}\")\n",
        "\n",
        "    if actual_empty_cols_to_drop:\n",
        "        print(f\"\\nDropping fully empty columns: {actual_empty_cols_to_drop}\")\n",
        "        merged_df = merged_df.drop(actual_empty_cols_to_drop)\n",
        "    else:\n",
        "        print(\"\\nNo fully empty columns (from the predefined list) identified for dropping.\")\n",
        "\n",
        "    if 'opportunity_zone' in merged_df.columns and 'opportunity_zone_bool' in merged_df.columns:\n",
        "        print(\"Dropping original opportunity zone column: 'opportunity_zone'\")\n",
        "        merged_df = merged_df.drop('opportunity_zone')\n",
        "\n",
        "    print(f\"\\nShape after Cleaning: {merged_df.shape}\")\n",
        "    print(\"Columns after cleaning:\", merged_df.columns)\n",
        "    gc.collect()\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Cell 7 cleaning: merged_df not available, previous merge failed, or merge_successful flag is False.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Reducing DataFrame to Essential Columns ---\n",
            "Attempting to select these 18 essential columns: ['experiment_id', 'student_id', 'timestamp', 'action', 'action_log_id', 'assignment_start_time', 'assignment_end_time', 'assignment_log_id', 'assignment_session_count', 'condition_problem_count', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_total_correct', 'condition_total_attempt_count', 'condition_total_hints_given', 'condition_total_explanations_given', 'student_prior_average_correctness', 'opportunity_zone_bool']\n",
            "\n",
            "Reduced DataFrame Info: Shape (3711215, 18)\n",
            "shape: (5, 18)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
            "│ experimen ┆ student_i ┆ timestamp ┆ action    ┆ … ┆ condition ┆ condition ┆ student_p ┆ opportun │\n",
            "│ t_id      ┆ d         ┆ ---       ┆ ---       ┆   ┆ _total_hi ┆ _total_ex ┆ rior_aver ┆ ity_zone │\n",
            "│ ---       ┆ ---       ┆ datetime[ ┆ cat       ┆   ┆ nts_given ┆ planation ┆ age_corre ┆ _bool    │\n",
            "│ cat       ┆ cat       ┆ μs, UTC]  ┆           ┆   ┆ ---       ┆ s_g…      ┆ ctn…      ┆ ---      │\n",
            "│           ┆           ┆           ┆           ┆   ┆ u32       ┆ ---       ┆ ---       ┆ bool     │\n",
            "│           ┆           ┆           ┆           ┆   ┆           ┆ u32       ┆ f32       ┆          │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ assignmen ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:41:4 ┆ t_started ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 6.393 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ problem_s ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:41:4 ┆ tarted    ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 7.437 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ correct_r ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:42:0 ┆ esponse   ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 3.665 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ problem_f ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:42:0 ┆ inished   ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 3.675 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ continue_ ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:42:0 ┆ selected  ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 5.295 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'action': Categorical(ordering='physical'), 'action_log_id': UInt64, 'assignment_start_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_end_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_log_id': UInt64, 'assignment_session_count': UInt16, 'condition_problem_count': UInt16, 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_total_correct': UInt16, 'condition_total_attempt_count': UInt32, 'condition_total_hints_given': UInt32, 'condition_total_explanations_given': UInt32, 'student_prior_average_correctness': Float32, 'opportunity_zone_bool': Boolean})\n",
            "\n",
            "Attempting to save cleaned and reduced DataFrame to: /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "Successfully saved to /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "\n",
            "Full cleaned merged_df deleted from memory.\n"
          ]
        }
      ],
      "source": [
        "# Create Reduced DataFrame and Save\n",
        "\n",
        "if 'merged_df' in locals() and merged_df is not None and merge_successful:\n",
        "    print(\"\\n--- Reducing DataFrame to Essential Columns ---\")\n",
        "\n",
        "    essential_cols_to_keep_polars = [\n",
        "        # Keys / Base Info from actions_df\n",
        "        'experiment_id', \n",
        "        'student_id', \n",
        "        'timestamp', \n",
        "        'action', \n",
        "        'action_log_id',\n",
        "        \n",
        "        # From performance_df \n",
        "        'assignment_start_time', \n",
        "        'assignment_end_time',   \n",
        "        'assignment_log_id',     \n",
        "        'assignment_session_count', \n",
        "        'condition_problem_count',  \n",
        "        'condition_time_on_task',   \n",
        "        'condition_average_first_response_or_request_time', \n",
        "        'condition_total_correct',  \n",
        "        'condition_total_attempt_count', \n",
        "        'condition_total_hints_given', \n",
        "        'condition_total_explanations_given', \n",
        "        \n",
        "        # From metrics_df \n",
        "        'student_prior_average_correctness', \n",
        "        \n",
        "        'opportunity_zone_bool', \n",
        "    ]\n",
        "    \n",
        "    # Filter to only include columns that actually exist in the cleaned merged_df\n",
        "    final_essential_columns = [col for col in essential_cols_to_keep_polars if col in merged_df.columns]\n",
        "    \n",
        "    print(f\"Attempting to select these {len(final_essential_columns)} essential columns: {final_essential_columns}\")\n",
        "    missing_essentials_for_reduction = [col for col in essential_cols_to_keep_polars if col not in final_essential_columns]\n",
        "\n",
        "    if missing_essentials_for_reduction:\n",
        "        print(f\"Warning: The following conceptual essential columns were NOT FOUND in merged_df for reduction: {missing_essentials_for_reduction}\")\n",
        "        print(\"Please ensure their names are correct in the 'essential_cols_to_keep_polars' list and they exist in the output of Cell 7.\")\n",
        "    \n",
        "    if not final_essential_columns:\n",
        "        print(\"Error: No essential columns available for selection based on your list. Cannot create reduced DataFrame.\")\n",
        "        merged_df_reduced = None\n",
        "    else:\n",
        "        try:\n",
        "            merged_df_reduced = merged_df.select(final_essential_columns)\n",
        "            print(f\"\\nReduced DataFrame Info: Shape {merged_df_reduced.shape}\")\n",
        "            print(merged_df_reduced.head())\n",
        "            print(merged_df_reduced.schema)\n",
        "\n",
        "            print(f\"\\nAttempting to save cleaned and reduced DataFrame to: {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "            merged_df_reduced.write_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET) \n",
        "            print(f\"Successfully saved to {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error during final select or save: {e}\")\n",
        "            merged_df_reduced = None\n",
        "            \n",
        "    if 'merged_df' in locals(): \n",
        "        del merged_df \n",
        "        gc.collect()\n",
        "        print(\"\\nFull cleaned merged_df deleted from memory.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Cell 8 (reduction and save): merged_df not available from Cell 7 or previous steps failed.\")\n",
        "    merged_df_reduced = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading Cleaned Parquet File ---\n",
            "Successfully reloaded: /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "Reloaded DataFrame Shape: (3711215, 18)\n",
            "\n",
            "Reloaded DataFrame Head (from Parquet):\n",
            "shape: (5, 18)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
            "│ experimen ┆ student_i ┆ timestamp ┆ action    ┆ … ┆ condition ┆ condition ┆ student_p ┆ opportun │\n",
            "│ t_id      ┆ d         ┆ ---       ┆ ---       ┆   ┆ _total_hi ┆ _total_ex ┆ rior_aver ┆ ity_zone │\n",
            "│ ---       ┆ ---       ┆ datetime[ ┆ cat       ┆   ┆ nts_given ┆ planation ┆ age_corre ┆ _bool    │\n",
            "│ cat       ┆ cat       ┆ μs, UTC]  ┆           ┆   ┆ ---       ┆ s_g…      ┆ ctn…      ┆ ---      │\n",
            "│           ┆           ┆           ┆           ┆   ┆ u32       ┆ ---       ┆ ---       ┆ bool     │\n",
            "│           ┆           ┆           ┆           ┆   ┆           ┆ u32       ┆ f32       ┆          │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ assignmen ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:41:4 ┆ t_started ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 6.393 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ problem_s ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:41:4 ┆ tarted    ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 7.437 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ correct_r ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:42:0 ┆ esponse   ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 3.665 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ problem_f ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:42:0 ┆ inished   ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 3.675 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ continue_ ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:42:0 ┆ selected  ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 5.295 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
            "\n",
            "Reloaded DataFrame Schema (from Parquet):\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'action': Categorical(ordering='physical'), 'action_log_id': UInt64, 'assignment_start_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_end_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_log_id': UInt64, 'assignment_session_count': UInt16, 'condition_problem_count': UInt16, 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_total_correct': UInt16, 'condition_total_attempt_count': UInt32, 'condition_total_hints_given': UInt32, 'condition_total_explanations_given': UInt32, 'student_prior_average_correctness': Float32, 'opportunity_zone_bool': Boolean})\n"
          ]
        }
      ],
      "source": [
        "# Load Cleaned Data\n",
        "\n",
        "if 'merged_df_reduced' in locals() and merged_df_reduced is not None and not merged_df_reduced.is_empty() and SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "    print(f\"\\n--- Loading Cleaned Parquet File ---\")\n",
        "    try:\n",
        "        df_reloaded_polars = pl.read_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET)\n",
        "        print(f\"Successfully reloaded: {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "        print(f\"Reloaded DataFrame Shape: {df_reloaded_polars.shape}\")\n",
        "        print(\"\\nReloaded DataFrame Head (from Parquet):\")\n",
        "        print(df_reloaded_polars.head())\n",
        "        print(\"\\nReloaded DataFrame Schema (from Parquet):\")\n",
        "        print(df_reloaded_polars.schema)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reloading the cleaned Parquet file: {e}\")\n",
        "elif SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "     print(f\"Cleaned Parquet file found at {SAVE_CLEANED_PATH_POLARS_PARQUET}, but merged_df_reduced may not have been successfully created or was empty in the previous step (this script might have been re-run starting from here). Consider reloading manually if needed.\")\n",
        "else:\n",
        "    print(f\"\\nCleaned Parquet file not found at {SAVE_CLEANED_PATH_POLARS_PARQUET} or reduction/save step failed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
