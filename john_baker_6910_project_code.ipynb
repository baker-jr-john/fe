{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ9f09tc34r0"
      },
      "source": [
        "# Load, Merge, and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polars version: 1.29.0\n",
            "Base data path: /Users/john/Downloads/osfstorage-archive\n",
            "Experiment IDs path: /Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23\n",
            "Global String Cache enabled: True\n"
          ]
        }
      ],
      "source": [
        "# Setup and Configuration (Updated)\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "import gc\n",
        "\n",
        "# --- Enable Global String Cache for Categoricals ---\n",
        "pl.enable_string_cache()\n",
        "# --- ---\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_DATA_PATH = Path('/Users/john/Downloads/osfstorage-archive') # Use your actual path\n",
        "EXPERIMENT_IDS_PATH = BASE_DATA_PATH / 'experiment_dataset_2021-09-23'\n",
        "\n",
        "# Output path for the cleaned data\n",
        "SAVE_CLEANED_PATH_POLARS_PARQUET = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.parquet'\n",
        "SAVE_CLEANED_PATH_POLARS_CSV = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.csv'\n",
        "\n",
        "print(f\"Polars version: {pl.__version__}\")\n",
        "print(f\"Base data path: {BASE_DATA_PATH}\")\n",
        "print(f\"Experiment IDs path: {EXPERIMENT_IDS_PATH}\")\n",
        "print(f\"Global String Cache enabled: {pl.using_string_cache()}\") # Verify it's enabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 88 experiment ID directories.\n",
            "Sample performance file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_alogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_alogs.csv']\n",
            "Sample problems file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_plogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_plogs.csv']\n",
            "Sample actions file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_slogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_slogs.csv']\n",
            "Sample metrics file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/priors.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/priors.csv']\n"
          ]
        }
      ],
      "source": [
        "# Generate File Paths\n",
        "try:\n",
        "    if not EXPERIMENT_IDS_PATH.is_dir():\n",
        "        raise FileNotFoundError(f\"Error: Directory not found at {EXPERIMENT_IDS_PATH}\")\n",
        "    experiment_ids = [d.name for d in EXPERIMENT_IDS_PATH.iterdir() if d.is_dir()]\n",
        "    print(f\"Found {len(experiment_ids)} experiment ID directories.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    experiment_ids = []\n",
        "\n",
        "performance_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_alogs.csv') for exp_id in experiment_ids]\n",
        "problems_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_plogs.csv') for exp_id in experiment_ids]\n",
        "actions_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_slogs.csv') for exp_id in experiment_ids]\n",
        "metrics_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'priors.csv') for exp_id in experiment_ids]\n",
        "\n",
        "print(\"Sample performance file paths:\", performance_file_paths[:2])\n",
        "print(\"Sample problems file paths:\", problems_file_paths[:2])\n",
        "print(\"Sample actions file paths:\", actions_file_paths[:2])\n",
        "print(\"Sample metrics file paths:\", metrics_file_paths[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Schemas and Date Parsing Information\n",
        "\n",
        "actions_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'problem_id': pl.Categorical,\n",
        "    'problem_part': pl.Categorical, # Or Utf8 if it can have diverse values\n",
        "    'scaffold_id': pl.Categorical, # Or Utf8\n",
        "    'experiment_tag_path': pl.Utf8, # Can be long and varied\n",
        "    'action': pl.Categorical,\n",
        "    'timestamp': pl.Utf8, # Read as string, parse later\n",
        "    'assistments_reference_action_log_id': pl.UInt64\n",
        "}\n",
        "actions_parse_dates = ['timestamp']\n",
        "\n",
        "problems_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'problem_id': pl.Categorical,\n",
        "    'problem_part': pl.Categorical,\n",
        "    'scaffold_id': pl.Categorical,\n",
        "    'problem_condition': pl.Categorical, # Or Utf8\n",
        "    'start_time': pl.Utf8, # Parse later\n",
        "    'end_time': pl.Utf8,   # Parse later\n",
        "    'session_count': pl.UInt16,\n",
        "    'time_on_task': pl.Float32,\n",
        "    'first_response_or_request_time': pl.Float32,\n",
        "    'first_answer': pl.Utf8,\n",
        "    'correct': pl.Boolean,\n",
        "    'reported_score': pl.Float32,\n",
        "    'answer_before_tutoring': pl.Boolean,\n",
        "    'attempt_count': pl.UInt16,\n",
        "    'hints_available': pl.UInt16,\n",
        "    'hints_given': pl.UInt16,\n",
        "    'scaffold_problems_available': pl.UInt16,\n",
        "    'scaffold_problems_given': pl.UInt16,\n",
        "    'explanation_available': pl.Boolean,\n",
        "    'explanation_given': pl.Boolean,\n",
        "    'answer_given': pl.Boolean,\n",
        "    'assistments_reference_problem_log_id': pl.UInt64\n",
        "}\n",
        "problems_parse_dates = ['start_time', 'end_time']\n",
        "\n",
        "performance_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'release_date': pl.Utf8, # Will be parsed to Datetime\n",
        "    'due_date': pl.Utf8,     # Will be parsed to Datetime\n",
        "    'start_time': pl.Utf8,   # Will be parsed to Datetime\n",
        "    'end_time': pl.Utf8,     # Will be parsed to Datetime\n",
        "    \n",
        "    # Counts and scores changed to Float32 for robust parsing from CSV\n",
        "    'assignment_session_count': pl.Float32, # Was UInt16\n",
        "    'pretest_problem_count': pl.Float32,    # Was UInt16\n",
        "    'pretest_correct': pl.Float32,          # Was UInt16\n",
        "    'pretest_time_on_task': pl.Float32,     # Already Float32, OK\n",
        "    'pretest_average_first_response_time': pl.Float32, # Already Float32, OK\n",
        "    'pretest_session_count': pl.Float32,    # Was UInt16\n",
        "    \n",
        "    'assigned_condition': pl.Categorical,\n",
        "    \n",
        "    'condition_time_on_task': pl.Float32,   # Already Float32, OK\n",
        "    'condition_average_first_response_or_request_time': pl.Float32, # Already Float32, OK\n",
        "    'condition_problem_count': pl.Float32,  # Changed in previous step, OK\n",
        "    'condition_total_correct': pl.Float32,  # << FAILING COLUMN, NOW CHANGED (was UInt16)\n",
        "    'condition_total_correct_after_wrong_response': pl.Float32, # Was UInt16\n",
        "    'condition_total_correct_after_tutoring': pl.Float32,       # Was UInt16\n",
        "    'condition_total_answers_before_tutoring': pl.Float32,    # Was UInt16\n",
        "    'condition_total_attempt_count': pl.Float32,                # Was UInt32\n",
        "    'condition_total_hints_available': pl.Float32,              # Was UInt32\n",
        "    'condition_total_hints_given': pl.Float32,                  # Was UInt32\n",
        "    'condition_total_scaffold_problems_available': pl.Float32,  # Was UInt32\n",
        "    'condition_total_scaffold_problems_given': pl.Float32,      # Was UInt32\n",
        "    'condition_total_explanations_available': pl.Float32,       # Was UInt32\n",
        "    'condition_total_explanations_given': pl.Float32,           # Was UInt32\n",
        "    'condition_total_answers_given': pl.Float32,                # Was UInt32\n",
        "    'condition_session_count': pl.Float32,    # Was UInt16\n",
        "    \n",
        "    'posttest_problem_count': pl.Float32,   # Was UInt16\n",
        "    'posttest_correct': pl.Float32,         # Was UInt16\n",
        "    'posttest_time_on_task': pl.Float32,    # Already Float32, OK\n",
        "    'posttest_average_first_response_time': pl.Float32, # Already Float32, OK\n",
        "    'posttest_session_count': pl.Float32,   # Was UInt16\n",
        "    \n",
        "    'assistments_reference_assignment_log_id': pl.UInt64 # IDs are usually safe as integers if no decimals\n",
        "}\n",
        "performance_parse_dates = ['release_date', 'due_date', 'start_time', 'end_time']\n",
        "\n",
        "metrics_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'student_prior_started_skill_builder_count': pl.UInt32,\n",
        "    'student_prior_completed_skill_builder_count': pl.UInt32,\n",
        "    'student_prior_started_problem_set_count': pl.UInt32,\n",
        "    'student_prior_completed_problem_set_count': pl.UInt32,\n",
        "    'student_prior_completed_problem_count': pl.UInt32,\n",
        "    'student_prior_median_first_response_time': pl.Float32,\n",
        "    'student_prior_median_time_on_task': pl.Float32,\n",
        "    'student_prior_average_correctness': pl.Float32,\n",
        "    'student_prior_average_attempt_count': pl.Float32,\n",
        "    'class_id': pl.Categorical,\n",
        "    'class_creation_date': pl.Utf8,\n",
        "    'class_student_count': pl.UInt16,\n",
        "    'class_prior_skill_builder_count': pl.UInt32,\n",
        "    'class_prior_problem_set_count': pl.UInt32,\n",
        "    'class_prior_skill_builder_percent_started': pl.Float32,\n",
        "    'class_prior_skill_builder_percent_completed': pl.Float32,\n",
        "    'class_prior_problem_set_percent_started': pl.Float32,\n",
        "    'class_prior_problem_set_percent_completed': pl.Float32,\n",
        "    'class_prior_completed_problem_count': pl.UInt32,\n",
        "    'class_prior_median_time_on_task': pl.Float32,\n",
        "    'class_prior_median_first_response_time': pl.Float32,\n",
        "    'class_prior_average_correctness': pl.Float32,\n",
        "    'class_prior_average_attempt_count': pl.Float32,\n",
        "    'teacher id': pl.Categorical, # Note: column name with space, will need quoting or renaming\n",
        "    'teacher_account_creation_date': pl.Utf8,\n",
        "    'district_id': pl.Categorical, # Or Utf8\n",
        "    'location': pl.Categorical,    # Or Utf8\n",
        "    'opportunity_zone': pl.Categorical, # Or Utf8\n",
        "    'locale_description': pl.Categorical # Or Utf8\n",
        "}\n",
        "metrics_parse_dates = ['class_creation_date', 'teacher_account_creation_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper Function for Memory-Efficient CSV Concatenation\n",
        "\n",
        "def combine_polars_csvs(file_paths, schema=None, parse_dates_list=None, \n",
        "                        # CRITICAL: Provide this if your date format is consistent!\n",
        "                        known_date_format_str: str = None, \n",
        "                        date_time_unit='us'):\n",
        "    \"\"\"\n",
        "    Scans multiple CSVs lazily, concatenates them, and then collects into a Polars DataFrame.\n",
        "    Includes robust date parsing and UTC conversion.\n",
        "    Optimized to reduce schema collection calls and use known date format.\n",
        "    \"\"\"\n",
        "    lazy_frames = []\n",
        "    print(f\"\\nScanning {len(file_paths)} files...\")\n",
        "\n",
        "    # Attempt to get a common set of columns from the first file if a schema is provided\n",
        "    # This helps in applying date parsing expressions more consistently.\n",
        "    common_columns_from_first_file = None\n",
        "    if file_paths and schema:\n",
        "        try:\n",
        "            common_columns_from_first_file = pl.scan_csv(\n",
        "                file_paths[0], infer_schema_length=100, n_rows=10 # Scan a bit more for robustness\n",
        "            ).collect_schema().names()\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not determine common columns from first file {file_paths[0]}: {e}\")\n",
        "            common_columns_from_first_file = list(schema.keys()) # Fallback to all schema keys\n",
        "\n",
        "    for i, file_path_str in enumerate(file_paths):\n",
        "        file_path = Path(file_path_str)\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Scanning file {i+1}/{len(file_paths)}: {file_path.parent.name}/{file_path.name}\")\n",
        "\n",
        "        try:\n",
        "            # Use the provided schema directly. scan_csv is generally tolerant of \n",
        "            # schema columns not in the file (will be null) or file columns not in schema (will be excluded or inferred).\n",
        "            # For more control, use `dtypes` argument to override types for specific columns.\n",
        "            lf = pl.scan_csv(file_path, \n",
        "                             schema=schema, # Use the global schema for this file type\n",
        "                             infer_schema_length=100, # Allow some inference for columns not in schema\n",
        "                             null_values=[\"\", \"NA\", \"NaN\", \"null\"])\n",
        "            \n",
        "            if parse_dates_list:\n",
        "                date_parsing_expressions = []\n",
        "                # Use common_columns_from_first_file or fallback to current lf schema if first file scan failed\n",
        "                # This avoids calling collect_schema() for *every* file inside the loop for this check\n",
        "                columns_to_check = common_columns_from_first_file if common_columns_from_first_file else lf.collect_schema().names()\n",
        "\n",
        "                for col_name in parse_dates_list:\n",
        "                    if col_name in columns_to_check: \n",
        "                        # Check if column actually exists in the current specific file's lazy frame\n",
        "                        # This is a compromise: call collect_schema() once per file if needed,\n",
        "                        # but only if the more optimized `columns_to_check` suggests it might be there.\n",
        "                        if col_name not in lf.collect_schema().names():\n",
        "                            continue\n",
        "\n",
        "                        date_expr = pl.col(col_name).cast(pl.Utf8, strict=False)\n",
        "                        \n",
        "                        if known_date_format_str:\n",
        "                            date_expr = date_expr.str.to_datetime(\n",
        "                                format=known_date_format_str,\n",
        "                                strict=False,\n",
        "                                time_unit=date_time_unit\n",
        "                            )\n",
        "                        else: # Fallback to inference if no format string is given (slower)\n",
        "                            date_expr = date_expr.str.to_datetime(\n",
        "                                strict=False,\n",
        "                                time_unit=date_time_unit\n",
        "                            )\n",
        "                        \n",
        "                        # Always convert to UTC after parsing\n",
        "                        date_parsing_expressions.append(\n",
        "                            date_expr.dt.convert_time_zone(\"UTC\").alias(col_name)\n",
        "                        )\n",
        "                if date_parsing_expressions:\n",
        "                    lf = lf.with_columns(date_parsing_expressions)\n",
        "            \n",
        "            lazy_frames.append(lf)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  Warning: File not found, skipping: {file_path}\")\n",
        "        except pl.exceptions.NoDataError: \n",
        "             print(f\"  Warning: File is empty, skipping: {file_path}\")\n",
        "        except Exception as e: \n",
        "            # Catching Polars specific ComputeError which can happen during .str.to_datetime if format is wrong\n",
        "            if \"strptime\" in str(e).lower() or \"conversion\" in str(e).lower():\n",
        "                 print(f\"  Potential date parsing error for {file_path} (column likely {col_name if 'col_name' in locals() else 'unknown'}): {e}\")\n",
        "            else:\n",
        "                print(f\"  Error scanning {file_path} or applying initial transforms: {e}\")\n",
        "\n",
        "\n",
        "    if not lazy_frames:\n",
        "        print(\"  No lazy frames were created from scanning files.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Concatenating {len(lazy_frames)} lazy frames...\")\n",
        "    try:\n",
        "        combined_lf = pl.concat(lazy_frames, how=\"vertical_relaxed\")\n",
        "        print(\"Collecting data into DataFrame...\")\n",
        "        collected_df = combined_lf.collect(engine=\"streaming\") # Enable streaming for collect\n",
        "        print(\"Concatenation and collection complete.\")\n",
        "        return collected_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error during lazy concatenation or collection: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combining Actions Data (exp_slogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_slogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_slogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_slogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_slogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_slogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_slogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_slogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_slogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_slogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame...\n",
            "Concatenation and collection complete.\n",
            "Actions DataFrame shape: (3708299, 9)\n",
            "\n",
            "Combining Problems Data (exp_plogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_plogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_plogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_plogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_plogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_plogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_plogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_plogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_plogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_plogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame...\n",
            "Concatenation and collection complete.\n",
            "Problems DataFrame shape: (771386, 24)\n",
            "\n",
            "Combining Performance Data (exp_alogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_alogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_alogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_alogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_alogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_alogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_alogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_alogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_alogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_alogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame...\n",
            "Concatenation and collection complete.\n",
            "Performance DataFrame shape: (95990, 35)\n",
            "\n",
            "Combining Metrics Data (priors)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/priors.csv\n",
            "  Scanning file 11/88: PSAYCFH/priors.csv\n",
            "  Scanning file 21/88: PSAZ2G4/priors.csv\n",
            "  Scanning file 31/88: PSAQJFP/priors.csv\n",
            "  Scanning file 41/88: PSAJVP8/priors.csv\n",
            "  Scanning file 51/88: PSA9XWV/priors.csv\n",
            "  Scanning file 61/88: PSAM4NK/priors.csv\n",
            "  Scanning file 71/88: PSATP2Z/priors.csv\n",
            "  Scanning file 81/88: PSASDZY/priors.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame...\n",
            "Concatenation and collection complete.\n",
            "Metrics DataFrame shape: (95979, 31)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "17793"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load DataFrames for each type using the Polars helper\n",
        "\n",
        "# *** Use the VERIFIED format string from Cell 3.5 output ***\n",
        "COMMON_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S%.f%z\" \n",
        "\n",
        "print(\"Combining Actions Data (exp_slogs)...\")\n",
        "actions_df = combine_polars_csvs(\n",
        "    actions_file_paths, \n",
        "    schema=actions_schema, \n",
        "    parse_dates_list=actions_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT \n",
        ")\n",
        "if actions_df is not None:\n",
        "    print(f\"Actions DataFrame shape: {actions_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Problems Data (exp_plogs)...\")\n",
        "# Assuming problems_df date columns also use this format. If not, adjust or pass None.\n",
        "problems_df = combine_polars_csvs(\n",
        "    problems_file_paths, \n",
        "    schema=problems_schema, \n",
        "    parse_dates_list=problems_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT \n",
        ")\n",
        "if problems_df is not None:\n",
        "    print(f\"Problems DataFrame shape: {problems_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Performance Data (exp_alogs)...\")\n",
        "# Assuming performance_df date columns also use this format.\n",
        "performance_df = combine_polars_csvs(\n",
        "    performance_file_paths, \n",
        "    schema=performance_schema, \n",
        "    parse_dates_list=performance_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT\n",
        ")\n",
        "if performance_df is not None:\n",
        "    print(f\"Performance DataFrame shape: {performance_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Metrics Data (priors)...\")\n",
        "# Assuming metrics_df date columns also use this format.\n",
        "if 'teacher id' in metrics_schema: \n",
        "    metrics_schema_corrected = metrics_schema.copy()\n",
        "    metrics_schema_corrected['teacher_id'] = metrics_schema_corrected.pop('teacher id')\n",
        "else:\n",
        "    metrics_schema_corrected = metrics_schema\n",
        "\n",
        "metrics_df = combine_polars_csvs(\n",
        "    metrics_file_paths, \n",
        "    schema=metrics_schema_corrected, \n",
        "    parse_dates_list=metrics_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT\n",
        ")\n",
        "if metrics_df is not None:\n",
        "    if 'teacher id' in metrics_df.columns: \n",
        "        metrics_df = metrics_df.rename({'teacher id': 'teacher_id'})\n",
        "    print(f\"Metrics DataFrame shape: {metrics_df.shape}\")\n",
        "\n",
        "import gc # Ensure gc is imported if you use it here\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Merge Operations (Polars) ---\n",
            "Starting with actions_df: (3708299, 9)\n",
            "Merging problems_df...\n",
            "After merging problems_df: (3708299, 28)\n",
            "Merging performance_df...\n",
            "After merging performance_df: (3711215, 61)\n",
            "Merging metrics_df...\n",
            "After merging metrics_df: (3711215, 90)\n",
            "\n",
            "--- Merge Complete ---\n",
            "Final Merged DataFrame Info:\n",
            "Shape: (3711215, 90)\n",
            "\n",
            "IMPORTANT: Review these column names carefully before proceeding to Cell 7!\n",
            "Columns in merged_df: ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id', 'experiment_tag_path', 'action', 'timestamp', 'assistments_reference_action_log_id', 'problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task', 'first_response_or_request_time', 'first_answer', 'correct', 'reported_score', 'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given', 'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available', 'explanation_given', 'answer_given', 'assistments_reference_problem_log_id', 'release_date', 'due_date', 'start_time_perf', 'end_time_perf', 'assignment_session_count', 'pretest_problem_count', 'pretest_correct', 'pretest_time_on_task', 'pretest_average_first_response_time', 'pretest_session_count', 'assigned_condition', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_problem_count', 'condition_total_correct', 'condition_total_correct_after_wrong_response', 'condition_total_correct_after_tutoring', 'condition_total_answers_before_tutoring', 'condition_total_attempt_count', 'condition_total_hints_available', 'condition_total_hints_given', 'condition_total_scaffold_problems_available', 'condition_total_scaffold_problems_given', 'condition_total_explanations_available', 'condition_total_explanations_given', 'condition_total_answers_given', 'condition_session_count', 'posttest_problem_count', 'posttest_correct', 'posttest_time_on_task', 'posttest_average_first_response_time', 'posttest_session_count', 'assistments_reference_assignment_log_id', 'student_prior_started_skill_builder_count', 'student_prior_completed_skill_builder_count', 'student_prior_started_problem_set_count', 'student_prior_completed_problem_set_count', 'student_prior_completed_problem_count', 'student_prior_median_first_response_time', 'student_prior_median_time_on_task', 'student_prior_average_correctness', 'student_prior_average_attempt_count', 'class_id', 'class_creation_date', 'class_student_count', 'class_prior_skill_builder_count', 'class_prior_problem_set_count', 'class_prior_skill_builder_percent_started', 'class_prior_skill_builder_percent_completed', 'class_prior_problem_set_percent_started', 'class_prior_problem_set_percent_completed', 'class_prior_completed_problem_count', 'class_prior_median_time_on_task', 'class_prior_median_first_response_time', 'class_prior_average_correctness', 'class_prior_average_attempt_count', 'teacher_account_creation_date', 'district_id', 'location', 'opportunity_zone', 'locale_description', 'teacher_id']\n",
            "shape: (3, 90)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
            "│ experimen ┆ student_i ┆ problem_i ┆ problem_p ┆ … ┆ location ┆ opportuni ┆ locale_de ┆ teacher_i │\n",
            "│ t_id      ┆ d         ┆ d         ┆ art       ┆   ┆ ---      ┆ ty_zone   ┆ scription ┆ d         │\n",
            "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ cat      ┆ ---       ┆ ---       ┆ ---       │\n",
            "│ cat       ┆ cat       ┆ cat       ┆ cat       ┆   ┆          ┆ cat       ┆ cat       ┆ cat       │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ null      ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 1.0       ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 1.0       ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴──────────┴───────────┴───────────┴───────────┘\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'problem_id': Categorical(ordering='physical'), 'problem_part': Categorical(ordering='physical'), 'scaffold_id': Categorical(ordering='physical'), 'experiment_tag_path': String, 'action': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'assistments_reference_action_log_id': UInt64, 'problem_condition': Categorical(ordering='physical'), 'start_time': Datetime(time_unit='us', time_zone='UTC'), 'end_time': Datetime(time_unit='us', time_zone='UTC'), 'session_count': UInt16, 'time_on_task': Float32, 'first_response_or_request_time': Float32, 'first_answer': String, 'correct': Boolean, 'reported_score': Float32, 'answer_before_tutoring': Boolean, 'attempt_count': UInt16, 'hints_available': UInt16, 'hints_given': UInt16, 'scaffold_problems_available': UInt16, 'scaffold_problems_given': UInt16, 'explanation_available': Boolean, 'explanation_given': Boolean, 'answer_given': Boolean, 'assistments_reference_problem_log_id': UInt64, 'release_date': Datetime(time_unit='us', time_zone='UTC'), 'due_date': Datetime(time_unit='us', time_zone='UTC'), 'start_time_perf': Datetime(time_unit='us', time_zone='UTC'), 'end_time_perf': Datetime(time_unit='us', time_zone='UTC'), 'assignment_session_count': Float32, 'pretest_problem_count': Float32, 'pretest_correct': Float32, 'pretest_time_on_task': Float32, 'pretest_average_first_response_time': Float32, 'pretest_session_count': Float32, 'assigned_condition': Categorical(ordering='physical'), 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_problem_count': Float32, 'condition_total_correct': Float32, 'condition_total_correct_after_wrong_response': Float32, 'condition_total_correct_after_tutoring': Float32, 'condition_total_answers_before_tutoring': Float32, 'condition_total_attempt_count': Float32, 'condition_total_hints_available': Float32, 'condition_total_hints_given': Float32, 'condition_total_scaffold_problems_available': Float32, 'condition_total_scaffold_problems_given': Float32, 'condition_total_explanations_available': Float32, 'condition_total_explanations_given': Float32, 'condition_total_answers_given': Float32, 'condition_session_count': Float32, 'posttest_problem_count': Float32, 'posttest_correct': Float32, 'posttest_time_on_task': Float32, 'posttest_average_first_response_time': Float32, 'posttest_session_count': Float32, 'assistments_reference_assignment_log_id': UInt64, 'student_prior_started_skill_builder_count': UInt32, 'student_prior_completed_skill_builder_count': UInt32, 'student_prior_started_problem_set_count': UInt32, 'student_prior_completed_problem_set_count': UInt32, 'student_prior_completed_problem_count': UInt32, 'student_prior_median_first_response_time': Float32, 'student_prior_median_time_on_task': Float32, 'student_prior_average_correctness': Float32, 'student_prior_average_attempt_count': Float32, 'class_id': Categorical(ordering='physical'), 'class_creation_date': Datetime(time_unit='us', time_zone='UTC'), 'class_student_count': UInt16, 'class_prior_skill_builder_count': UInt32, 'class_prior_problem_set_count': UInt32, 'class_prior_skill_builder_percent_started': Float32, 'class_prior_skill_builder_percent_completed': Float32, 'class_prior_problem_set_percent_started': Float32, 'class_prior_problem_set_percent_completed': Float32, 'class_prior_completed_problem_count': UInt32, 'class_prior_median_time_on_task': Float32, 'class_prior_median_first_response_time': Float32, 'class_prior_average_correctness': Float32, 'class_prior_average_attempt_count': Float32, 'teacher_account_creation_date': Datetime(time_unit='us', time_zone='UTC'), 'district_id': Categorical(ordering='physical'), 'location': Categorical(ordering='physical'), 'opportunity_zone': Categorical(ordering='physical'), 'locale_description': Categorical(ordering='physical'), 'teacher_id': Categorical(ordering='physical')})\n"
          ]
        }
      ],
      "source": [
        "# Merge DataFrames into One\n",
        "\n",
        "merged_df = None\n",
        "merge_successful = True\n",
        "\n",
        "print(\"\\n--- Starting Merge Operations (Polars) ---\")\n",
        "\n",
        "if actions_df is None or actions_df.is_empty():\n",
        "    print(\"Actions DataFrame is empty or None. Cannot proceed with merge.\")\n",
        "    merge_successful = False\n",
        "else:\n",
        "    merged_df = actions_df.clone()\n",
        "    print(f\"Starting with actions_df: {merged_df.shape}\")\n",
        "\n",
        "    # Merge Problems Data\n",
        "    if problems_df is not None and not problems_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging problems_df...\")\n",
        "            problem_keys = ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id']\n",
        "            # Ensure keys are present in both DataFrames. Polars will error if not.\n",
        "            merged_df = merged_df.join(problems_df, on=problem_keys, how=\"left\", suffix=\"_problem\")\n",
        "            print(f\"After merging problems_df: {merged_df.shape}\")\n",
        "            del problems_df \n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging problems_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful: # only print skip message if not already failed\n",
        "        print(\"Skipping problems_df merge (not loaded or empty).\")\n",
        "\n",
        "    # Merge Performance Data\n",
        "    if performance_df is not None and not performance_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging performance_df...\")\n",
        "            perf_keys = ['experiment_id', 'student_id']\n",
        "            merged_df = merged_df.join(performance_df, on=perf_keys, how=\"left\", suffix=\"_perf\")\n",
        "            print(f\"After merging performance_df: {merged_df.shape}\")\n",
        "            del performance_df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging performance_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful:\n",
        "        print(\"Skipping performance_df merge (not loaded or empty).\")\n",
        "\n",
        "    # Merge Metrics Data\n",
        "    if metrics_df is not None and not metrics_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging metrics_df...\")\n",
        "            metrics_keys = ['experiment_id', 'student_id']\n",
        "            merged_df = merged_df.join(metrics_df, on=metrics_keys, how=\"left\", suffix=\"_metrics\")\n",
        "            print(f\"After merging metrics_df: {merged_df.shape}\")\n",
        "            del metrics_df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging metrics_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful:\n",
        "        print(\"Skipping metrics_df merge (not loaded or empty).\")\n",
        "\n",
        "    if merged_df is not None and merge_successful:\n",
        "        print(\"\\n--- Merge Complete ---\")\n",
        "        print(\"Final Merged DataFrame Info:\")\n",
        "        print(f\"Shape: {merged_df.shape}\")\n",
        "        print(\"\\nIMPORTANT: Review these column names carefully before proceeding to Cell 7!\")\n",
        "        print(\"Columns in merged_df:\", merged_df.columns)\n",
        "        print(merged_df.head(3))\n",
        "        print(merged_df.schema)\n",
        "        \n",
        "        if 'actions_df' in locals() and actions_df is not merged_df: # only delete if it's a separate object\n",
        "            del actions_df\n",
        "            gc.collect()\n",
        "            \n",
        "    elif merged_df is not None: # Merge was partially complete or some DFs were missing\n",
        "        print(\"\\n--- Merge Partially Complete or Some DataFrames Skipped ---\")\n",
        "        print(\"Columns in partially merged_df:\", merged_df.columns)\n",
        "    else: # merge_successful is False and merged_df might be None or the initial actions_df\n",
        "        print(\"\\n--- Merge Failed or Base DataFrame (actions_df) was not suitable ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8NxlbW3ynlT"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Data Cleaning (Polars) ---\n",
            "Initial merged_df shape for cleaning: (3711215, 90)\n",
            "Initial columns in merged_df for cleaning: ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id', 'experiment_tag_path', 'action', 'timestamp', 'assistments_reference_action_log_id', 'problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task', 'first_response_or_request_time', 'first_answer', 'correct', 'reported_score', 'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given', 'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available', 'explanation_given', 'answer_given', 'assistments_reference_problem_log_id', 'release_date', 'due_date', 'start_time_perf', 'end_time_perf', 'assignment_session_count', 'pretest_problem_count', 'pretest_correct', 'pretest_time_on_task', 'pretest_average_first_response_time', 'pretest_session_count', 'assigned_condition', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_problem_count', 'condition_total_correct', 'condition_total_correct_after_wrong_response', 'condition_total_correct_after_tutoring', 'condition_total_answers_before_tutoring', 'condition_total_attempt_count', 'condition_total_hints_available', 'condition_total_hints_given', 'condition_total_scaffold_problems_available', 'condition_total_scaffold_problems_given', 'condition_total_explanations_available', 'condition_total_explanations_given', 'condition_total_answers_given', 'condition_session_count', 'posttest_problem_count', 'posttest_correct', 'posttest_time_on_task', 'posttest_average_first_response_time', 'posttest_session_count', 'assistments_reference_assignment_log_id', 'student_prior_started_skill_builder_count', 'student_prior_completed_skill_builder_count', 'student_prior_started_problem_set_count', 'student_prior_completed_problem_set_count', 'student_prior_completed_problem_count', 'student_prior_median_first_response_time', 'student_prior_median_time_on_task', 'student_prior_average_correctness', 'student_prior_average_attempt_count', 'class_id', 'class_creation_date', 'class_student_count', 'class_prior_skill_builder_count', 'class_prior_problem_set_count', 'class_prior_skill_builder_percent_started', 'class_prior_skill_builder_percent_completed', 'class_prior_problem_set_percent_started', 'class_prior_problem_set_percent_completed', 'class_prior_completed_problem_count', 'class_prior_median_time_on_task', 'class_prior_median_first_response_time', 'class_prior_average_correctness', 'class_prior_average_attempt_count', 'teacher_account_creation_date', 'district_id', 'location', 'opportunity_zone', 'locale_description', 'teacher_id']\n",
            "\n",
            "--- Renaming Columns ---\n",
            "Applying renames: {'assistments_reference_action_log_id': 'action_log_id', 'start_time_perf': 'assignment_start_time', 'end_time_perf': 'assignment_end_time', 'assistments_reference_assignment_log_id': 'assignment_log_id'}\n",
            "Scheduled for categorical conversion: experiment_tag_path\n",
            "Scheduled 'assignment_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct_after_wrong_response' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct_after_tutoring' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_answers_before_tutoring' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_attempt_count' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_hints_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_hints_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_scaffold_problems_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_scaffold_problems_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_explanations_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_explanations_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_answers_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_session_count' for Float32 to UInt16 conversion.\n",
            "\n",
            "Applying column type transformations...\n",
            "Type transformations applied.\n",
            "\n",
            "--- Specific Value Cleaning (Polars) ---\n",
            "Scheduled 'opportunity_zone' to boolean 'opportunity_zone_bool' conversion.\n",
            "Scheduled fill_null for categorical district_id with 'Unknown_District'.\n",
            "Scheduled fill_null for categorical location with 'Unknown_Location'.\n",
            "Scheduled fill_null for categorical locale_description with 'Unknown_Locale'.\n",
            "\n",
            "Applying specific value cleaning expressions...\n",
            "Specific value cleaning applied.\n",
            "\n",
            "Dropping fully empty columns (identified from pandas analysis): ['problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task', 'first_response_or_request_time', 'first_answer', 'correct', 'reported_score', 'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given', 'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available', 'explanation_given', 'answer_given', 'assistments_reference_problem_log_id']\n",
            "Dropping original opportunity zone column: 'opportunity_zone'\n",
            "\n",
            "Shape after Cleaning: (3711215, 71)\n",
            "Columns after cleaning: ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id', 'experiment_tag_path', 'action', 'timestamp', 'action_log_id', 'release_date', 'due_date', 'assignment_start_time', 'assignment_end_time', 'assignment_session_count', 'pretest_problem_count', 'pretest_correct', 'pretest_time_on_task', 'pretest_average_first_response_time', 'pretest_session_count', 'assigned_condition', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_problem_count', 'condition_total_correct', 'condition_total_correct_after_wrong_response', 'condition_total_correct_after_tutoring', 'condition_total_answers_before_tutoring', 'condition_total_attempt_count', 'condition_total_hints_available', 'condition_total_hints_given', 'condition_total_scaffold_problems_available', 'condition_total_scaffold_problems_given', 'condition_total_explanations_available', 'condition_total_explanations_given', 'condition_total_answers_given', 'condition_session_count', 'posttest_problem_count', 'posttest_correct', 'posttest_time_on_task', 'posttest_average_first_response_time', 'posttest_session_count', 'assignment_log_id', 'student_prior_started_skill_builder_count', 'student_prior_completed_skill_builder_count', 'student_prior_started_problem_set_count', 'student_prior_completed_problem_set_count', 'student_prior_completed_problem_count', 'student_prior_median_first_response_time', 'student_prior_median_time_on_task', 'student_prior_average_correctness', 'student_prior_average_attempt_count', 'class_id', 'class_creation_date', 'class_student_count', 'class_prior_skill_builder_count', 'class_prior_problem_set_count', 'class_prior_skill_builder_percent_started', 'class_prior_skill_builder_percent_completed', 'class_prior_problem_set_percent_started', 'class_prior_problem_set_percent_completed', 'class_prior_completed_problem_count', 'class_prior_median_time_on_task', 'class_prior_median_first_response_time', 'class_prior_average_correctness', 'class_prior_average_attempt_count', 'teacher_account_creation_date', 'district_id', 'location', 'locale_description', 'teacher_id', 'opportunity_zone_bool']\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Data Cleaning (Polars Style) - UPDATED\n",
        "\n",
        "if 'merged_df' in locals() and merged_df is not None and merge_successful:\n",
        "    print(\"\\n--- Starting Data Cleaning (Polars) ---\")\n",
        "    print(f\"Initial merged_df shape for cleaning: {merged_df.shape}\")\n",
        "    print(\"Initial columns in merged_df for cleaning:\", merged_df.columns) # Good for reference\n",
        "\n",
        "    # --- Renaming Columns ---\n",
        "    print(\"\\n--- Renaming Columns ---\")\n",
        "    \n",
        "    # Based on your merged_df.columns output:\n",
        "    rename_map = {\n",
        "        'assistments_reference_action_log_id': 'action_log_id', # From actions_df\n",
        "        'start_time_perf': 'assignment_start_time', # 'start_time' from performance_df became 'start_time_perf'\n",
        "        'end_time_perf': 'assignment_end_time',     # 'end_time' from performance_df became 'end_time_perf'\n",
        "        'assistments_reference_assignment_log_id': 'assignment_log_id' # From performance_df, NO suffix needed as per your output\n",
        "    }\n",
        "    actual_renames = {k: v for k, v in rename_map.items() if k in merged_df.columns}\n",
        "    if actual_renames:\n",
        "        print(f\"Applying renames: {actual_renames}\")\n",
        "        merged_df = merged_df.rename(actual_renames)\n",
        "    else:\n",
        "        print(\"No columns matched for renaming based on the current rename_map.\")\n",
        "    \n",
        "    # --- Type Transformations ---\n",
        "    column_transformations = [] # Reset list for this section\n",
        "\n",
        "    # Ensure Datetime columns are correctly parsed (most should be from load) and UTC\n",
        "    # These are original names or target names from renames above.\n",
        "    datetime_cols_final_check = [\n",
        "        'timestamp', # from actions\n",
        "        'start_time', 'end_time', # from problems\n",
        "        'release_date', 'due_date', # from performance\n",
        "        'assignment_start_time', 'assignment_end_time', # TARGETS of rename from _perf\n",
        "        'class_creation_date', 'teacher_account_creation_date' # from metrics\n",
        "    ]\n",
        "    for col_name in datetime_cols_final_check:\n",
        "        if col_name in merged_df.columns:\n",
        "            if merged_df[col_name].dtype == pl.Utf8: # If any were missed and are still string\n",
        "                column_transformations.append(\n",
        "                    pl.col(col_name).str.to_datetime(format=COMMON_DATETIME_FORMAT, strict=False, time_unit='us')\n",
        "                    .dt.convert_time_zone(\"UTC\")\n",
        "                    .alias(col_name)\n",
        "                )\n",
        "                print(f\"Scheduled for datetime re-parsing: {col_name}\")\n",
        "            elif isinstance(merged_df[col_name].dtype, pl.Datetime): # A more robust check for Datetime dtype\n",
        "                # Access the time_zone attribute from the dtype object itself\n",
        "                current_column_dtype = merged_df[col_name].dtype\n",
        "                if hasattr(current_column_dtype, 'time_zone'): # Check if the dtype object has time_zone (it should for pl.Datetime)\n",
        "                    current_tz = current_column_dtype.time_zone\n",
        "                    if current_tz is not None and current_tz != \"UTC\":\n",
        "                        column_transformations.append(\n",
        "                            pl.col(col_name).dt.convert_time_zone(\"UTC\").alias(col_name)\n",
        "                        )\n",
        "                        print(f\"Scheduled for UTC conversion (already datetime, was '{current_tz}'): {col_name}\")\n",
        "                    elif current_tz is None:\n",
        "                        # This case (naive datetime) should ideally not happen if combine_polars_csvs worked correctly,\n",
        "                        # as it's supposed to convert to UTC.\n",
        "                        # If it does happen, you might want to assume it's UTC and localize it:\n",
        "                        # column_transformations.append(\n",
        "                        #     pl.col(col_name).dt.replace_time_zone(\"UTC\", ambiguous='raise').alias(col_name)\n",
        "                        # )\n",
        "                        # print(f\"Warning: Naive datetime found for '{col_name}'. Attempting to localize to UTC.\")\n",
        "                        print(f\"Info: Datetime column '{col_name}' is naive (no timezone). Assuming it should be UTC from prior steps.\")\n",
        "                else:\n",
        "                    # This should not happen for a pl.Datetime dtype from recent Polars versions\n",
        "                    print(f\"Warning: Datetime column '{col_name}' does not have a 'time_zone' attribute on its dtype, though dtype is {current_column_dtype}.\")\n",
        "\n",
        "    # Convert to Categorical (using original names as suffixes were minimal)\n",
        "    # These names are directly from your merged_df.columns list (or schemas)\n",
        "    cols_to_category_polars = [\n",
        "        'experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id',\n",
        "        'experiment_tag_path', 'action', 'problem_condition', 'assigned_condition',\n",
        "        'class_id', 'district_id', 'location', 'opportunity_zone', \n",
        "        'locale_description', 'teacher_id'\n",
        "    ]\n",
        "    for col_name in cols_to_category_polars:\n",
        "        if col_name in merged_df.columns and merged_df[col_name].dtype != pl.Categorical :\n",
        "             column_transformations.append(pl.col(col_name).cast(pl.Categorical).alias(col_name))\n",
        "             print(f\"Scheduled for categorical conversion: {col_name}\")\n",
        "    \n",
        "    # Cast columns loaded as Float32 (from performance_df) back to appropriate Integer types\n",
        "    # These are original column names from performance_schema\n",
        "    float_to_int_casts = {\n",
        "        # Col name : Target Polars Int Dtype\n",
        "        'assignment_session_count': pl.UInt16,\n",
        "        'pretest_problem_count': pl.UInt16,\n",
        "        'pretest_correct': pl.UInt16,\n",
        "        'pretest_session_count': pl.UInt16,\n",
        "        'condition_problem_count': pl.UInt16,\n",
        "        'condition_total_correct': pl.UInt16,\n",
        "        'condition_total_correct_after_wrong_response': pl.UInt16,\n",
        "        'condition_total_correct_after_tutoring': pl.UInt16,\n",
        "        'condition_total_answers_before_tutoring': pl.UInt16,\n",
        "        'condition_total_attempt_count': pl.UInt32, # Note: some were UInt32\n",
        "        'condition_total_hints_available': pl.UInt32,\n",
        "        'condition_total_hints_given': pl.UInt32,\n",
        "        'condition_total_scaffold_problems_available': pl.UInt32,\n",
        "        'condition_total_scaffold_problems_given': pl.UInt32,\n",
        "        'condition_total_explanations_available': pl.UInt32,\n",
        "        'condition_total_explanations_given': pl.UInt32,\n",
        "        'condition_total_answers_given': pl.UInt32,\n",
        "        'condition_session_count': pl.UInt16,\n",
        "        'posttest_problem_count': pl.UInt16,\n",
        "        'posttest_correct': pl.UInt16,\n",
        "        'posttest_session_count': pl.UInt16,\n",
        "    }\n",
        "    for col_name, target_int_type in float_to_int_casts.items():\n",
        "        if col_name in merged_df.columns:\n",
        "            if merged_df[col_name].dtype == pl.Float32: # Ensure it's currently Float32\n",
        "                column_transformations.append(\n",
        "                    pl.col(col_name)\n",
        "                      .fill_null(0) # Or other appropriate fill strategy for counts\n",
        "                      .cast(target_int_type, strict=False) # strict=False allows 3.0 -> 3\n",
        "                      .alias(col_name) \n",
        "                )\n",
        "                print(f\"Scheduled '{col_name}' for Float32 to {target_int_type} conversion.\")\n",
        "            elif merged_df[col_name].dtype != target_int_type : # If it's some other type already\n",
        "                print(f\"Warning: Column '{col_name}' was expected to be Float32 for int conversion, but found {merged_df[col_name].dtype}. Skipping int cast.\")\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col_name}' intended for int conversion not found in merged_df.\")\n",
        "\n",
        "\n",
        "    # General Float64 to Float32 pass (if any remain)\n",
        "    for col_name in merged_df.columns: \n",
        "        if col_name in merged_df.columns and merged_df[col_name].dtype == pl.Float64: \n",
        "            column_transformations.append(pl.col(col_name).cast(pl.Float32).alias(col_name))\n",
        "            print(f\"Scheduled for Float64 to Float32 conversion: {col_name}\")\n",
        "\n",
        "    if column_transformations:\n",
        "        print(\"\\nApplying column type transformations...\")\n",
        "        merged_df = merged_df.with_columns(column_transformations)\n",
        "        print(\"Type transformations applied.\")\n",
        "        # print(merged_df.schema) # Good check here\n",
        "\n",
        "    # --- Specific Value Cleaning ---\n",
        "    print(\"\\n--- Specific Value Cleaning (Polars) ---\")\n",
        "    specific_value_cleaning_expressions = []\n",
        "    \n",
        "    # Convert 'opportunity_zone' (from metrics_df, original name) to boolean\n",
        "    if 'opportunity_zone' in merged_df.columns:\n",
        "        specific_value_cleaning_expressions.append(\n",
        "            pl.when(pl.col('opportunity_zone') == \"Yes\").then(True)\n",
        "              .when(pl.col('opportunity_zone') == \"No\").then(False)\n",
        "              .otherwise(None) \n",
        "              .cast(pl.Boolean)\n",
        "              .alias('opportunity_zone_bool')\n",
        "        )\n",
        "        print(f\"Scheduled 'opportunity_zone' to boolean 'opportunity_zone_bool' conversion.\")\n",
        "\n",
        "    # Fill NA for specific category columns (from metrics_df, original names)\n",
        "    cat_cols_to_fill_info = { \n",
        "        'district_id': 'Unknown_District', \n",
        "        'location': 'Unknown_Location',    \n",
        "        'locale_description': 'Unknown_Locale' \n",
        "    }\n",
        "    for col_name, fill_val in cat_cols_to_fill_info.items():\n",
        "        if col_name in merged_df.columns:\n",
        "            # Ensure column is categorical before filling, or cast it\n",
        "            if merged_df[col_name].dtype != pl.Categorical:\n",
        "                merged_df = merged_df.with_columns(pl.col(col_name).cast(pl.Categorical))\n",
        "                print(f\"Casted '{col_name}' to Categorical before fill_null.\")\n",
        "            specific_value_cleaning_expressions.append(pl.col(col_name).fill_null(fill_val).alias(col_name))\n",
        "            print(f\"Scheduled fill_null for categorical {col_name} with '{fill_val}'.\")\n",
        "    \n",
        "    if specific_value_cleaning_expressions:\n",
        "        print(\"\\nApplying specific value cleaning expressions...\")\n",
        "        merged_df = merged_df.with_columns(specific_value_cleaning_expressions)\n",
        "        print(\"Specific value cleaning applied.\")\n",
        "\n",
        "    # --- Dropping Columns ---\n",
        "    # Drop fully empty columns (likely from problems_df if not all actions had problem details)\n",
        "    # These are original column names from `problems_schema`.\n",
        "    empty_cols_candidates_from_problems = [] \n",
        "    pandas_identified_empty_cols = [\n",
        "         'problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task',\n",
        "         'first_response_or_request_time', 'first_answer', 'correct', 'reported_score',\n",
        "         'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given',\n",
        "         'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available',\n",
        "         'explanation_given', 'answer_given',\n",
        "         'assistments_reference_problem_log_id'\n",
        "    ]\n",
        "\n",
        "    actual_empty_cols_to_drop = []\n",
        "    if not merged_df.is_empty():\n",
        "        for col_name in pandas_identified_empty_cols:\n",
        "            if col_name in merged_df.columns and merged_df[col_name].is_null().all():\n",
        "                actual_empty_cols_to_drop.append(col_name)\n",
        "            elif col_name in merged_df.columns:\n",
        "                print(f\"Info: Column '{col_name}' (candidate for empty drop) was not fully null. Nulls: {merged_df[col_name].is_null().sum()}/{merged_df.height}\")\n",
        "    \n",
        "    if actual_empty_cols_to_drop:\n",
        "        print(f\"\\nDropping fully empty columns (identified from pandas analysis): {actual_empty_cols_to_drop}\")\n",
        "        merged_df = merged_df.drop(actual_empty_cols_to_drop)\n",
        "    else:\n",
        "        print(\"\\nNo fully empty columns (from the predefined list derived from pandas analysis) identified for dropping.\")\n",
        "\n",
        "    # Drop original 'opportunity_zone' if the boolean version 'opportunity_zone_bool' was created\n",
        "    if 'opportunity_zone' in merged_df.columns and 'opportunity_zone_bool' in merged_df.columns:\n",
        "        print(f\"Dropping original opportunity zone column: 'opportunity_zone'\")\n",
        "        merged_df = merged_df.drop('opportunity_zone')\n",
        "\n",
        "    print(f\"\\nShape after Cleaning: {merged_df.shape}\")\n",
        "    print(\"Columns after cleaning:\", merged_df.columns)\n",
        "    gc.collect()\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Cell 7 cleaning: merged_df not available or previous merge failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Reducing DataFrame to Essential Columns (Polars) ---\n",
            "Attempting to select these 18 essential columns: ['experiment_id', 'student_id', 'timestamp', 'action', 'action_log_id', 'assignment_start_time', 'assignment_end_time', 'assignment_log_id', 'assignment_session_count', 'condition_problem_count', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_total_correct', 'condition_total_attempt_count', 'condition_total_hints_given', 'condition_total_explanations_given', 'student_prior_average_correctness', 'opportunity_zone_bool']\n",
            "\n",
            "Reduced DataFrame Info (Polars): Shape (3711215, 18)\n",
            "\n",
            "Attempting to save cleaned and reduced Polars DataFrame to: /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "Successfully saved to /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "\n",
            "Full cleaned merged_df (Polars) deleted from memory.\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Create Reduced DataFrame and Save (Polars)\n",
        "\n",
        "if 'merged_df' in locals() and merged_df is not None and merge_successful: # merge_successful check might be redundant if merged_df exists\n",
        "    print(\"\\n--- Reducing DataFrame to Essential Columns (Polars) ---\")\n",
        "\n",
        "    # USER ACTION: Define this list with the EXACT final column names you want\n",
        "    # from the cleaned `merged_df` (output of Cell 7).\n",
        "    # Below is a *template* based on your original pandas variable names and common sense.\n",
        "    # YOU MUST VERIFY EACH COLUMN NAME AGAINST THE OUTPUT OF CELL 7.\n",
        "    essential_cols_to_keep_polars = [\n",
        "        # Keys / Base Info from actions_df (verify actual names after rename)\n",
        "        'experiment_id', \n",
        "        'student_id', \n",
        "        'timestamp', \n",
        "        'action', \n",
        "        'action_log_id', # Was 'assistments_reference_action_log_id'\n",
        "        \n",
        "        # From performance_df (verify actual names after rename and type cast)\n",
        "        'assignment_start_time', # Was 'start_time_perf'\n",
        "        'assignment_end_time',   # Was 'end_time_perf'\n",
        "        'assignment_log_id',     # Was 'assistments_reference_assignment_log_id'\n",
        "        'assignment_session_count', # Now UInt16\n",
        "        'condition_problem_count',  # Now UInt16\n",
        "        'condition_time_on_task',   # Float32\n",
        "        'condition_average_first_response_or_request_time', # Float32\n",
        "        'condition_total_correct',  # Now UInt16\n",
        "        'condition_total_attempt_count', # Now UInt32\n",
        "        'condition_total_hints_given', # Now UInt32\n",
        "        'condition_total_explanations_given', # Now UInt32\n",
        "        \n",
        "        # From metrics_df (verify actual names)\n",
        "        'student_prior_average_correctness', \n",
        "        \n",
        "        # Created in Cell 7\n",
        "        'opportunity_zone_bool', \n",
        "        \n",
        "        # Other columns you might need from your original list of 71:\n",
        "        # e.g., 'pretest_correct', 'posttest_correct', 'assigned_condition', 'class_id', 'teacher_id'\n",
        "        # 'release_date', 'due_date', \n",
        "        # 'experiment_tag_path'\n",
        "        # Add any other columns from the 71 that are essential for your next steps.\n",
        "    ]\n",
        "    \n",
        "    # Filter to only include columns that actually exist in the cleaned merged_df\n",
        "    final_essential_columns = [col for col in essential_cols_to_keep_polars if col in merged_df.columns]\n",
        "    \n",
        "    print(f\"Attempting to select these {len(final_essential_columns)} essential columns: {final_essential_columns}\")\n",
        "    missing_essentials_for_reduction = [col for col in essential_cols_to_keep_polars if col not in final_essential_columns]\n",
        "\n",
        "    if missing_essentials_for_reduction:\n",
        "        print(f\"Warning: The following conceptual essential columns were NOT FOUND in merged_df for reduction: {missing_essentials_for_reduction}\")\n",
        "        print(\"Please ensure their names are correct in the 'essential_cols_to_keep_polars' list and they exist in the output of Cell 7.\")\n",
        "    \n",
        "    if not final_essential_columns:\n",
        "        print(\"Error: No essential columns available for selection based on your list. Cannot create reduced DataFrame.\")\n",
        "        merged_df_reduced = None\n",
        "    else:\n",
        "        try:\n",
        "            merged_df_reduced = merged_df.select(final_essential_columns)\n",
        "            print(f\"\\nReduced DataFrame Info (Polars): Shape {merged_df_reduced.shape}\")\n",
        "            # print(merged_df_reduced.head())\n",
        "            # print(merged_df_reduced.schema) # Good to check the schema of the final saved df\n",
        "\n",
        "            print(f\"\\nAttempting to save cleaned and reduced Polars DataFrame to: {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "            merged_df_reduced.write_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET) # Parquet is preferred\n",
        "            print(f\"Successfully saved to {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error during final select or save: {e}\")\n",
        "            merged_df_reduced = None\n",
        "            \n",
        "    if 'merged_df' in locals(): \n",
        "        del merged_df # Free up memory from the full cleaned DataFrame\n",
        "        gc.collect()\n",
        "        print(\"\\nFull cleaned merged_df (Polars) deleted from memory.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Cell 8 (reduction and save): merged_df not available from Cell 7 or previous steps failed.\")\n",
        "    merged_df_reduced = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading Cleaned Parquet File (Polars) ---\n",
            "Successfully reloaded: /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "Reloaded DataFrame Shape: (3711215, 18)\n",
            "\n",
            "Reloaded DataFrame Head (from Parquet):\n",
            "shape: (5, 18)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
            "│ experimen ┆ student_i ┆ timestamp ┆ action    ┆ … ┆ condition ┆ condition ┆ student_p ┆ opportun │\n",
            "│ t_id      ┆ d         ┆ ---       ┆ ---       ┆   ┆ _total_hi ┆ _total_ex ┆ rior_aver ┆ ity_zone │\n",
            "│ ---       ┆ ---       ┆ datetime[ ┆ cat       ┆   ┆ nts_given ┆ planation ┆ age_corre ┆ _bool    │\n",
            "│ cat       ┆ cat       ┆ μs, UTC]  ┆           ┆   ┆ ---       ┆ s_g…      ┆ ctn…      ┆ ---      │\n",
            "│           ┆           ┆           ┆           ┆   ┆ u32       ┆ ---       ┆ ---       ┆ bool     │\n",
            "│           ┆           ┆           ┆           ┆   ┆           ┆ u32       ┆ f32       ┆          │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ assignmen ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:41:4 ┆ t_started ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 6.393 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ problem_s ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:41:4 ┆ tarted    ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 7.437 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ correct_r ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:42:0 ┆ esponse   ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 3.665 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ problem_f ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:42:0 ┆ inished   ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 3.675 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ 2021-03-2 ┆ continue_ ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆ 4 16:42:0 ┆ selected  ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆ 5.295 UTC ┆           ┆   ┆           ┆           ┆           ┆          │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
            "\n",
            "Reloaded DataFrame Schema (from Parquet):\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'action': Categorical(ordering='physical'), 'action_log_id': UInt64, 'assignment_start_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_end_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_log_id': UInt64, 'assignment_session_count': UInt16, 'condition_problem_count': UInt16, 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_total_correct': UInt16, 'condition_total_attempt_count': UInt32, 'condition_total_hints_given': UInt32, 'condition_total_explanations_given': UInt32, 'student_prior_average_correctness': Float32, 'opportunity_zone_bool': Boolean})\n"
          ]
        }
      ],
      "source": [
        "# Cell 9: Load Cleaned Data (Polars) - Optional\n",
        "\n",
        "if 'merged_df_reduced' in locals() and merged_df_reduced is not None and not merged_df_reduced.is_empty() and SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "    print(f\"\\n--- Loading Cleaned Parquet File (Polars) ---\")\n",
        "    try:\n",
        "        df_reloaded_polars = pl.read_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET)\n",
        "        print(f\"Successfully reloaded: {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "        print(f\"Reloaded DataFrame Shape: {df_reloaded_polars.shape}\")\n",
        "        print(\"\\nReloaded DataFrame Head (from Parquet):\")\n",
        "        print(df_reloaded_polars.head())\n",
        "        print(\"\\nReloaded DataFrame Schema (from Parquet):\")\n",
        "        print(df_reloaded_polars.schema)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reloading the cleaned Parquet file: {e}\")\n",
        "elif SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "     print(f\"Cleaned Parquet file found at {SAVE_CLEANED_PATH_POLARS_PARQUET}, but merged_df_reduced may not have been successfully created or was empty in the previous step (this script might have been re-run starting from here). Consider reloading manually if needed.\")\n",
        "else:\n",
        "    print(f\"\\nCleaned Parquet file not found at {SAVE_CLEANED_PATH_POLARS_PARQUET} or reduction/save step failed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
