{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Research Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Educational researchers and data scientists face significant challenges in identifying struggling students early enough to provide timely interventions in online learning environments. While there are extensive data available, extracting meaningful predictive signals from student interactions remains difficult, particularly within the first portion of assignments when intervention would be most valuable. [The ASSISTments dataset](https://osf.io/59shv/files/osfstorage) provides a unique opportunity to address this challenge through its comprehensive data from 88 distinct assignment-level randomized controlled experiments conducted within [the ASSISTments platform](https://www.assistments.org/). This collection, analyzed initially in [Prihar et al.'s 2022 paper *Exploring Common Trends in Online Educational Experiments*](https://osf.io/f58dz), includes detailed clickstream data that captures temporal aspects of student engagement across diverse educational interventions. The rich multi-level student interaction data enables the development and evaluation of early warning systems that could identify struggling students before they fall significantly behind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Research Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How can temporal engagement features derived from clickstream data in the ASSISTments experimental dataset predict student performance drops across different intervention types, and which feature selection methods most effectively identify at-risk students within the first 25% of an assignment? This research will leverage the dataset's granular student interaction logs to extract time-based engagement patterns, analyze how these patterns correlate with performance outcomes, and determine which combinations of features provide the earliest reliable signals of academic struggle across different intervention conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ9f09tc34r0"
      },
      "source": [
        "# Load, Merge, and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polars version: 1.29.0\n",
            "Base data path: /Users/john/Downloads/osfstorage-archive\n",
            "Experiment IDs path: /Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23\n",
            "Global String Cache enabled: True\n"
          ]
        }
      ],
      "source": [
        "# Setup and Configuration\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "import gc\n",
        "\n",
        "# --- Enable Global String Cache for Categoricals ---\n",
        "pl.enable_string_cache()\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_DATA_PATH = Path('/Users/john/Downloads/osfstorage-archive')\n",
        "EXPERIMENT_IDS_PATH = BASE_DATA_PATH / 'experiment_dataset_2021-09-23'\n",
        "\n",
        "# Output path for the cleaned data\n",
        "SAVE_CLEANED_PATH_POLARS_PARQUET = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.parquet'\n",
        "SAVE_CLEANED_PATH_POLARS_CSV = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.csv'\n",
        "\n",
        "print(f\"Polars version: {pl.__version__}\")\n",
        "print(f\"Base data path: {BASE_DATA_PATH}\")\n",
        "print(f\"Experiment IDs path: {EXPERIMENT_IDS_PATH}\")\n",
        "print(f\"Global String Cache enabled: {pl.using_string_cache()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 88 experiment ID directories.\n",
            "Sample performance file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_alogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_alogs.csv']\n",
            "Sample problems file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_plogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_plogs.csv']\n",
            "Sample actions file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_slogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_slogs.csv']\n",
            "Sample metrics file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/priors.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/priors.csv']\n"
          ]
        }
      ],
      "source": [
        "# Generate File Paths\n",
        "try:\n",
        "    if not EXPERIMENT_IDS_PATH.is_dir():\n",
        "        raise FileNotFoundError(f\"Error: Directory not found at {EXPERIMENT_IDS_PATH}\")\n",
        "    experiment_ids = [d.name for d in EXPERIMENT_IDS_PATH.iterdir() if d.is_dir()]\n",
        "    print(f\"Found {len(experiment_ids)} experiment ID directories.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    experiment_ids = []\n",
        "\n",
        "performance_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_alogs.csv') for exp_id in experiment_ids]\n",
        "problems_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_plogs.csv') for exp_id in experiment_ids]\n",
        "actions_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_slogs.csv') for exp_id in experiment_ids]\n",
        "metrics_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'priors.csv') for exp_id in experiment_ids]\n",
        "\n",
        "print(\"Sample performance file paths:\", performance_file_paths[:2])\n",
        "print(\"Sample problems file paths:\", problems_file_paths[:2])\n",
        "print(\"Sample actions file paths:\", actions_file_paths[:2])\n",
        "print(\"Sample metrics file paths:\", metrics_file_paths[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Schemas and Date Parsing Information\n",
        "\n",
        "actions_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'problem_id': pl.Categorical,\n",
        "    'problem_part': pl.Categorical, \n",
        "    'scaffold_id': pl.Categorical,  \n",
        "    'experiment_tag_path': pl.Utf8,\n",
        "    'action': pl.Categorical,\n",
        "    'timestamp': pl.Utf8,\n",
        "    'assistments_reference_action_log_id': pl.UInt64\n",
        "}\n",
        "actions_parse_dates = ['timestamp']\n",
        "\n",
        "problems_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'problem_id': pl.Categorical,\n",
        "    'problem_part': pl.Categorical, \n",
        "    'scaffold_id': pl.Categorical,  \n",
        "    'problem_condition': pl.Categorical,\n",
        "    'start_time': pl.Utf8,\n",
        "    'end_time': pl.Utf8,\n",
        "    'session_count': pl.UInt16,\n",
        "    'time_on_task': pl.Float32,\n",
        "    'first_response_or_request_time': pl.Float32,\n",
        "    'first_answer': pl.Utf8,\n",
        "    'correct': pl.Boolean,\n",
        "    'reported_score': pl.Float32,\n",
        "    'answer_before_tutoring': pl.Boolean,\n",
        "    'attempt_count': pl.UInt16,\n",
        "    'hints_available': pl.UInt16,\n",
        "    'hints_given': pl.UInt16,\n",
        "    'scaffold_problems_available': pl.UInt16,\n",
        "    'scaffold_problems_given': pl.UInt16,\n",
        "    'explanation_available': pl.Boolean,\n",
        "    'explanation_given': pl.Boolean,\n",
        "    'answer_given': pl.Boolean,\n",
        "    'assistments_reference_problem_log_id': pl.UInt64\n",
        "}\n",
        "problems_parse_dates = ['start_time', 'end_time']\n",
        "\n",
        "performance_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'release_date': pl.Utf8,\n",
        "    'due_date': pl.Utf8,\n",
        "    'start_time': pl.Utf8,\n",
        "    'end_time': pl.Utf8,\n",
        "    'assignment_session_count': pl.Float32,\n",
        "    'pretest_problem_count': pl.Float32,\n",
        "    'pretest_correct': pl.Float32,\n",
        "    'pretest_time_on_task': pl.Float32,\n",
        "    'pretest_average_first_response_time': pl.Float32,\n",
        "    'pretest_session_count': pl.Float32,\n",
        "    'assigned_condition': pl.Categorical,\n",
        "    'condition_time_on_task': pl.Float32,\n",
        "    'condition_average_first_response_or_request_time': pl.Float32,\n",
        "    'condition_problem_count': pl.Float32,\n",
        "    'condition_total_correct': pl.Float32,\n",
        "    'condition_total_correct_after_wrong_response': pl.Float32,\n",
        "    'condition_total_correct_after_tutoring': pl.Float32,\n",
        "    'condition_total_answers_before_tutoring': pl.Float32,\n",
        "    'condition_total_attempt_count': pl.Float32,\n",
        "    'condition_total_hints_available': pl.Float32,\n",
        "    'condition_total_hints_given': pl.Float32,\n",
        "    'condition_total_scaffold_problems_available': pl.Float32,\n",
        "    'condition_total_scaffold_problems_given': pl.Float32,\n",
        "    'condition_total_explanations_available': pl.Float32,\n",
        "    'condition_total_explanations_given': pl.Float32,\n",
        "    'condition_total_answers_given': pl.Float32,\n",
        "    'condition_session_count': pl.Float32,\n",
        "    'posttest_problem_count': pl.Float32,\n",
        "    'posttest_correct': pl.Float32,\n",
        "    'posttest_time_on_task': pl.Float32,\n",
        "    'posttest_average_first_response_time': pl.Float32,\n",
        "    'posttest_session_count': pl.Float32,\n",
        "    'assistments_reference_assignment_log_id': pl.UInt64\n",
        "}\n",
        "performance_parse_dates = ['release_date', 'due_date', 'start_time', 'end_time']\n",
        "\n",
        "metrics_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'student_prior_started_skill_builder_count': pl.UInt32,\n",
        "    'student_prior_completed_skill_builder_count': pl.UInt32,\n",
        "    'student_prior_started_problem_set_count': pl.UInt32,\n",
        "    'student_prior_completed_problem_set_count': pl.UInt32,\n",
        "    'student_prior_completed_problem_count': pl.UInt32,\n",
        "    'student_prior_median_first_response_time': pl.Float32,\n",
        "    'student_prior_median_time_on_task': pl.Float32,\n",
        "    'student_prior_average_correctness': pl.Float32,\n",
        "    'student_prior_average_attempt_count': pl.Float32,\n",
        "    'class_id': pl.Categorical,\n",
        "    'class_creation_date': pl.Utf8,\n",
        "    'class_student_count': pl.UInt16,\n",
        "    'class_prior_skill_builder_count': pl.UInt32,\n",
        "    'class_prior_problem_set_count': pl.UInt32,\n",
        "    'class_prior_skill_builder_percent_started': pl.Float32,\n",
        "    'class_prior_skill_builder_percent_completed': pl.Float32,\n",
        "    'class_prior_problem_set_percent_started': pl.Float32,\n",
        "    'class_prior_problem_set_percent_completed': pl.Float32,\n",
        "    'class_prior_completed_problem_count': pl.UInt32,\n",
        "    'class_prior_median_time_on_task': pl.Float32,\n",
        "    'class_prior_median_first_response_time': pl.Float32,\n",
        "    'class_prior_average_correctness': pl.Float32,\n",
        "    'class_prior_average_attempt_count': pl.Float32,\n",
        "    'teacher id': pl.Categorical, \n",
        "    'teacher_account_creation_date': pl.Utf8,\n",
        "    'district_id': pl.Categorical,\n",
        "    'location': pl.Categorical,\n",
        "    'opportunity_zone': pl.Categorical,\n",
        "    'locale_description': pl.Categorical\n",
        "}\n",
        "metrics_parse_dates = ['class_creation_date', 'teacher_account_creation_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper Function for Memory-Efficient CSV Concatenation\n",
        "\n",
        "def combine_polars_csvs(file_paths, schema=None, parse_dates_list=None,\n",
        "                        known_date_format_str: str = None,\n",
        "                        date_time_unit='us'):\n",
        "    lazy_frames = []\n",
        "    print(f\"\\nScanning {len(file_paths)} files...\")\n",
        "\n",
        "    common_columns_from_first_file = None\n",
        "    if file_paths and schema:\n",
        "        try:\n",
        "            common_columns_from_first_file = pl.scan_csv(\n",
        "                file_paths[0], infer_schema_length=100, n_rows=10\n",
        "            ).collect_schema().names()\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not determine common columns from first file {file_paths[0]}: {e}\")\n",
        "            common_columns_from_first_file = list(schema.keys())\n",
        "\n",
        "    problematic_file_for_date_parse = None\n",
        "    current_col_for_date_parse = \"unknown\"\n",
        "\n",
        "    for i, file_path_str in enumerate(file_paths):\n",
        "        file_path = Path(file_path_str)\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Scanning file {i+1}/{len(file_paths)}: {file_path.parent.name}/{file_path.name}\")\n",
        "\n",
        "        try:\n",
        "            lf = pl.scan_csv(file_path,\n",
        "                             schema=schema,\n",
        "                             infer_schema_length=100,\n",
        "                             null_values=[\"\", \"NA\", \"NaN\", \"null\"])\n",
        "\n",
        "            if parse_dates_list:\n",
        "                date_parsing_expressions = []\n",
        "                columns_to_check_for_dates = common_columns_from_first_file if common_columns_from_first_file else lf.collect_schema().names()\n",
        "\n",
        "                for col_name in parse_dates_list:\n",
        "                    current_col_for_date_parse = col_name\n",
        "                    if col_name in columns_to_check_for_dates:\n",
        "                        if col_name not in lf.collect_schema().names():\n",
        "                            continue\n",
        "\n",
        "                        date_expr = pl.col(col_name).cast(pl.Utf8, strict=False)\n",
        "\n",
        "                        if known_date_format_str:\n",
        "                            date_expr = date_expr.str.to_datetime(\n",
        "                                format=known_date_format_str,\n",
        "                                strict=False,\n",
        "                                time_unit=date_time_unit\n",
        "                            )\n",
        "                        else:\n",
        "                            date_expr = date_expr.str.to_datetime(\n",
        "                                strict=False,\n",
        "                                time_unit=date_time_unit\n",
        "                            )\n",
        "                        date_parsing_expressions.append(\n",
        "                            date_expr.dt.convert_time_zone(\"UTC\").alias(col_name)\n",
        "                        )\n",
        "                if date_parsing_expressions:\n",
        "                    lf = lf.with_columns(date_parsing_expressions)\n",
        "\n",
        "            lazy_frames.append(lf)\n",
        "            problematic_file_for_date_parse = None\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  Warning: File not found, skipping: {file_path}\")\n",
        "        except pl.exceptions.NoDataError:\n",
        "             print(f\"  Warning: File is empty, skipping: {file_path}\")\n",
        "        except Exception as e:\n",
        "            problematic_file_for_date_parse = file_path\n",
        "            if \"strptime\" in str(e).lower() or \"conversion\" in str(e).lower() or \"datetime\" in str(e).lower():\n",
        "                 print(f\"  Potential date parsing error for {problematic_file_for_date_parse} (column likely '{current_col_for_date_parse}'): {e}\")\n",
        "            else:\n",
        "                print(f\"  Error scanning {file_path} or applying initial transforms: {e}\")\n",
        "\n",
        "\n",
        "    if not lazy_frames:\n",
        "        print(\"  No lazy frames were created from scanning files.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Concatenating {len(lazy_frames)} lazy frames...\")\n",
        "    try:\n",
        "        combined_lf = pl.concat(lazy_frames, how=\"vertical_relaxed\")\n",
        "        print(\"Collecting data into DataFrame (streaming enabled)...\")\n",
        "        # Reverted to engine=\"streaming\" as per deprecation warning for Polars 1.29.0\n",
        "        collected_df = combined_lf.collect(engine=\"streaming\")\n",
        "        print(\"Concatenation and collection complete.\")\n",
        "        return collected_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error during lazy concatenation or collection: {e}\")\n",
        "        if problematic_file_for_date_parse:\n",
        "            print(f\"  This might be related to an earlier issue in file: {problematic_file_for_date_parse}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combining Actions Data (exp_slogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_slogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_slogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_slogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_slogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_slogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_slogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_slogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_slogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_slogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Actions DataFrame shape: (3708299, 9)\n",
            "\n",
            "Combining Problems Data (exp_plogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_plogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_plogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_plogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_plogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_plogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_plogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_plogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_plogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_plogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Problems DataFrame shape: (771386, 24)\n",
            "\n",
            "Combining Performance Data (exp_alogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_alogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_alogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_alogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_alogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_alogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_alogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_alogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_alogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_alogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Performance DataFrame shape: (95990, 35)\n",
            "\n",
            "Combining Metrics Data (priors)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/priors.csv\n",
            "  Scanning file 11/88: PSAYCFH/priors.csv\n",
            "  Scanning file 21/88: PSAZ2G4/priors.csv\n",
            "  Scanning file 31/88: PSAQJFP/priors.csv\n",
            "  Scanning file 41/88: PSAJVP8/priors.csv\n",
            "  Scanning file 51/88: PSA9XWV/priors.csv\n",
            "  Scanning file 61/88: PSAM4NK/priors.csv\n",
            "  Scanning file 71/88: PSATP2Z/priors.csv\n",
            "  Scanning file 81/88: PSASDZY/priors.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Metrics DataFrame shape: (95979, 31)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load DataFrames\n",
        "\n",
        "COMMON_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S%.f%z\" \n",
        "\n",
        "print(\"Combining Actions Data (exp_slogs)...\")\n",
        "actions_df = combine_polars_csvs(\n",
        "    actions_file_paths, \n",
        "    schema=actions_schema, \n",
        "    parse_dates_list=actions_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT \n",
        ")\n",
        "if actions_df is not None:\n",
        "    print(f\"Actions DataFrame shape: {actions_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Problems Data (exp_plogs)...\")\n",
        "problems_df = combine_polars_csvs(\n",
        "    problems_file_paths, \n",
        "    schema=problems_schema, \n",
        "    parse_dates_list=problems_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT \n",
        ")\n",
        "if problems_df is not None:\n",
        "    print(f\"Problems DataFrame shape: {problems_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Performance Data (exp_alogs)...\")\n",
        "performance_df = combine_polars_csvs(\n",
        "    performance_file_paths, \n",
        "    schema=performance_schema, \n",
        "    parse_dates_list=performance_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT\n",
        ")\n",
        "if performance_df is not None:\n",
        "    print(f\"Performance DataFrame shape: {performance_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Metrics Data (priors)...\")\n",
        "if 'teacher id' in metrics_schema: \n",
        "    metrics_schema_corrected = metrics_schema.copy()\n",
        "    metrics_schema_corrected['teacher_id'] = metrics_schema_corrected.pop('teacher id')\n",
        "else:\n",
        "    metrics_schema_corrected = metrics_schema\n",
        "\n",
        "metrics_df = combine_polars_csvs(\n",
        "    metrics_file_paths, \n",
        "    schema=metrics_schema_corrected, \n",
        "    parse_dates_list=metrics_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT\n",
        ")\n",
        "if metrics_df is not None:\n",
        "    if 'teacher id' in metrics_df.columns: \n",
        "        metrics_df = metrics_df.rename({'teacher id': 'teacher_id'})\n",
        "    print(f\"Metrics DataFrame shape: {metrics_df.shape}\")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Merge Operations ---\n",
            "Starting with actions_df: (3708299, 9)\n",
            "Merging problems_df...\n",
            "After merging problems_df: (3708299, 28)\n",
            "Merging performance_df...\n",
            "After merging performance_df: (3711215, 61)\n",
            "Merging metrics_df...\n",
            "After merging metrics_df: (3711215, 90)\n",
            "\n",
            "--- Merge Complete ---\n",
            "Final Merged DataFrame Info:\n",
            "Shape: (3711215, 90)\n",
            "Columns in merged_df: ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id', 'experiment_tag_path', 'action', 'timestamp', 'assistments_reference_action_log_id', 'problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task', 'first_response_or_request_time', 'first_answer', 'correct', 'reported_score', 'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given', 'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available', 'explanation_given', 'answer_given', 'assistments_reference_problem_log_id', 'release_date', 'due_date', 'start_time_perf', 'end_time_perf', 'assignment_session_count', 'pretest_problem_count', 'pretest_correct', 'pretest_time_on_task', 'pretest_average_first_response_time', 'pretest_session_count', 'assigned_condition', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_problem_count', 'condition_total_correct', 'condition_total_correct_after_wrong_response', 'condition_total_correct_after_tutoring', 'condition_total_answers_before_tutoring', 'condition_total_attempt_count', 'condition_total_hints_available', 'condition_total_hints_given', 'condition_total_scaffold_problems_available', 'condition_total_scaffold_problems_given', 'condition_total_explanations_available', 'condition_total_explanations_given', 'condition_total_answers_given', 'condition_session_count', 'posttest_problem_count', 'posttest_correct', 'posttest_time_on_task', 'posttest_average_first_response_time', 'posttest_session_count', 'assistments_reference_assignment_log_id', 'student_prior_started_skill_builder_count', 'student_prior_completed_skill_builder_count', 'student_prior_started_problem_set_count', 'student_prior_completed_problem_set_count', 'student_prior_completed_problem_count', 'student_prior_median_first_response_time', 'student_prior_median_time_on_task', 'student_prior_average_correctness', 'student_prior_average_attempt_count', 'class_id', 'class_creation_date', 'class_student_count', 'class_prior_skill_builder_count', 'class_prior_problem_set_count', 'class_prior_skill_builder_percent_started', 'class_prior_skill_builder_percent_completed', 'class_prior_problem_set_percent_started', 'class_prior_problem_set_percent_completed', 'class_prior_completed_problem_count', 'class_prior_median_time_on_task', 'class_prior_median_first_response_time', 'class_prior_average_correctness', 'class_prior_average_attempt_count', 'teacher_account_creation_date', 'district_id', 'location', 'opportunity_zone', 'locale_description', 'teacher_id']\n",
            "<bound method DataFrame.head of shape: (3_711_215, 90)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
            "│ experimen ┆ student_i ┆ problem_i ┆ problem_p ┆ … ┆ location ┆ opportuni ┆ locale_de ┆ teacher_i │\n",
            "│ t_id      ┆ d         ┆ d         ┆ art       ┆   ┆ ---      ┆ ty_zone   ┆ scription ┆ d         │\n",
            "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ cat      ┆ ---       ┆ ---       ┆ ---       │\n",
            "│ cat       ┆ cat       ┆ cat       ┆ cat       ┆   ┆          ┆ cat       ┆ cat       ┆ cat       │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ null      ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 1.0       ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 1.0       ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 1.0       ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ null      ┆ … ┆ 17.0     ┆ Alabama   ┆ Yes       ┆ Town:     │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆ Distant   │\n",
            "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …        ┆ …         ┆ …         ┆ …         │\n",
            "│ PSA2KP9   ┆ 992517    ┆ PRABAWMQ  ┆ 1.0       ┆ … ┆ 1335.0   ┆ North     ┆ No        ┆ None      │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆ Carolina  ┆           ┆           │\n",
            "│ PSA2KP9   ┆ 992517    ┆ PRABAWMQ  ┆ 1.0       ┆ … ┆ 1335.0   ┆ North     ┆ No        ┆ None      │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆ Carolina  ┆           ┆           │\n",
            "│ PSA2KP9   ┆ 992517    ┆ PRABAWMQ  ┆ 1.0       ┆ … ┆ 1335.0   ┆ North     ┆ No        ┆ None      │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆ Carolina  ┆           ┆           │\n",
            "│ PSA2KP9   ┆ 992517    ┆ null      ┆ null      ┆ … ┆ 1335.0   ┆ North     ┆ No        ┆ None      │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆ Carolina  ┆           ┆           │\n",
            "│ PSA2KP9   ┆ 992517    ┆ PRAJDU2   ┆ 1.0       ┆ … ┆ 1335.0   ┆ North     ┆ No        ┆ None      │\n",
            "│           ┆           ┆           ┆           ┆   ┆          ┆ Carolina  ┆           ┆           │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴──────────┴───────────┴───────────┴───────────┘>\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'problem_id': Categorical(ordering='physical'), 'problem_part': Categorical(ordering='physical'), 'scaffold_id': Categorical(ordering='physical'), 'experiment_tag_path': String, 'action': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'assistments_reference_action_log_id': UInt64, 'problem_condition': Categorical(ordering='physical'), 'start_time': Datetime(time_unit='us', time_zone='UTC'), 'end_time': Datetime(time_unit='us', time_zone='UTC'), 'session_count': UInt16, 'time_on_task': Float32, 'first_response_or_request_time': Float32, 'first_answer': String, 'correct': Boolean, 'reported_score': Float32, 'answer_before_tutoring': Boolean, 'attempt_count': UInt16, 'hints_available': UInt16, 'hints_given': UInt16, 'scaffold_problems_available': UInt16, 'scaffold_problems_given': UInt16, 'explanation_available': Boolean, 'explanation_given': Boolean, 'answer_given': Boolean, 'assistments_reference_problem_log_id': UInt64, 'release_date': Datetime(time_unit='us', time_zone='UTC'), 'due_date': Datetime(time_unit='us', time_zone='UTC'), 'start_time_perf': Datetime(time_unit='us', time_zone='UTC'), 'end_time_perf': Datetime(time_unit='us', time_zone='UTC'), 'assignment_session_count': Float32, 'pretest_problem_count': Float32, 'pretest_correct': Float32, 'pretest_time_on_task': Float32, 'pretest_average_first_response_time': Float32, 'pretest_session_count': Float32, 'assigned_condition': Categorical(ordering='physical'), 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_problem_count': Float32, 'condition_total_correct': Float32, 'condition_total_correct_after_wrong_response': Float32, 'condition_total_correct_after_tutoring': Float32, 'condition_total_answers_before_tutoring': Float32, 'condition_total_attempt_count': Float32, 'condition_total_hints_available': Float32, 'condition_total_hints_given': Float32, 'condition_total_scaffold_problems_available': Float32, 'condition_total_scaffold_problems_given': Float32, 'condition_total_explanations_available': Float32, 'condition_total_explanations_given': Float32, 'condition_total_answers_given': Float32, 'condition_session_count': Float32, 'posttest_problem_count': Float32, 'posttest_correct': Float32, 'posttest_time_on_task': Float32, 'posttest_average_first_response_time': Float32, 'posttest_session_count': Float32, 'assistments_reference_assignment_log_id': UInt64, 'student_prior_started_skill_builder_count': UInt32, 'student_prior_completed_skill_builder_count': UInt32, 'student_prior_started_problem_set_count': UInt32, 'student_prior_completed_problem_set_count': UInt32, 'student_prior_completed_problem_count': UInt32, 'student_prior_median_first_response_time': Float32, 'student_prior_median_time_on_task': Float32, 'student_prior_average_correctness': Float32, 'student_prior_average_attempt_count': Float32, 'class_id': Categorical(ordering='physical'), 'class_creation_date': Datetime(time_unit='us', time_zone='UTC'), 'class_student_count': UInt16, 'class_prior_skill_builder_count': UInt32, 'class_prior_problem_set_count': UInt32, 'class_prior_skill_builder_percent_started': Float32, 'class_prior_skill_builder_percent_completed': Float32, 'class_prior_problem_set_percent_started': Float32, 'class_prior_problem_set_percent_completed': Float32, 'class_prior_completed_problem_count': UInt32, 'class_prior_median_time_on_task': Float32, 'class_prior_median_first_response_time': Float32, 'class_prior_average_correctness': Float32, 'class_prior_average_attempt_count': Float32, 'teacher_account_creation_date': Datetime(time_unit='us', time_zone='UTC'), 'district_id': Categorical(ordering='physical'), 'location': Categorical(ordering='physical'), 'opportunity_zone': Categorical(ordering='physical'), 'locale_description': Categorical(ordering='physical'), 'teacher_id': Categorical(ordering='physical')})\n"
          ]
        }
      ],
      "source": [
        "# Merge DataFrames into One\n",
        "\n",
        "merged_df = None\n",
        "merge_successful = True\n",
        "\n",
        "print(\"\\n--- Starting Merge Operations ---\")\n",
        "\n",
        "if actions_df is None or actions_df.is_empty():\n",
        "    print(\"Actions DataFrame is empty or None. Cannot proceed with merge.\")\n",
        "    merge_successful = False\n",
        "else:\n",
        "    merged_df = actions_df.clone()\n",
        "    print(f\"Starting with actions_df: {merged_df.shape}\")\n",
        "\n",
        "    # Merge Problems Data\n",
        "    if problems_df is not None and not problems_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging problems_df...\")\n",
        "            problem_keys = ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id']\n",
        "            merged_df = merged_df.join(problems_df, on=problem_keys, how=\"left\", suffix=\"_problem\")\n",
        "            print(f\"After merging problems_df: {merged_df.shape}\")\n",
        "            del problems_df \n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging problems_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful: \n",
        "        print(\"Skipping problems_df merge (not loaded or empty).\")\n",
        "\n",
        "    # Merge Performance Data\n",
        "    if performance_df is not None and not performance_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging performance_df...\")\n",
        "            perf_keys = ['experiment_id', 'student_id']\n",
        "            merged_df = merged_df.join(performance_df, on=perf_keys, how=\"left\", suffix=\"_perf\")\n",
        "            print(f\"After merging performance_df: {merged_df.shape}\")\n",
        "            del performance_df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging performance_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful:\n",
        "        print(\"Skipping performance_df merge (not loaded or empty).\")\n",
        "\n",
        "    # Merge Metrics Data\n",
        "    if metrics_df is not None and not metrics_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging metrics_df...\")\n",
        "            metrics_keys = ['experiment_id', 'student_id']\n",
        "            merged_df = merged_df.join(metrics_df, on=metrics_keys, how=\"left\", suffix=\"_metrics\")\n",
        "            print(f\"After merging metrics_df: {merged_df.shape}\")\n",
        "            del metrics_df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging metrics_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful:\n",
        "        print(\"Skipping metrics_df merge (not loaded or empty).\")\n",
        "\n",
        "    if merged_df is not None and merge_successful:\n",
        "        print(\"\\n--- Merge Complete ---\")\n",
        "        print(\"Final Merged DataFrame Info:\")\n",
        "        print(f\"Shape: {merged_df.shape}\")\n",
        "        print(\"Columns in merged_df:\", merged_df.columns)\n",
        "        print(merged_df.head)\n",
        "        print(merged_df.schema)\n",
        "        \n",
        "        if 'actions_df' in locals() and actions_df is not merged_df: \n",
        "            del actions_df\n",
        "            gc.collect()\n",
        "            \n",
        "    elif merged_df is not None: \n",
        "        print(\"\\n--- Merge Partially Complete or Some DataFrames Skipped ---\")\n",
        "        print(\"Columns in partially merged_df:\", merged_df.columns)\n",
        "    else: \n",
        "        print(\"\\n--- Merge Failed or Base DataFrame (actions_df) was not suitable ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8NxlbW3ynlT"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Data Cleaning ---\n",
            "Initial merged_df shape for cleaning: (3711215, 90)\n",
            "\n",
            "--- Renaming Columns ---\n",
            "Applying renames: {'assistments_reference_action_log_id': 'action_log_id', 'start_time_perf': 'assignment_start_time', 'end_time_perf': 'assignment_end_time', 'assistments_reference_assignment_log_id': 'assignment_log_id'}\n",
            "Scheduled for categorical conversion: experiment_tag_path\n",
            "Scheduled 'assignment_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct_after_wrong_response' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct_after_tutoring' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_answers_before_tutoring' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_attempt_count' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_hints_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_hints_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_scaffold_problems_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_scaffold_problems_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_explanations_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_explanations_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_answers_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_session_count' for Float32 to UInt16 conversion.\n",
            "\n",
            "Applying column type transformations...\n",
            "Type transformations applied.\n",
            "\n",
            "--- Specific Value Cleaning ---\n",
            "Scheduled 'opportunity_zone' to boolean 'opportunity_zone_bool' conversion.\n",
            "Scheduled fill_null for categorical district_id with 'Unknown_District'.\n",
            "Scheduled fill_null for categorical location with 'Unknown_Location'.\n",
            "Scheduled fill_null for categorical locale_description with 'Unknown_Locale'.\n",
            "\n",
            "Applying specific value cleaning expressions...\n",
            "Specific value cleaning applied.\n",
            "\n",
            "Dropping fully empty columns: ['problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task', 'first_response_or_request_time', 'first_answer', 'correct', 'reported_score', 'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given', 'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available', 'explanation_given', 'answer_given', 'assistments_reference_problem_log_id']\n",
            "Dropping original opportunity zone column: 'opportunity_zone'\n",
            "\n",
            "Shape after Cleaning: (3711215, 71)\n",
            "Columns after cleaning: ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id', 'experiment_tag_path', 'action', 'timestamp', 'action_log_id', 'release_date', 'due_date', 'assignment_start_time', 'assignment_end_time', 'assignment_session_count', 'pretest_problem_count', 'pretest_correct', 'pretest_time_on_task', 'pretest_average_first_response_time', 'pretest_session_count', 'assigned_condition', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_problem_count', 'condition_total_correct', 'condition_total_correct_after_wrong_response', 'condition_total_correct_after_tutoring', 'condition_total_answers_before_tutoring', 'condition_total_attempt_count', 'condition_total_hints_available', 'condition_total_hints_given', 'condition_total_scaffold_problems_available', 'condition_total_scaffold_problems_given', 'condition_total_explanations_available', 'condition_total_explanations_given', 'condition_total_answers_given', 'condition_session_count', 'posttest_problem_count', 'posttest_correct', 'posttest_time_on_task', 'posttest_average_first_response_time', 'posttest_session_count', 'assignment_log_id', 'student_prior_started_skill_builder_count', 'student_prior_completed_skill_builder_count', 'student_prior_started_problem_set_count', 'student_prior_completed_problem_set_count', 'student_prior_completed_problem_count', 'student_prior_median_first_response_time', 'student_prior_median_time_on_task', 'student_prior_average_correctness', 'student_prior_average_attempt_count', 'class_id', 'class_creation_date', 'class_student_count', 'class_prior_skill_builder_count', 'class_prior_problem_set_count', 'class_prior_skill_builder_percent_started', 'class_prior_skill_builder_percent_completed', 'class_prior_problem_set_percent_started', 'class_prior_problem_set_percent_completed', 'class_prior_completed_problem_count', 'class_prior_median_time_on_task', 'class_prior_median_first_response_time', 'class_prior_average_correctness', 'class_prior_average_attempt_count', 'teacher_account_creation_date', 'district_id', 'location', 'locale_description', 'teacher_id', 'opportunity_zone_bool']\n"
          ]
        }
      ],
      "source": [
        "# Data Cleaning\n",
        "\n",
        "if 'merged_df' in locals() and merged_df is not None and merge_successful: \n",
        "    print(\"\\n--- Starting Data Cleaning ---\")\n",
        "    print(f\"Initial merged_df shape for cleaning: {merged_df.shape}\")\n",
        "\n",
        "    print(\"\\n--- Renaming Columns ---\")\n",
        "    rename_map = {\n",
        "        'assistments_reference_action_log_id': 'action_log_id',\n",
        "        'start_time_perf': 'assignment_start_time',\n",
        "        'end_time_perf': 'assignment_end_time',\n",
        "        'assistments_reference_assignment_log_id': 'assignment_log_id'\n",
        "    }\n",
        "    actual_renames = {k: v for k, v in rename_map.items() if k in merged_df.columns}\n",
        "    if actual_renames:\n",
        "        print(f\"Applying renames: {actual_renames}\")\n",
        "        merged_df = merged_df.rename(actual_renames)\n",
        "    else:\n",
        "        print(\"No columns matched for renaming based on the current rename_map.\")\n",
        "\n",
        "    column_transformations = []\n",
        "\n",
        "    datetime_cols_final_check = [\n",
        "        'timestamp', 'start_time', 'end_time', 'release_date', 'due_date',\n",
        "        'assignment_start_time', 'assignment_end_time',\n",
        "        'class_creation_date', 'teacher_account_creation_date'\n",
        "    ]\n",
        "    for col_name in datetime_cols_final_check:\n",
        "        if col_name in merged_df.columns:\n",
        "            current_dtype = merged_df[col_name].dtype\n",
        "            if current_dtype == pl.Utf8:\n",
        "                print(f\"Scheduled for datetime re-parsing (UTF8 found): {col_name}\")\n",
        "                column_transformations.append(\n",
        "                    pl.col(col_name).str.to_datetime(format=COMMON_DATETIME_FORMAT, strict=False, time_unit='us')\n",
        "                    .dt.convert_time_zone(\"UTC\")\n",
        "                    .alias(col_name)\n",
        "                )\n",
        "            elif isinstance(current_dtype, pl.Datetime):\n",
        "                current_tz = current_dtype.time_zone\n",
        "                if current_tz is None:\n",
        "                    print(f\"Info: Datetime column '{col_name}' is naive. Localizing to UTC.\")\n",
        "                    column_transformations.append(\n",
        "                        pl.col(col_name).dt.replace_time_zone(\"UTC\", ambiguous='earliest').alias(col_name)\n",
        "                    )\n",
        "                elif current_tz != \"UTC\":\n",
        "                    print(f\"Scheduled for UTC conversion (already datetime, was '{current_tz}'): {col_name}\")\n",
        "                    column_transformations.append(\n",
        "                        pl.col(col_name).dt.convert_time_zone(\"UTC\").alias(col_name)\n",
        "                    )\n",
        "\n",
        "    cols_to_category_polars = [\n",
        "        'experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id',\n",
        "        'experiment_tag_path', 'action', 'problem_condition', 'assigned_condition',\n",
        "        'class_id', 'district_id', 'location', 'opportunity_zone',\n",
        "        'locale_description', 'teacher_id'\n",
        "    ]\n",
        "    for col_name in cols_to_category_polars:\n",
        "        if col_name in merged_df.columns and merged_df[col_name].dtype != pl.Categorical:\n",
        "             column_transformations.append(pl.col(col_name).cast(pl.Categorical).alias(col_name))\n",
        "             print(f\"Scheduled for categorical conversion: {col_name}\")\n",
        "\n",
        "    float_to_int_casts = {\n",
        "        'assignment_session_count': pl.UInt16, 'pretest_problem_count': pl.UInt16,\n",
        "        'pretest_correct': pl.UInt16, 'pretest_session_count': pl.UInt16,\n",
        "        'condition_problem_count': pl.UInt16, 'condition_total_correct': pl.UInt16,\n",
        "        'condition_total_correct_after_wrong_response': pl.UInt16,\n",
        "        'condition_total_correct_after_tutoring': pl.UInt16,\n",
        "        'condition_total_answers_before_tutoring': pl.UInt16,\n",
        "        'condition_total_attempt_count': pl.UInt32,\n",
        "        'condition_total_hints_available': pl.UInt32, 'condition_total_hints_given': pl.UInt32,\n",
        "        'condition_total_scaffold_problems_available': pl.UInt32,\n",
        "        'condition_total_scaffold_problems_given': pl.UInt32,\n",
        "        'condition_total_explanations_available': pl.UInt32,\n",
        "        'condition_total_explanations_given': pl.UInt32,\n",
        "        'condition_total_answers_given': pl.UInt32,\n",
        "        'condition_session_count': pl.UInt16, 'posttest_problem_count': pl.UInt16,\n",
        "        'posttest_correct': pl.UInt16, 'posttest_session_count': pl.UInt16,\n",
        "    }\n",
        "    for col_name, target_int_type in float_to_int_casts.items():\n",
        "        if col_name in merged_df.columns:\n",
        "            if merged_df[col_name].dtype == pl.Float32:\n",
        "                column_transformations.append(\n",
        "                    pl.col(col_name)\n",
        "                      .fill_null(0)\n",
        "                      .cast(target_int_type, strict=False)\n",
        "                      .alias(col_name)\n",
        "                )\n",
        "                print(f\"Scheduled '{col_name}' for Float32 to {target_int_type} conversion.\")\n",
        "            elif merged_df[col_name].dtype != target_int_type:\n",
        "                print(f\"Warning: Column '{col_name}' was expected to be Float32 for int conversion, but found {merged_df[col_name].dtype}. Skipping specific int cast.\")\n",
        "\n",
        "    # General Float64 to Float32 pass\n",
        "    float64_cols = [col_name for col_name, dtype in merged_df.schema.items() if dtype == pl.Float64]\n",
        "    for col_name in float64_cols:\n",
        "        if col_name in merged_df.columns:\n",
        "            column_transformations.append(pl.col(col_name).cast(pl.Float32).alias(col_name))\n",
        "            print(f\"Scheduled for Float64 to Float32 conversion: {col_name}\")\n",
        "\n",
        "    if column_transformations:\n",
        "        print(\"\\nApplying column type transformations...\")\n",
        "        merged_df = merged_df.with_columns(column_transformations)\n",
        "        print(\"Type transformations applied.\")\n",
        "\n",
        "    print(\"\\n--- Specific Value Cleaning ---\")\n",
        "    specific_value_cleaning_expressions = []\n",
        "\n",
        "    if 'opportunity_zone' in merged_df.columns:\n",
        "        if merged_df['opportunity_zone'].dtype != pl.Categorical:\n",
        "             merged_df = merged_df.with_columns(pl.col('opportunity_zone').cast(pl.Categorical))\n",
        "        specific_value_cleaning_expressions.append(\n",
        "            pl.when(pl.col('opportunity_zone').cast(pl.Utf8) == \"Yes\").then(True)\n",
        "              .when(pl.col('opportunity_zone').cast(pl.Utf8) == \"No\").then(False)\n",
        "              .otherwise(None)\n",
        "              .cast(pl.Boolean)\n",
        "              .alias('opportunity_zone_bool')\n",
        "        )\n",
        "        print(\"Scheduled 'opportunity_zone' to boolean 'opportunity_zone_bool' conversion.\")\n",
        "\n",
        "    cat_cols_to_fill_info = {\n",
        "        'district_id': 'Unknown_District',\n",
        "        'location': 'Unknown_Location',\n",
        "        'locale_description': 'Unknown_Locale'\n",
        "    }\n",
        "    for col_name, fill_val in cat_cols_to_fill_info.items():\n",
        "        if col_name in merged_df.columns:\n",
        "            if merged_df[col_name].dtype != pl.Categorical:\n",
        "                merged_df = merged_df.with_columns(pl.col(col_name).cast(pl.Categorical))\n",
        "                print(f\"Casted '{col_name}' to Categorical before fill_null.\")\n",
        "            specific_value_cleaning_expressions.append(pl.col(col_name).fill_null(pl.lit(fill_val).cast(pl.Categorical)).alias(col_name))\n",
        "            print(f\"Scheduled fill_null for categorical {col_name} with '{fill_val}'.\")\n",
        "\n",
        "    if specific_value_cleaning_expressions:\n",
        "        print(\"\\nApplying specific value cleaning expressions...\")\n",
        "        merged_df = merged_df.with_columns(specific_value_cleaning_expressions)\n",
        "        print(\"Specific value cleaning applied.\")\n",
        "\n",
        "    pandas_identified_empty_cols = [\n",
        "         'problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task',\n",
        "         'first_response_or_request_time', 'first_answer', 'correct', 'reported_score',\n",
        "         'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given',\n",
        "         'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available',\n",
        "         'explanation_given', 'answer_given',\n",
        "         'assistments_reference_problem_log_id'\n",
        "    ]\n",
        "    actual_empty_cols_to_drop = []\n",
        "    if not merged_df.is_empty():\n",
        "        for col_name in pandas_identified_empty_cols:\n",
        "            if col_name in merged_df.columns and merged_df[col_name].is_null().all():\n",
        "                actual_empty_cols_to_drop.append(col_name)\n",
        "            elif col_name in merged_df.columns:\n",
        "                null_count = merged_df[col_name].is_null().sum()\n",
        "                if null_count > 0 :\n",
        "                    print(f\"Info: Column '{col_name}' (candidate for empty drop) was not fully null. Nulls: {null_count}/{merged_df.height}\")\n",
        "\n",
        "    if actual_empty_cols_to_drop:\n",
        "        print(f\"\\nDropping fully empty columns: {actual_empty_cols_to_drop}\")\n",
        "        merged_df = merged_df.drop(actual_empty_cols_to_drop)\n",
        "    else:\n",
        "        print(\"\\nNo fully empty columns (from the predefined list) identified for dropping.\")\n",
        "\n",
        "    if 'opportunity_zone' in merged_df.columns and 'opportunity_zone_bool' in merged_df.columns:\n",
        "        print(\"Dropping original opportunity zone column: 'opportunity_zone'\")\n",
        "        merged_df = merged_df.drop('opportunity_zone')\n",
        "\n",
        "    print(f\"\\nShape after Cleaning: {merged_df.shape}\")\n",
        "    print(\"Columns after cleaning:\", merged_df.columns)\n",
        "    gc.collect()\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Cell 7 cleaning: merged_df not available, previous merge failed, or merge_successful flag is False.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Reducing DataFrame to Essential Columns ---\n",
            "Attempting to select these 19 essential columns: ['experiment_id', 'student_id', 'problem_id', 'timestamp', 'action', 'action_log_id', 'assignment_start_time', 'assignment_end_time', 'assignment_log_id', 'assignment_session_count', 'condition_problem_count', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_total_correct', 'condition_total_attempt_count', 'condition_total_hints_given', 'condition_total_explanations_given', 'student_prior_average_correctness', 'opportunity_zone_bool']\n",
            "\n",
            "Reduced DataFrame Info: Shape (3711215, 19)\n",
            "shape: (5, 19)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
            "│ experimen ┆ student_i ┆ problem_i ┆ timestamp ┆ … ┆ condition ┆ condition ┆ student_p ┆ opportun │\n",
            "│ t_id      ┆ d         ┆ d         ┆ ---       ┆   ┆ _total_hi ┆ _total_ex ┆ rior_aver ┆ ity_zone │\n",
            "│ ---       ┆ ---       ┆ ---       ┆ datetime[ ┆   ┆ nts_given ┆ planation ┆ age_corre ┆ _bool    │\n",
            "│ cat       ┆ cat       ┆ cat       ┆ μs, UTC]  ┆   ┆ ---       ┆ s_g…      ┆ ctn…      ┆ ---      │\n",
            "│           ┆           ┆           ┆           ┆   ┆ u32       ┆ ---       ┆ ---       ┆ bool     │\n",
            "│           ┆           ┆           ┆           ┆   ┆           ┆ u32       ┆ f32       ┆          │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ 2021-03-2 ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆           ┆ 4 16:41:4 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 6.393 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 2021-03-2 ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆           ┆ 4 16:41:4 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 7.437 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 2021-03-2 ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆           ┆ 4 16:42:0 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 3.665 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 2021-03-2 ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆           ┆ 4 16:42:0 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 3.675 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ 2021-03-2 ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆           ┆ 4 16:42:0 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 5.295 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'problem_id': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'action': Categorical(ordering='physical'), 'action_log_id': UInt64, 'assignment_start_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_end_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_log_id': UInt64, 'assignment_session_count': UInt16, 'condition_problem_count': UInt16, 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_total_correct': UInt16, 'condition_total_attempt_count': UInt32, 'condition_total_hints_given': UInt32, 'condition_total_explanations_given': UInt32, 'student_prior_average_correctness': Float32, 'opportunity_zone_bool': Boolean})\n",
            "\n",
            "Attempting to save cleaned and reduced DataFrame to: /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "Successfully saved to /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "\n",
            "Full cleaned merged_df deleted from memory.\n"
          ]
        }
      ],
      "source": [
        "# Create Reduced DataFrame and Save\n",
        "\n",
        "if 'merged_df' in locals() and merged_df is not None and merge_successful: # Check if merged_df exists\n",
        "    print(\"\\n--- Reducing DataFrame to Essential Columns ---\")\n",
        "\n",
        "    essential_cols_to_keep_polars = [\n",
        "        # Keys / Base Info from actions_df\n",
        "        'experiment_id',\n",
        "        'student_id',\n",
        "        'problem_id', # <--- ADD THIS LINE\n",
        "        'timestamp',\n",
        "        'action',\n",
        "        'action_log_id', # This was 'assistments_reference_action_log_id'\n",
        "\n",
        "        # From performance_df\n",
        "        'assignment_start_time', # This was 'start_time_perf'\n",
        "        'assignment_end_time',   # This was 'end_time_perf'\n",
        "        'assignment_log_id',     # This was 'assistments_reference_assignment_log_id'\n",
        "        'assignment_session_count',\n",
        "        'condition_problem_count',\n",
        "        'condition_time_on_task',\n",
        "        'condition_average_first_response_or_request_time',\n",
        "        'condition_total_correct',\n",
        "        'condition_total_attempt_count',\n",
        "        'condition_total_hints_given',\n",
        "        'condition_total_explanations_given',\n",
        "\n",
        "        # From metrics_df\n",
        "        'student_prior_average_correctness',\n",
        "\n",
        "        'opportunity_zone_bool', # This was derived and original 'opportunity_zone' dropped\n",
        "    ]\n",
        "\n",
        "    # Filter to only include columns that actually exist in the cleaned merged_df\n",
        "    final_essential_columns = [col for col in essential_cols_to_keep_polars if col in merged_df.columns]\n",
        "\n",
        "    # Check if problem_id was found in merged_df.columns\n",
        "    if 'problem_id' not in final_essential_columns and 'problem_id' in essential_cols_to_keep_polars:\n",
        "        print(\"WARNING: 'problem_id' was requested but not found in merged_df.columns. It will not be included.\")\n",
        "        print(f\"Available columns in merged_df: {merged_df.columns}\")\n",
        "\n",
        "\n",
        "    print(f\"Attempting to select these {len(final_essential_columns)} essential columns: {final_essential_columns}\")\n",
        "    missing_essentials_for_reduction = [col for col in essential_cols_to_keep_polars if col not in final_essential_columns]\n",
        "\n",
        "    if missing_essentials_for_reduction:\n",
        "        print(f\"Warning: The following conceptual essential columns were NOT FOUND in merged_df for reduction: {missing_essentials_for_reduction}\")\n",
        "        print(\"Please ensure their names are correct in the 'essential_cols_to_keep_polars' list and they exist in the output of Cell 7.\")\n",
        "\n",
        "    if not final_essential_columns:\n",
        "        print(\"Error: No essential columns available for selection based on your list. Cannot create reduced DataFrame.\")\n",
        "        merged_df_reduced = None\n",
        "    else:\n",
        "        try:\n",
        "            merged_df_reduced = merged_df.select(final_essential_columns) # This line uses the updated list\n",
        "            print(f\"\\nReduced DataFrame Info: Shape {merged_df_reduced.shape}\")\n",
        "            print(merged_df_reduced.head())\n",
        "            print(merged_df_reduced.schema) # Check the schema here to confirm problem_id\n",
        "\n",
        "            print(f\"\\nAttempting to save cleaned and reduced DataFrame to: {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "            merged_df_reduced.write_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET)\n",
        "            print(f\"Successfully saved to {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during final select or save: {e}\")\n",
        "            merged_df_reduced = None\n",
        "\n",
        "    if 'merged_df' in locals(): # Check if merged_df exists before trying to delete\n",
        "        del merged_df\n",
        "        gc.collect()\n",
        "        print(\"\\nFull cleaned merged_df deleted from memory.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Cell 8 (reduction and save): merged_df not available from Cell 7 or previous steps failed.\")\n",
        "    merged_df_reduced = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading Cleaned Parquet File ---\n",
            "Successfully reloaded: /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "Reloaded DataFrame Shape: (3711215, 19)\n",
            "\n",
            "Reloaded DataFrame Head (from Parquet):\n",
            "shape: (5, 19)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
            "│ experimen ┆ student_i ┆ problem_i ┆ timestamp ┆ … ┆ condition ┆ condition ┆ student_p ┆ opportun │\n",
            "│ t_id      ┆ d         ┆ d         ┆ ---       ┆   ┆ _total_hi ┆ _total_ex ┆ rior_aver ┆ ity_zone │\n",
            "│ ---       ┆ ---       ┆ ---       ┆ datetime[ ┆   ┆ nts_given ┆ planation ┆ age_corre ┆ _bool    │\n",
            "│ cat       ┆ cat       ┆ cat       ┆ μs, UTC]  ┆   ┆ ---       ┆ s_g…      ┆ ctn…      ┆ ---      │\n",
            "│           ┆           ┆           ┆           ┆   ┆ u32       ┆ ---       ┆ ---       ┆ bool     │\n",
            "│           ┆           ┆           ┆           ┆   ┆           ┆ u32       ┆ f32       ┆          │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ 2021-03-2 ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆           ┆ 4 16:41:4 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 6.393 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 2021-03-2 ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆           ┆ 4 16:41:4 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 7.437 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 2021-03-2 ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆           ┆ 4 16:42:0 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 3.665 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 2021-03-2 ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆           ┆ 4 16:42:0 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 3.675 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ 2021-03-2 ┆ … ┆ 1         ┆ 0         ┆ 0.738739  ┆ null     │\n",
            "│           ┆           ┆           ┆ 4 16:42:0 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 5.295 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
            "\n",
            "Reloaded DataFrame Schema (from Parquet):\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'problem_id': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'action': Categorical(ordering='physical'), 'action_log_id': UInt64, 'assignment_start_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_end_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_log_id': UInt64, 'assignment_session_count': UInt16, 'condition_problem_count': UInt16, 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_total_correct': UInt16, 'condition_total_attempt_count': UInt32, 'condition_total_hints_given': UInt32, 'condition_total_explanations_given': UInt32, 'student_prior_average_correctness': Float32, 'opportunity_zone_bool': Boolean})\n"
          ]
        }
      ],
      "source": [
        "# Load Cleaned Data\n",
        "\n",
        "if 'merged_df_reduced' in locals() and merged_df_reduced is not None and not merged_df_reduced.is_empty() and SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "    print(f\"\\n--- Loading Cleaned Parquet File ---\")\n",
        "    try:\n",
        "        df_reloaded_polars = pl.read_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET)\n",
        "        print(f\"Successfully reloaded: {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "        print(f\"Reloaded DataFrame Shape: {df_reloaded_polars.shape}\")\n",
        "        print(\"\\nReloaded DataFrame Head (from Parquet):\")\n",
        "        print(df_reloaded_polars.head())\n",
        "        print(\"\\nReloaded DataFrame Schema (from Parquet):\")\n",
        "        print(df_reloaded_polars.schema)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reloading the cleaned Parquet file: {e}\")\n",
        "elif SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "     print(f\"Cleaned Parquet file found at {SAVE_CLEANED_PATH_POLARS_PARQUET}, but merged_df_reduced may not have been successfully created or was empty in the previous step (this script might have been re-run starting from here). Consider reloading manually if needed.\")\n",
        "else:\n",
        "    print(f\"\\nCleaned Parquet file not found at {SAVE_CLEANED_PATH_POLARS_PARQUET} or reduction/save step failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema of df_reloaded_polars after adding problem_id:\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'problem_id': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'action': Categorical(ordering='physical'), 'action_log_id': UInt64, 'assignment_start_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_end_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_log_id': UInt64, 'assignment_session_count': UInt16, 'condition_problem_count': UInt16, 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_total_correct': UInt16, 'condition_total_attempt_count': UInt32, 'condition_total_hints_given': UInt32, 'condition_total_explanations_given': UInt32, 'student_prior_average_correctness': Float32, 'opportunity_zone_bool': Boolean})\n",
            "Columns: ['experiment_id', 'student_id', 'problem_id', 'timestamp', 'action', 'action_log_id', 'assignment_start_time', 'assignment_end_time', 'assignment_log_id', 'assignment_session_count', 'condition_problem_count', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_total_correct', 'condition_total_attempt_count', 'condition_total_hints_given', 'condition_total_explanations_given', 'student_prior_average_correctness', 'opportunity_zone_bool']\n",
            "'problem_id' successfully included!\n",
            "shape: (5, 1)\n",
            "┌────────────┐\n",
            "│ problem_id │\n",
            "│ ---        │\n",
            "│ cat        │\n",
            "╞════════════╡\n",
            "│ null       │\n",
            "│ PRA5EW7    │\n",
            "│ PRA5EW7    │\n",
            "│ PRA5EW7    │\n",
            "│ null       │\n",
            "└────────────┘\n"
          ]
        }
      ],
      "source": [
        "# Add this in a new cell after Cell 9 runs\n",
        "if 'df_reloaded_polars' in locals():\n",
        "    print(\"Schema of df_reloaded_polars after adding problem_id:\")\n",
        "    print(df_reloaded_polars.schema)\n",
        "    print(f\"Columns: {df_reloaded_polars.columns}\")\n",
        "    if 'problem_id' in df_reloaded_polars.columns:\n",
        "        print(\"'problem_id' successfully included!\")\n",
        "        print(df_reloaded_polars.select(\"problem_id\").head()) # See some sample problem_id values\n",
        "    else:\n",
        "        print(\"ERROR: 'problem_id' is still NOT in df_reloaded_polars. Check Cell 8 logic and merged_df columns before select.\")\n",
        "else:\n",
        "    print(\"df_reloaded_polars not found. Ensure Cell 9 has run correctly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape: (5, 21)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
            "│ experimen ┆ student_i ┆ problem_i ┆ timestamp ┆ … ┆ student_p ┆ opportuni ┆ condition ┆ is_at_ri │\n",
            "│ t_id      ┆ d         ┆ d         ┆ ---       ┆   ┆ rior_aver ┆ ty_zone_b ┆ _correctn ┆ sk_targe │\n",
            "│ ---       ┆ ---       ┆ ---       ┆ datetime[ ┆   ┆ age_corre ┆ ool       ┆ ess_perce ┆ t        │\n",
            "│ cat       ┆ cat       ┆ cat       ┆ μs, UTC]  ┆   ┆ ctn…      ┆ ---       ┆ nta…      ┆ ---      │\n",
            "│           ┆           ┆           ┆           ┆   ┆ ---       ┆ bool      ┆ ---       ┆ bool     │\n",
            "│           ┆           ┆           ┆           ┆   ┆ f32       ┆           ┆ f64       ┆          │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ 2021-03-2 ┆ … ┆ 0.738739  ┆ null      ┆ 0.333333  ┆ true     │\n",
            "│           ┆           ┆           ┆ 4 16:41:4 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 6.393 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 2021-03-2 ┆ … ┆ 0.738739  ┆ null      ┆ 0.333333  ┆ true     │\n",
            "│           ┆           ┆           ┆ 4 16:41:4 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 7.437 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 2021-03-2 ┆ … ┆ 0.738739  ┆ null      ┆ 0.333333  ┆ true     │\n",
            "│           ┆           ┆           ┆ 4 16:42:0 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 3.665 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ PRA5EW7   ┆ 2021-03-2 ┆ … ┆ 0.738739  ┆ null      ┆ 0.333333  ┆ true     │\n",
            "│           ┆           ┆           ┆ 4 16:42:0 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 3.675 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "│ PSAU85Y   ┆ 10408     ┆ null      ┆ 2021-03-2 ┆ … ┆ 0.738739  ┆ null      ┆ 0.333333  ┆ true     │\n",
            "│           ┆           ┆           ┆ 4 16:42:0 ┆   ┆           ┆           ┆           ┆          │\n",
            "│           ┆           ┆           ┆ 5.295 UTC ┆   ┆           ┆           ┆           ┆          │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
            "shape: (2, 2)\n",
            "┌───────────────────┬─────────┐\n",
            "│ is_at_risk_target ┆ count   │\n",
            "│ ---               ┆ ---     │\n",
            "│ bool              ┆ u32     │\n",
            "╞═══════════════════╪═════════╡\n",
            "│ false             ┆ 2256708 │\n",
            "│ true              ┆ 937084  │\n",
            "└───────────────────┴─────────┘\n"
          ]
        }
      ],
      "source": [
        "# Ensure relevant columns are not null and condition_problem_count is not zero to avoid division by zero\n",
        "df_with_target = df_reloaded_polars.filter(\n",
        "    pl.col(\"condition_problem_count\").is_not_null() & (pl.col(\"condition_problem_count\") > 0) &\n",
        "    pl.col(\"condition_total_correct\").is_not_null()\n",
        ")\n",
        "\n",
        "# Calculate correctness percentage for the 'condition' part of the assignment\n",
        "df_with_target = df_with_target.with_columns(\n",
        "    (pl.col(\"condition_total_correct\") / pl.col(\"condition_problem_count\")).alias(\"condition_correctness_percentage\")\n",
        ")\n",
        "\n",
        "# Define a threshold for \"at-risk\" (e.g., less than 50% correct)\n",
        "# This threshold might need to be determined by domain knowledge or data exploration (e.g., quartiles)\n",
        "at_risk_threshold = 0.5\n",
        "df_with_target = df_with_target.with_columns(\n",
        "    pl.when(pl.col(\"condition_correctness_percentage\") < at_risk_threshold)\n",
        "    .then(True) # Student is at-risk\n",
        "    .otherwise(False) # Student is not at-risk\n",
        "    .alias(\"is_at_risk_target\")\n",
        ")\n",
        "\n",
        "# Display the new columns and their distribution\n",
        "print(df_with_target.head())\n",
        "print(df_with_target.group_by(\"is_at_risk_target\").agg(pl.len().alias(\"count\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting temporal feature engineering using 'df_with_target' as input.\n",
            "Proceeding with problem_id for 'first 25%' definition.\n",
            "Identified 385349 early actions based on problem order.\n",
            "Aggregated temporal features (problem-based window):\n",
            "shape: (5, 7)\n",
            "┌────────────┬──────────────┬──────────────┬─────────────┬─────────────┬─────────────┬─────────────┐\n",
            "│ student_id ┆ assignment_l ┆ early_hint_r ┆ early_attem ┆ early_durat ┆ early_disti ┆ avg_actions │\n",
            "│ ---        ┆ og_id        ┆ equests      ┆ pts_or_resp ┆ ion_seconds ┆ nct_problem ┆ _per_early_ │\n",
            "│ cat        ┆ ---          ┆ ---          ┆ onses       ┆ ---         ┆ s_worked…   ┆ problem     │\n",
            "│            ┆ u64          ┆ i32          ┆ ---         ┆ i64         ┆ ---         ┆ ---         │\n",
            "│            ┆              ┆              ┆ i32         ┆             ┆ u32         ┆ f64         │\n",
            "╞════════════╪══════════════╪══════════════╪═════════════╪═════════════╪═════════════╪═════════════╡\n",
            "│ 32839      ┆ 158105       ┆ 0            ┆ 1           ┆ 129         ┆ 1           ┆ 3.0         │\n",
            "│ 629762     ┆ 11974554     ┆ 0            ┆ 1           ┆ 12          ┆ 1           ┆ 3.0         │\n",
            "│ 112697     ┆ 3746070      ┆ 0            ┆ 4           ┆ 380         ┆ 2           ┆ 7.5         │\n",
            "│ 489211     ┆ 5438616      ┆ 0            ┆ 2           ┆ 119         ┆ 1           ┆ 4.0         │\n",
            "│ 869779     ┆ 11101421     ┆ 0            ┆ 1           ┆ 8           ┆ 1           ┆ 3.0         │\n",
            "└────────────┴──────────────┴──────────────┴─────────────┴─────────────┴─────────────┴─────────────┘\n",
            "Joining temporal features...\n",
            "Attempting to fill nulls in column: 'early_hint_requests' with value: 0\n",
            "  Original dtype of 'early_hint_requests': Int32\n",
            "  Successfully filled nulls in 'early_hint_requests'. New dtype: Int32\n",
            "Attempting to fill nulls in column: 'early_attempts_or_responses' with value: 0\n",
            "  Original dtype of 'early_attempts_or_responses': Int32\n",
            "  Successfully filled nulls in 'early_attempts_or_responses'. New dtype: Int32\n",
            "Attempting to fill nulls in column: 'early_duration_seconds' with value: 0\n",
            "  Original dtype of 'early_duration_seconds': Int64\n",
            "  Successfully filled nulls in 'early_duration_seconds'. New dtype: Int64\n",
            "Attempting to fill nulls in column: 'early_distinct_problems_worked_on' with value: 0\n",
            "  Original dtype of 'early_distinct_problems_worked_on': UInt32\n",
            "  Successfully filled nulls in 'early_distinct_problems_worked_on'. New dtype: UInt32\n",
            "Attempting to fill nulls in column: 'avg_actions_per_early_problem' with value: 0\n",
            "  Original dtype of 'avg_actions_per_early_problem': Float64\n",
            "  Successfully filled nulls in 'avg_actions_per_early_problem'. New dtype: Float64\n",
            "\n",
            "Final assignment-level DataFrame for modeling (head):\n",
            "shape: (5, 23)\n",
            "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
            "│ student_i ┆ assignmen ┆ experimen ┆ problem_i ┆ … ┆ early_att ┆ early_dur ┆ early_dis ┆ avg_acti │\n",
            "│ d         ┆ t_log_id  ┆ t_id      ┆ d         ┆   ┆ empts_or_ ┆ ation_sec ┆ tinct_pro ┆ ons_per_ │\n",
            "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ responses ┆ onds      ┆ blems_wor ┆ early_pr │\n",
            "│ cat       ┆ u64       ┆ cat       ┆ cat       ┆   ┆ ---       ┆ ---       ┆ ked…      ┆ oblem    │\n",
            "│           ┆           ┆           ┆           ┆   ┆ i32       ┆ i64       ┆ ---       ┆ ---      │\n",
            "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ u32       ┆ f64      │\n",
            "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
            "│ 10595     ┆ 3966330   ┆ PSA2KNR   ┆ null      ┆ … ┆ 1         ┆ 6         ┆ 1         ┆ 3.0      │\n",
            "│ 898119    ┆ 13643007  ┆ PSAHQV    ┆ null      ┆ … ┆ 1         ┆ 5         ┆ 1         ┆ 3.0      │\n",
            "│ 833539    ┆ 15656785  ┆ PSAV89B   ┆ null      ┆ … ┆ 1         ┆ 193       ┆ 1         ┆ 3.0      │\n",
            "│ 432751    ┆ 4314907   ┆ PSAHQV    ┆ null      ┆ … ┆ 1         ┆ 21        ┆ 1         ┆ 3.0      │\n",
            "│ 477980    ┆ 12004673  ┆ PSAVTMK   ┆ null      ┆ … ┆ 3         ┆ 2538      ┆ 3         ┆ 6.0      │\n",
            "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
            "Shape of assignment_level_final_df: (77070, 23)\n",
            "\n",
            "Target variable distribution at assignment level:\n",
            "shape: (2, 2)\n",
            "┌───────────────────┬───────┐\n",
            "│ is_at_risk_target ┆ count │\n",
            "│ ---               ┆ ---   │\n",
            "│ bool              ┆ u32   │\n",
            "╞═══════════════════╪═══════╡\n",
            "│ false             ┆ 58635 │\n",
            "│ true              ┆ 18435 │\n",
            "└───────────────────┴───────┘\n"
          ]
        }
      ],
      "source": [
        "# Ensure df_with_target from the previous cell (your target creation cell) exists\n",
        "if 'df_with_target' in locals() and isinstance(df_with_target, pl.DataFrame) and not df_with_target.is_empty():\n",
        "    print(\"Starting temporal feature engineering using 'df_with_target' as input.\")\n",
        "    action_level_df = df_with_target.sort([\"student_id\", \"assignment_log_id\", \"timestamp\"])\n",
        "else:\n",
        "    print(\"ERROR: 'df_with_target' not found or is empty. Please ensure the target variable creation cell (Cell 11) has run successfully.\")\n",
        "    action_level_df = pl.DataFrame()\n",
        "\n",
        "if not action_level_df.is_empty():\n",
        "    print(\"Proceeding with problem_id for 'first 25%' definition.\")\n",
        "\n",
        "    action_level_df = action_level_df.with_columns(\n",
        "        (pl.col(\"condition_problem_count\") * 0.25).floor().cast(pl.Int32)\n",
        "        .pipe(lambda s: pl.when(s == 0).then(1).otherwise(s))\n",
        "        .alias(\"first_25_percent_problem_threshold\")\n",
        "    )\n",
        "\n",
        "    actions_with_problem_id = action_level_df.filter(pl.col(\"problem_id\").is_not_null())\n",
        "\n",
        "    if not actions_with_problem_id.is_empty():\n",
        "        problem_order_df = actions_with_problem_id.group_by(\n",
        "            [\"student_id\", \"assignment_log_id\", \"problem_id\"]\n",
        "        ).agg(\n",
        "            pl.min(\"timestamp\").alias(\"first_seen_timestamp\")\n",
        "        ).sort([\"student_id\", \"assignment_log_id\", \"first_seen_timestamp\"]) # Sorting is important\n",
        "\n",
        "        # *** THIS IS THE MODIFIED LINE ***\n",
        "        problem_order_df = problem_order_df.with_columns(\n",
        "            pl.col(\"first_seen_timestamp\").rank(method=\"ordinal\").over([\"student_id\", \"assignment_log_id\"]).alias(\"problem_order_in_assignment\")\n",
        "        )\n",
        "        # *** END MODIFIED LINE ***\n",
        "\n",
        "        actions_with_problem_order = actions_with_problem_id.join(\n",
        "            problem_order_df.select([\"student_id\", \"assignment_log_id\", \"problem_id\", \"problem_order_in_assignment\"]),\n",
        "            on=[\"student_id\", \"assignment_log_id\", \"problem_id\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        early_actions_df = actions_with_problem_order.filter(\n",
        "            pl.col(\"problem_order_in_assignment\") <= pl.col(\"first_25_percent_problem_threshold\")\n",
        "        )\n",
        "        print(f\"Identified {early_actions_df.height} early actions based on problem order.\")\n",
        "    else:\n",
        "        print(\"No actions with problem_id found. Cannot create problem-based early_actions_df.\")\n",
        "        early_actions_df = pl.DataFrame()\n",
        "\n",
        "    # --- Part 2: Aggregate Temporal Features from early_actions_df ---\n",
        "    if not early_actions_df.is_empty() and early_actions_df.height > 0:\n",
        "        temporal_features_aggregated = early_actions_df.group_by(\n",
        "            [\"student_id\", \"assignment_log_id\"]\n",
        "        ).agg([\n",
        "            (pl.when(pl.col(\"action\") == \"hint_request\").then(1).otherwise(0)).sum().alias(\"early_hint_requests\"), # This comparison should be okay\n",
        "            \n",
        "            # Corrected line: cast 'action' to Utf8 before .str.contains()\n",
        "            (pl.when(pl.col(\"action\").cast(pl.Utf8).str.contains(r\"(?i)attempt|response\")).then(1).otherwise(0)).sum().alias(\"early_attempts_or_responses\"),\n",
        "            \n",
        "            (pl.col(\"timestamp\").max() - pl.col(\"timestamp\").min()).dt.total_seconds().alias(\"early_duration_seconds\"),\n",
        "            pl.n_unique(\"problem_id\").alias(\"early_distinct_problems_worked_on\"), # n_unique on Categorical is fine\n",
        "            (pl.len() / pl.col(\"problem_id\").n_unique()).alias(\"avg_actions_per_early_problem\") # n_unique on Categorical is fine\n",
        "        ])\n",
        "        print(\"Aggregated temporal features (problem-based window):\")\n",
        "        print(temporal_features_aggregated.head())\n",
        "    else:\n",
        "        print(\"No early actions identified or early_actions_df is empty, so no temporal features created.\")\n",
        "        temporal_features_aggregated = pl.DataFrame(schema={\n",
        "            \"student_id\": pl.Categorical, \"assignment_log_id\": pl.UInt64,\n",
        "            \"early_hint_requests\": pl.UInt32, \"early_attempts_or_responses\": pl.UInt32,\n",
        "            \"early_duration_seconds\": pl.Float64, \"early_distinct_problems_worked_on\": pl.UInt32,\n",
        "            \"avg_actions_per_early_problem\": pl.Float64\n",
        "        })\n",
        "\n",
        "    # --- Part 3: Create Assignment-Level Base Data and Join Temporal Features ---\n",
        "\n",
        "    assignment_level_final_df = action_level_df.group_by(\n",
        "        [\"student_id\", \"assignment_log_id\"]\n",
        "    ).agg([\n",
        "        pl.first(\"experiment_id\"),\n",
        "        pl.first(\"problem_id\"), # You might want the first problem_id or leave it out if it's not truly assignment-level\n",
        "        pl.first(\"assignment_start_time\"),\n",
        "        pl.first(\"assignment_end_time\"),\n",
        "        pl.first(\"assignment_session_count\"),\n",
        "        pl.first(\"condition_problem_count\"),\n",
        "        pl.first(\"condition_time_on_task\"),\n",
        "        pl.first(\"condition_average_first_response_or_request_time\"),\n",
        "        pl.first(\"condition_total_correct\"),\n",
        "        pl.first(\"condition_total_attempt_count\"),\n",
        "        pl.first(\"condition_total_hints_given\"),\n",
        "        pl.first(\"condition_total_explanations_given\"),\n",
        "        pl.first(\"student_prior_average_correctness\"),\n",
        "        pl.first(\"opportunity_zone_bool\"),\n",
        "        pl.first(\"condition_correctness_percentage\"), # Your calculated correctness\n",
        "        pl.first(\"is_at_risk_target\") # Your target variable\n",
        "    ]).drop_nulls(subset=[\"student_id\", \"assignment_log_id\"])\n",
        "\n",
        "\n",
        "# (Assuming 'assignment_level_final_df' has been created from Part 3 just before this)\n",
        "# (Assuming 'temporal_features_aggregated' has been created from Part 2 just before this)\n",
        "\n",
        "# Join the aggregated temporal features\n",
        "if temporal_features_aggregated.height > 0:  # This condition checks if there's data to join\n",
        "    print(\"Joining temporal features...\")\n",
        "    assignment_level_final_df = assignment_level_final_df.join(\n",
        "        temporal_features_aggregated,\n",
        "        on=[\"student_id\", \"assignment_log_id\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Fill NaNs for temporal columns that arose from the left join\n",
        "    temporal_cols_to_fill = [\n",
        "        \"early_hint_requests\", \"early_attempts_or_responses\",\n",
        "        \"early_duration_seconds\", \"early_distinct_problems_worked_on\",\n",
        "        \"avg_actions_per_early_problem\"\n",
        "    ]\n",
        "    fill_value = 0\n",
        "    for col_name in temporal_cols_to_fill: # START OF FOR LOOP\n",
        "        if col_name in assignment_level_final_df.columns:\n",
        "            # ... (your successful fill_null logic for each column, which includes:)\n",
        "            print(f\"Attempting to fill nulls in column: '{col_name}' with value: {fill_value}\")\n",
        "            current_dtype = assignment_level_final_df[col_name].dtype\n",
        "            print(f\"  Original dtype of '{col_name}': {current_dtype}\")\n",
        "\n",
        "            fill_expr = None\n",
        "            if current_dtype in [pl.Float32, pl.Float64]:\n",
        "                fill_expr = pl.lit(float(fill_value), dtype=current_dtype)\n",
        "            elif current_dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64]:\n",
        "                fill_expr = pl.lit(int(fill_value), dtype=current_dtype)\n",
        "            else:\n",
        "                print(f\"Warning: Column '{col_name}' (dtype: {current_dtype}) is not float or standard integer. Filling with default int literal.\")\n",
        "                fill_expr = pl.lit(int(fill_value))\n",
        "            \n",
        "            if fill_expr is not None:\n",
        "                try:\n",
        "                    assignment_level_final_df = assignment_level_final_df.with_columns(\n",
        "                        pl.col(col_name).fill_null(fill_expr).alias(col_name)\n",
        "                    )\n",
        "                    print(f\"  Successfully filled nulls in '{col_name}'. New dtype: {assignment_level_final_df[col_name].dtype}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR filling nulls in column '{col_name}' with expr {fill_expr}: {e}\")\n",
        "                    raise # Re-raise the exception\n",
        "            else:\n",
        "                print(f\"Could not determine fill expression for column '{col_name}' with dtype {current_dtype}\")\n",
        "        else:\n",
        "            print(f\"Warning: Expected temporal column '{col_name}' not found in assignment_level_final_df for fill_null.\")\n",
        "    # END OF FOR LOOP (NO 'else' directly attached to this 'for' loop)\n",
        "\n",
        "# This 'else' is now correctly paired with 'if temporal_features_aggregated.height > 0:'\n",
        "else:\n",
        "    print(\"Skipping join of temporal features as 'temporal_features_aggregated' was empty or not generated.\")\n",
        "    # Optionally, add empty columns to assignment_level_final_df here if temporal_features_aggregated was empty\n",
        "    # This ensures assignment_level_final_df always has the columns, even if filled with 0 or null.\n",
        "    # For example:\n",
        "    # temporal_cols_to_add_if_missing = [\n",
        "    #     (\"early_hint_requests\", pl.UInt32), (\"early_attempts_or_responses\", pl.UInt32),\n",
        "    #     (\"early_duration_seconds\", pl.Float64), (\"early_distinct_problems_worked_on\", pl.UInt32),\n",
        "    #     (\"avg_actions_per_early_problem\", pl.Float64)\n",
        "    # ]\n",
        "    # for col_name, col_type in temporal_cols_to_add_if_missing:\n",
        "    #     if col_name not in assignment_level_final_df.columns:\n",
        "    #         assignment_level_final_df = assignment_level_final_df.with_columns(\n",
        "    #             pl.lit(0, dtype=col_type).alias(col_name) # Fill with 0 of the expected type\n",
        "    #         )\n",
        "\n",
        "\n",
        "# This block prints the final results and should be at the same indentation level\n",
        "# as the 'if temporal_features_aggregated.height > 0:' block above\n",
        "# (i.e., still inside the main 'if not action_level_df.is_empty():')\n",
        "print(\"\\nFinal assignment-level DataFrame for modeling (head):\")\n",
        "print(assignment_level_final_df.head())\n",
        "print(f\"Shape of assignment_level_final_df: {assignment_level_final_df.shape}\")\n",
        "\n",
        "if \"is_at_risk_target\" in assignment_level_final_df.columns:\n",
        "    print(\"\\nTarget variable distribution at assignment level:\")\n",
        "    print(assignment_level_final_df.group_by(\"is_at_risk_target\").agg(pl.len().alias(\"count\")))\n",
        "\n",
        "# The final 'else' for 'if not action_level_df.is_empty():' would be further down,\n",
        "# at the same indentation level as that initial 'if'.\n",
        "# else:\n",
        "#     print(\"action_level_df is empty. Cannot proceed with temporal feature engineering.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
