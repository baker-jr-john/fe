{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Research Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Educational researchers and data scientists face significant challenges in identifying struggling students early enough to provide timely interventions in online learning environments. While there are extensive data available, extracting meaningful predictive signals from student interactions remains difficult, particularly within the first portion of assignments when intervention would be most valuable. [The ASSISTments dataset](https://osf.io/59shv/files/osfstorage) provides a unique opportunity to address this challenge through its comprehensive data from 88 distinct assignment-level randomized controlled experiments conducted within [the ASSISTments platform](https://www.assistments.org/). This collection, analyzed initially in [Prihar et al.'s 2022 paper *Exploring Common Trends in Online Educational Experiments*](https://osf.io/f58dz), includes detailed clickstream data that captures temporal aspects of student engagement across diverse educational interventions. The rich multi-level student interaction data enables the development and evaluation of early warning systems that could identify struggling students before they fall significantly behind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Research Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How can temporal engagement features derived from clickstream data in the ASSISTments experimental dataset predict student performance drops across different intervention types, and which feature selection methods most effectively identify at-risk students within the first 25% of an assignment? This research will leverage the dataset's granular student interaction logs to extract time-based engagement patterns, analyze how these patterns correlate with performance outcomes, and determine which combinations of features provide the earliest reliable signals of academic struggle across different intervention conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ9f09tc34r0"
      },
      "source": [
        "# Load, Merge, and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Configuration\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "import gc\n",
        "\n",
        "# --- Enable Global String Cache for Categoricals ---\n",
        "pl.enable_string_cache()\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_DATA_PATH = Path('/Users/john/Downloads/osfstorage-archive')\n",
        "EXPERIMENT_IDS_PATH = BASE_DATA_PATH / 'experiment_dataset_2021-09-23'\n",
        "\n",
        "# Output path for the cleaned data\n",
        "SAVE_CLEANED_PATH_POLARS_PARQUET = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.parquet'\n",
        "SAVE_CLEANED_PATH_POLARS_CSV = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.csv'\n",
        "\n",
        "print(f\"Polars version: {pl.__version__}\")\n",
        "print(f\"Base data path: {BASE_DATA_PATH}\")\n",
        "print(f\"Experiment IDs path: {EXPERIMENT_IDS_PATH}\")\n",
        "print(f\"Global String Cache enabled: {pl.using_string_cache()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate File Paths\n",
        "try:\n",
        "    if not EXPERIMENT_IDS_PATH.is_dir():\n",
        "        raise FileNotFoundError(f\"Error: Directory not found at {EXPERIMENT_IDS_PATH}\")\n",
        "    experiment_ids = [d.name for d in EXPERIMENT_IDS_PATH.iterdir() if d.is_dir()]\n",
        "    print(f\"Found {len(experiment_ids)} experiment ID directories.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    experiment_ids = []\n",
        "\n",
        "performance_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_alogs.csv') for exp_id in experiment_ids]\n",
        "problems_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_plogs.csv') for exp_id in experiment_ids]\n",
        "actions_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_slogs.csv') for exp_id in experiment_ids]\n",
        "metrics_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'priors.csv') for exp_id in experiment_ids]\n",
        "\n",
        "print(\"Sample performance file paths:\", performance_file_paths[:2])\n",
        "print(\"Sample problems file paths:\", problems_file_paths[:2])\n",
        "print(\"Sample actions file paths:\", actions_file_paths[:2])\n",
        "print(\"Sample metrics file paths:\", metrics_file_paths[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Schemas and Date Parsing Information\n",
        "\n",
        "actions_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'problem_id': pl.Categorical,\n",
        "    'problem_part': pl.Categorical, \n",
        "    'scaffold_id': pl.Categorical,  \n",
        "    'experiment_tag_path': pl.Utf8,\n",
        "    'action': pl.Categorical,\n",
        "    'timestamp': pl.Utf8,\n",
        "    'assistments_reference_action_log_id': pl.UInt64\n",
        "}\n",
        "actions_parse_dates = ['timestamp']\n",
        "\n",
        "problems_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'problem_id': pl.Categorical,\n",
        "    'problem_part': pl.Categorical, \n",
        "    'scaffold_id': pl.Categorical,  \n",
        "    'problem_condition': pl.Categorical,\n",
        "    'start_time': pl.Utf8,\n",
        "    'end_time': pl.Utf8,\n",
        "    'session_count': pl.UInt16,\n",
        "    'time_on_task': pl.Float32,\n",
        "    'first_response_or_request_time': pl.Float32,\n",
        "    'first_answer': pl.Utf8,\n",
        "    'correct': pl.Boolean,\n",
        "    'reported_score': pl.Float32,\n",
        "    'answer_before_tutoring': pl.Boolean,\n",
        "    'attempt_count': pl.UInt16,\n",
        "    'hints_available': pl.UInt16,\n",
        "    'hints_given': pl.UInt16,\n",
        "    'scaffold_problems_available': pl.UInt16,\n",
        "    'scaffold_problems_given': pl.UInt16,\n",
        "    'explanation_available': pl.Boolean,\n",
        "    'explanation_given': pl.Boolean,\n",
        "    'answer_given': pl.Boolean,\n",
        "    'assistments_reference_problem_log_id': pl.UInt64\n",
        "}\n",
        "problems_parse_dates = ['start_time', 'end_time']\n",
        "\n",
        "performance_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'release_date': pl.Utf8,\n",
        "    'due_date': pl.Utf8,\n",
        "    'start_time': pl.Utf8,\n",
        "    'end_time': pl.Utf8,\n",
        "    'assignment_session_count': pl.Float32,\n",
        "    'pretest_problem_count': pl.Float32,\n",
        "    'pretest_correct': pl.Float32,\n",
        "    'pretest_time_on_task': pl.Float32,\n",
        "    'pretest_average_first_response_time': pl.Float32,\n",
        "    'pretest_session_count': pl.Float32,\n",
        "    'assigned_condition': pl.Categorical,\n",
        "    'condition_time_on_task': pl.Float32,\n",
        "    'condition_average_first_response_or_request_time': pl.Float32,\n",
        "    'condition_problem_count': pl.Float32,\n",
        "    'condition_total_correct': pl.Float32,\n",
        "    'condition_total_correct_after_wrong_response': pl.Float32,\n",
        "    'condition_total_correct_after_tutoring': pl.Float32,\n",
        "    'condition_total_answers_before_tutoring': pl.Float32,\n",
        "    'condition_total_attempt_count': pl.Float32,\n",
        "    'condition_total_hints_available': pl.Float32,\n",
        "    'condition_total_hints_given': pl.Float32,\n",
        "    'condition_total_scaffold_problems_available': pl.Float32,\n",
        "    'condition_total_scaffold_problems_given': pl.Float32,\n",
        "    'condition_total_explanations_available': pl.Float32,\n",
        "    'condition_total_explanations_given': pl.Float32,\n",
        "    'condition_total_answers_given': pl.Float32,\n",
        "    'condition_session_count': pl.Float32,\n",
        "    'posttest_problem_count': pl.Float32,\n",
        "    'posttest_correct': pl.Float32,\n",
        "    'posttest_time_on_task': pl.Float32,\n",
        "    'posttest_average_first_response_time': pl.Float32,\n",
        "    'posttest_session_count': pl.Float32,\n",
        "    'assistments_reference_assignment_log_id': pl.UInt64\n",
        "}\n",
        "performance_parse_dates = ['release_date', 'due_date', 'start_time', 'end_time']\n",
        "\n",
        "metrics_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'student_prior_started_skill_builder_count': pl.UInt32,\n",
        "    'student_prior_completed_skill_builder_count': pl.UInt32,\n",
        "    'student_prior_started_problem_set_count': pl.UInt32,\n",
        "    'student_prior_completed_problem_set_count': pl.UInt32,\n",
        "    'student_prior_completed_problem_count': pl.UInt32,\n",
        "    'student_prior_median_first_response_time': pl.Float32,\n",
        "    'student_prior_median_time_on_task': pl.Float32,\n",
        "    'student_prior_average_correctness': pl.Float32,\n",
        "    'student_prior_average_attempt_count': pl.Float32,\n",
        "    'class_id': pl.Categorical,\n",
        "    'class_creation_date': pl.Utf8,\n",
        "    'class_student_count': pl.UInt16,\n",
        "    'class_prior_skill_builder_count': pl.UInt32,\n",
        "    'class_prior_problem_set_count': pl.UInt32,\n",
        "    'class_prior_skill_builder_percent_started': pl.Float32,\n",
        "    'class_prior_skill_builder_percent_completed': pl.Float32,\n",
        "    'class_prior_problem_set_percent_started': pl.Float32,\n",
        "    'class_prior_problem_set_percent_completed': pl.Float32,\n",
        "    'class_prior_completed_problem_count': pl.UInt32,\n",
        "    'class_prior_median_time_on_task': pl.Float32,\n",
        "    'class_prior_median_first_response_time': pl.Float32,\n",
        "    'class_prior_average_correctness': pl.Float32,\n",
        "    'class_prior_average_attempt_count': pl.Float32,\n",
        "    'teacher id': pl.Categorical, \n",
        "    'teacher_account_creation_date': pl.Utf8,\n",
        "    'district_id': pl.Categorical,\n",
        "    'location': pl.Categorical,\n",
        "    'opportunity_zone': pl.Categorical,\n",
        "    'locale_description': pl.Categorical\n",
        "}\n",
        "metrics_parse_dates = ['class_creation_date', 'teacher_account_creation_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper Function for Memory-Efficient CSV Concatenation\n",
        "\n",
        "def combine_polars_csvs(file_paths, schema=None, parse_dates_list=None,\n",
        "                        known_date_format_str: str = None,\n",
        "                        date_time_unit='us'):\n",
        "    lazy_frames = []\n",
        "    print(f\"\\nScanning {len(file_paths)} files...\")\n",
        "\n",
        "    common_columns_from_first_file = None\n",
        "    if file_paths and schema:\n",
        "        try:\n",
        "            common_columns_from_first_file = pl.scan_csv(\n",
        "                file_paths[0], infer_schema_length=100, n_rows=10\n",
        "            ).collect_schema().names()\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not determine common columns from first file {file_paths[0]}: {e}\")\n",
        "            common_columns_from_first_file = list(schema.keys())\n",
        "\n",
        "    problematic_file_for_date_parse = None\n",
        "    current_col_for_date_parse = \"unknown\"\n",
        "\n",
        "    for i, file_path_str in enumerate(file_paths):\n",
        "        file_path = Path(file_path_str)\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Scanning file {i+1}/{len(file_paths)}: {file_path.parent.name}/{file_path.name}\")\n",
        "\n",
        "        try:\n",
        "            lf = pl.scan_csv(file_path,\n",
        "                             schema=schema,\n",
        "                             infer_schema_length=100,\n",
        "                             null_values=[\"\", \"NA\", \"NaN\", \"null\"])\n",
        "\n",
        "            if parse_dates_list:\n",
        "                date_parsing_expressions = []\n",
        "                columns_to_check_for_dates = common_columns_from_first_file if common_columns_from_first_file else lf.collect_schema().names()\n",
        "\n",
        "                for col_name in parse_dates_list:\n",
        "                    current_col_for_date_parse = col_name\n",
        "                    if col_name in columns_to_check_for_dates:\n",
        "                        if col_name not in lf.collect_schema().names():\n",
        "                            continue\n",
        "\n",
        "                        date_expr = pl.col(col_name).cast(pl.Utf8, strict=False)\n",
        "\n",
        "                        if known_date_format_str:\n",
        "                            date_expr = date_expr.str.to_datetime(\n",
        "                                format=known_date_format_str,\n",
        "                                strict=False,\n",
        "                                time_unit=date_time_unit\n",
        "                            )\n",
        "                        else:\n",
        "                            date_expr = date_expr.str.to_datetime(\n",
        "                                strict=False,\n",
        "                                time_unit=date_time_unit\n",
        "                            )\n",
        "                        date_parsing_expressions.append(\n",
        "                            date_expr.dt.convert_time_zone(\"UTC\").alias(col_name)\n",
        "                        )\n",
        "                if date_parsing_expressions:\n",
        "                    lf = lf.with_columns(date_parsing_expressions)\n",
        "\n",
        "            lazy_frames.append(lf)\n",
        "            problematic_file_for_date_parse = None\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  Warning: File not found, skipping: {file_path}\")\n",
        "        except pl.exceptions.NoDataError:\n",
        "             print(f\"  Warning: File is empty, skipping: {file_path}\")\n",
        "        except Exception as e:\n",
        "            problematic_file_for_date_parse = file_path\n",
        "            if \"strptime\" in str(e).lower() or \"conversion\" in str(e).lower() or \"datetime\" in str(e).lower():\n",
        "                 print(f\"  Potential date parsing error for {problematic_file_for_date_parse} (column likely '{current_col_for_date_parse}'): {e}\")\n",
        "            else:\n",
        "                print(f\"  Error scanning {file_path} or applying initial transforms: {e}\")\n",
        "\n",
        "\n",
        "    if not lazy_frames:\n",
        "        print(\"  No lazy frames were created from scanning files.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Concatenating {len(lazy_frames)} lazy frames...\")\n",
        "    try:\n",
        "        combined_lf = pl.concat(lazy_frames, how=\"vertical_relaxed\")\n",
        "        print(\"Collecting data into DataFrame (streaming enabled)...\")\n",
        "        # Reverted to engine=\"streaming\" as per deprecation warning for Polars 1.29.0\n",
        "        collected_df = combined_lf.collect(engine=\"streaming\")\n",
        "        print(\"Concatenation and collection complete.\")\n",
        "        return collected_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error during lazy concatenation or collection: {e}\")\n",
        "        if problematic_file_for_date_parse:\n",
        "            print(f\"  This might be related to an earlier issue in file: {problematic_file_for_date_parse}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load DataFrames\n",
        "\n",
        "COMMON_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S%.f%z\" \n",
        "\n",
        "print(\"Combining Actions Data (exp_slogs)...\")\n",
        "actions_df = combine_polars_csvs(\n",
        "    actions_file_paths, \n",
        "    schema=actions_schema, \n",
        "    parse_dates_list=actions_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT \n",
        ")\n",
        "if actions_df is not None:\n",
        "    print(f\"Actions DataFrame shape: {actions_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Problems Data (exp_plogs)...\")\n",
        "problems_df = combine_polars_csvs(\n",
        "    problems_file_paths, \n",
        "    schema=problems_schema, \n",
        "    parse_dates_list=problems_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT \n",
        ")\n",
        "if problems_df is not None:\n",
        "    print(f\"Problems DataFrame shape: {problems_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Performance Data (exp_alogs)...\")\n",
        "performance_df = combine_polars_csvs(\n",
        "    performance_file_paths, \n",
        "    schema=performance_schema, \n",
        "    parse_dates_list=performance_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT\n",
        ")\n",
        "if performance_df is not None:\n",
        "    print(f\"Performance DataFrame shape: {performance_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Metrics Data (priors)...\")\n",
        "if 'teacher id' in metrics_schema: \n",
        "    metrics_schema_corrected = metrics_schema.copy()\n",
        "    metrics_schema_corrected['teacher_id'] = metrics_schema_corrected.pop('teacher id')\n",
        "else:\n",
        "    metrics_schema_corrected = metrics_schema\n",
        "\n",
        "metrics_df = combine_polars_csvs(\n",
        "    metrics_file_paths, \n",
        "    schema=metrics_schema_corrected, \n",
        "    parse_dates_list=metrics_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT\n",
        ")\n",
        "if metrics_df is not None:\n",
        "    if 'teacher id' in metrics_df.columns: \n",
        "        metrics_df = metrics_df.rename({'teacher id': 'teacher_id'})\n",
        "    print(f\"Metrics DataFrame shape: {metrics_df.shape}\")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge DataFrames into One\n",
        "\n",
        "merged_df = None\n",
        "merge_successful = True\n",
        "\n",
        "print(\"\\n--- Starting Merge Operations ---\")\n",
        "\n",
        "if actions_df is None or actions_df.is_empty():\n",
        "    print(\"Actions DataFrame is empty or None. Cannot proceed with merge.\")\n",
        "    merge_successful = False\n",
        "else:\n",
        "    merged_df = actions_df.clone()\n",
        "    print(f\"Starting with actions_df: {merged_df.shape}\")\n",
        "\n",
        "    # Merge Problems Data\n",
        "    if problems_df is not None and not problems_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging problems_df...\")\n",
        "            problem_keys = ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id']\n",
        "            merged_df = merged_df.join(problems_df, on=problem_keys, how=\"left\", suffix=\"_problem\")\n",
        "            print(f\"After merging problems_df: {merged_df.shape}\")\n",
        "            del problems_df \n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging problems_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful: \n",
        "        print(\"Skipping problems_df merge (not loaded or empty).\")\n",
        "\n",
        "    # Merge Performance Data\n",
        "    if performance_df is not None and not performance_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging performance_df...\")\n",
        "            perf_keys = ['experiment_id', 'student_id']\n",
        "            merged_df = merged_df.join(performance_df, on=perf_keys, how=\"left\", suffix=\"_perf\")\n",
        "            print(f\"After merging performance_df: {merged_df.shape}\")\n",
        "            del performance_df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging performance_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful:\n",
        "        print(\"Skipping performance_df merge (not loaded or empty).\")\n",
        "\n",
        "    # Merge Metrics Data\n",
        "    if metrics_df is not None and not metrics_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging metrics_df...\")\n",
        "            metrics_keys = ['experiment_id', 'student_id']\n",
        "            merged_df = merged_df.join(metrics_df, on=metrics_keys, how=\"left\", suffix=\"_metrics\")\n",
        "            print(f\"After merging metrics_df: {merged_df.shape}\")\n",
        "            del metrics_df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging metrics_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful:\n",
        "        print(\"Skipping metrics_df merge (not loaded or empty).\")\n",
        "\n",
        "    if merged_df is not None and merge_successful:\n",
        "        print(\"\\n--- Merge Complete ---\")\n",
        "        print(\"Final Merged DataFrame Info:\")\n",
        "        print(f\"Shape: {merged_df.shape}\")\n",
        "        print(\"Columns in merged_df:\", merged_df.columns)\n",
        "        print(merged_df.head)\n",
        "        print(merged_df.schema)\n",
        "        \n",
        "        if 'actions_df' in locals() and actions_df is not merged_df: \n",
        "            del actions_df\n",
        "            gc.collect()\n",
        "            \n",
        "    elif merged_df is not None: \n",
        "        print(\"\\n--- Merge Partially Complete or Some DataFrames Skipped ---\")\n",
        "        print(\"Columns in partially merged_df:\", merged_df.columns)\n",
        "    else: \n",
        "        print(\"\\n--- Merge Failed or Base DataFrame (actions_df) was not suitable ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8NxlbW3ynlT"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Cleaning\n",
        "\n",
        "if 'merged_df' in locals() and merged_df is not None and merge_successful: \n",
        "    print(\"\\n--- Starting Data Cleaning ---\")\n",
        "    print(f\"Initial merged_df shape for cleaning: {merged_df.shape}\")\n",
        "\n",
        "    print(\"\\n--- Renaming Columns ---\")\n",
        "    rename_map = {\n",
        "        'assistments_reference_action_log_id': 'action_log_id',\n",
        "        'start_time_perf': 'assignment_start_time',\n",
        "        'end_time_perf': 'assignment_end_time',\n",
        "        'assistments_reference_assignment_log_id': 'assignment_log_id'\n",
        "    }\n",
        "    actual_renames = {k: v for k, v in rename_map.items() if k in merged_df.columns}\n",
        "    if actual_renames:\n",
        "        print(f\"Applying renames: {actual_renames}\")\n",
        "        merged_df = merged_df.rename(actual_renames)\n",
        "    else:\n",
        "        print(\"No columns matched for renaming based on the current rename_map.\")\n",
        "\n",
        "    column_transformations = []\n",
        "\n",
        "    datetime_cols_final_check = [\n",
        "        'timestamp', 'start_time', 'end_time', 'release_date', 'due_date',\n",
        "        'assignment_start_time', 'assignment_end_time',\n",
        "        'class_creation_date', 'teacher_account_creation_date'\n",
        "    ]\n",
        "    for col_name in datetime_cols_final_check:\n",
        "        if col_name in merged_df.columns:\n",
        "            current_dtype = merged_df[col_name].dtype\n",
        "            if current_dtype == pl.Utf8:\n",
        "                print(f\"Scheduled for datetime re-parsing (UTF8 found): {col_name}\")\n",
        "                column_transformations.append(\n",
        "                    pl.col(col_name).str.to_datetime(format=COMMON_DATETIME_FORMAT, strict=False, time_unit='us')\n",
        "                    .dt.convert_time_zone(\"UTC\")\n",
        "                    .alias(col_name)\n",
        "                )\n",
        "            elif isinstance(current_dtype, pl.Datetime):\n",
        "                current_tz = current_dtype.time_zone\n",
        "                if current_tz is None:\n",
        "                    print(f\"Info: Datetime column '{col_name}' is naive. Localizing to UTC.\")\n",
        "                    column_transformations.append(\n",
        "                        pl.col(col_name).dt.replace_time_zone(\"UTC\", ambiguous='earliest').alias(col_name)\n",
        "                    )\n",
        "                elif current_tz != \"UTC\":\n",
        "                    print(f\"Scheduled for UTC conversion (already datetime, was '{current_tz}'): {col_name}\")\n",
        "                    column_transformations.append(\n",
        "                        pl.col(col_name).dt.convert_time_zone(\"UTC\").alias(col_name)\n",
        "                    )\n",
        "\n",
        "    cols_to_category_polars = [\n",
        "        'experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id',\n",
        "        'experiment_tag_path', 'action', 'problem_condition', 'assigned_condition',\n",
        "        'class_id', 'district_id', 'location', 'opportunity_zone',\n",
        "        'locale_description', 'teacher_id'\n",
        "    ]\n",
        "    for col_name in cols_to_category_polars:\n",
        "        if col_name in merged_df.columns and merged_df[col_name].dtype != pl.Categorical:\n",
        "             column_transformations.append(pl.col(col_name).cast(pl.Categorical).alias(col_name))\n",
        "             print(f\"Scheduled for categorical conversion: {col_name}\")\n",
        "\n",
        "    float_to_int_casts = {\n",
        "        'assignment_session_count': pl.UInt16, 'pretest_problem_count': pl.UInt16,\n",
        "        'pretest_correct': pl.UInt16, 'pretest_session_count': pl.UInt16,\n",
        "        'condition_problem_count': pl.UInt16, 'condition_total_correct': pl.UInt16,\n",
        "        'condition_total_correct_after_wrong_response': pl.UInt16,\n",
        "        'condition_total_correct_after_tutoring': pl.UInt16,\n",
        "        'condition_total_answers_before_tutoring': pl.UInt16,\n",
        "        'condition_total_attempt_count': pl.UInt32,\n",
        "        'condition_total_hints_available': pl.UInt32, 'condition_total_hints_given': pl.UInt32,\n",
        "        'condition_total_scaffold_problems_available': pl.UInt32,\n",
        "        'condition_total_scaffold_problems_given': pl.UInt32,\n",
        "        'condition_total_explanations_available': pl.UInt32,\n",
        "        'condition_total_explanations_given': pl.UInt32,\n",
        "        'condition_total_answers_given': pl.UInt32,\n",
        "        'condition_session_count': pl.UInt16, 'posttest_problem_count': pl.UInt16,\n",
        "        'posttest_correct': pl.UInt16, 'posttest_session_count': pl.UInt16,\n",
        "    }\n",
        "    for col_name, target_int_type in float_to_int_casts.items():\n",
        "        if col_name in merged_df.columns:\n",
        "            if merged_df[col_name].dtype == pl.Float32:\n",
        "                column_transformations.append(\n",
        "                    pl.col(col_name)\n",
        "                      .fill_null(0)\n",
        "                      .cast(target_int_type, strict=False)\n",
        "                      .alias(col_name)\n",
        "                )\n",
        "                print(f\"Scheduled '{col_name}' for Float32 to {target_int_type} conversion.\")\n",
        "            elif merged_df[col_name].dtype != target_int_type:\n",
        "                print(f\"Warning: Column '{col_name}' was expected to be Float32 for int conversion, but found {merged_df[col_name].dtype}. Skipping specific int cast.\")\n",
        "\n",
        "    # General Float64 to Float32 pass\n",
        "    float64_cols = [col_name for col_name, dtype in merged_df.schema.items() if dtype == pl.Float64]\n",
        "    for col_name in float64_cols:\n",
        "        if col_name in merged_df.columns:\n",
        "            column_transformations.append(pl.col(col_name).cast(pl.Float32).alias(col_name))\n",
        "            print(f\"Scheduled for Float64 to Float32 conversion: {col_name}\")\n",
        "\n",
        "    if column_transformations:\n",
        "        print(\"\\nApplying column type transformations...\")\n",
        "        merged_df = merged_df.with_columns(column_transformations)\n",
        "        print(\"Type transformations applied.\")\n",
        "\n",
        "    print(\"\\n--- Specific Value Cleaning ---\")\n",
        "    specific_value_cleaning_expressions = []\n",
        "\n",
        "    if 'opportunity_zone' in merged_df.columns:\n",
        "        if merged_df['opportunity_zone'].dtype != pl.Categorical:\n",
        "             merged_df = merged_df.with_columns(pl.col('opportunity_zone').cast(pl.Categorical))\n",
        "        specific_value_cleaning_expressions.append(\n",
        "            pl.when(pl.col('opportunity_zone').cast(pl.Utf8) == \"Yes\").then(True)\n",
        "              .when(pl.col('opportunity_zone').cast(pl.Utf8) == \"No\").then(False)\n",
        "              .otherwise(None)\n",
        "              .cast(pl.Boolean)\n",
        "              .alias('opportunity_zone_bool')\n",
        "        )\n",
        "        print(\"Scheduled 'opportunity_zone' to boolean 'opportunity_zone_bool' conversion.\")\n",
        "\n",
        "    cat_cols_to_fill_info = {\n",
        "        'district_id': 'Unknown_District',\n",
        "        'location': 'Unknown_Location',\n",
        "        'locale_description': 'Unknown_Locale'\n",
        "    }\n",
        "    for col_name, fill_val in cat_cols_to_fill_info.items():\n",
        "        if col_name in merged_df.columns:\n",
        "            if merged_df[col_name].dtype != pl.Categorical:\n",
        "                merged_df = merged_df.with_columns(pl.col(col_name).cast(pl.Categorical))\n",
        "                print(f\"Casted '{col_name}' to Categorical before fill_null.\")\n",
        "            specific_value_cleaning_expressions.append(pl.col(col_name).fill_null(pl.lit(fill_val).cast(pl.Categorical)).alias(col_name))\n",
        "            print(f\"Scheduled fill_null for categorical {col_name} with '{fill_val}'.\")\n",
        "\n",
        "    if specific_value_cleaning_expressions:\n",
        "        print(\"\\nApplying specific value cleaning expressions...\")\n",
        "        merged_df = merged_df.with_columns(specific_value_cleaning_expressions)\n",
        "        print(\"Specific value cleaning applied.\")\n",
        "\n",
        "    pandas_identified_empty_cols = [\n",
        "         'problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task',\n",
        "         'first_response_or_request_time', 'first_answer', 'correct', 'reported_score',\n",
        "         'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given',\n",
        "         'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available',\n",
        "         'explanation_given', 'answer_given',\n",
        "         'assistments_reference_problem_log_id'\n",
        "    ]\n",
        "    actual_empty_cols_to_drop = []\n",
        "    if not merged_df.is_empty():\n",
        "        for col_name in pandas_identified_empty_cols:\n",
        "            if col_name in merged_df.columns and merged_df[col_name].is_null().all():\n",
        "                actual_empty_cols_to_drop.append(col_name)\n",
        "            elif col_name in merged_df.columns:\n",
        "                null_count = merged_df[col_name].is_null().sum()\n",
        "                if null_count > 0 :\n",
        "                    print(f\"Info: Column '{col_name}' (candidate for empty drop) was not fully null. Nulls: {null_count}/{merged_df.height}\")\n",
        "\n",
        "    if actual_empty_cols_to_drop:\n",
        "        print(f\"\\nDropping fully empty columns: {actual_empty_cols_to_drop}\")\n",
        "        merged_df = merged_df.drop(actual_empty_cols_to_drop)\n",
        "    else:\n",
        "        print(\"\\nNo fully empty columns (from the predefined list) identified for dropping.\")\n",
        "\n",
        "    if 'opportunity_zone' in merged_df.columns and 'opportunity_zone_bool' in merged_df.columns:\n",
        "        print(\"Dropping original opportunity zone column: 'opportunity_zone'\")\n",
        "        merged_df = merged_df.drop('opportunity_zone')\n",
        "\n",
        "    print(f\"\\nShape after Cleaning: {merged_df.shape}\")\n",
        "    print(\"Columns after cleaning:\", merged_df.columns)\n",
        "    gc.collect()\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Cell 7 cleaning: merged_df not available, previous merge failed, or merge_successful flag is False.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Reduced DataFrame and Save\n",
        "\n",
        "if 'merged_df' in locals() and merged_df is not None and merge_successful: # Check if merged_df exists\n",
        "    print(\"\\n--- Reducing DataFrame to Essential Columns ---\")\n",
        "\n",
        "    essential_cols_to_keep_polars = [\n",
        "        # Keys / Base Info from actions_df\n",
        "        'experiment_id',\n",
        "        'student_id',\n",
        "        'problem_id', # <--- ADD THIS LINE\n",
        "        'timestamp',\n",
        "        'action',\n",
        "        'action_log_id', # This was 'assistments_reference_action_log_id'\n",
        "\n",
        "        # From performance_df\n",
        "        'assignment_start_time', # This was 'start_time_perf'\n",
        "        'assignment_end_time',   # This was 'end_time_perf'\n",
        "        'assignment_log_id',     # This was 'assistments_reference_assignment_log_id'\n",
        "        'assignment_session_count',\n",
        "        'condition_problem_count',\n",
        "        'condition_time_on_task',\n",
        "        'condition_average_first_response_or_request_time',\n",
        "        'condition_total_correct',\n",
        "        'condition_total_attempt_count',\n",
        "        'condition_total_hints_given',\n",
        "        'condition_total_explanations_given',\n",
        "\n",
        "        # From metrics_df\n",
        "        'student_prior_average_correctness',\n",
        "\n",
        "        'opportunity_zone_bool', # This was derived and original 'opportunity_zone' dropped\n",
        "    ]\n",
        "\n",
        "    # Filter to only include columns that actually exist in the cleaned merged_df\n",
        "    final_essential_columns = [col for col in essential_cols_to_keep_polars if col in merged_df.columns]\n",
        "\n",
        "    # Check if problem_id was found in merged_df.columns\n",
        "    if 'problem_id' not in final_essential_columns and 'problem_id' in essential_cols_to_keep_polars:\n",
        "        print(\"WARNING: 'problem_id' was requested but not found in merged_df.columns. It will not be included.\")\n",
        "        print(f\"Available columns in merged_df: {merged_df.columns}\")\n",
        "\n",
        "\n",
        "    print(f\"Attempting to select these {len(final_essential_columns)} essential columns: {final_essential_columns}\")\n",
        "    missing_essentials_for_reduction = [col for col in essential_cols_to_keep_polars if col not in final_essential_columns]\n",
        "\n",
        "    if missing_essentials_for_reduction:\n",
        "        print(f\"Warning: The following conceptual essential columns were NOT FOUND in merged_df for reduction: {missing_essentials_for_reduction}\")\n",
        "        print(\"Please ensure their names are correct in the 'essential_cols_to_keep_polars' list and they exist in the output of Cell 7.\")\n",
        "\n",
        "    if not final_essential_columns:\n",
        "        print(\"Error: No essential columns available for selection based on your list. Cannot create reduced DataFrame.\")\n",
        "        merged_df_reduced = None\n",
        "    else:\n",
        "        try:\n",
        "            merged_df_reduced = merged_df.select(final_essential_columns) # This line uses the updated list\n",
        "            print(f\"\\nReduced DataFrame Info: Shape {merged_df_reduced.shape}\")\n",
        "            print(merged_df_reduced.head())\n",
        "            print(merged_df_reduced.schema) # Check the schema here to confirm problem_id\n",
        "\n",
        "            print(f\"\\nAttempting to save cleaned and reduced DataFrame to: {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "            merged_df_reduced.write_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET)\n",
        "            print(f\"Successfully saved to {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during final select or save: {e}\")\n",
        "            merged_df_reduced = None\n",
        "\n",
        "    if 'merged_df' in locals(): # Check if merged_df exists before trying to delete\n",
        "        del merged_df\n",
        "        gc.collect()\n",
        "        print(\"\\nFull cleaned merged_df deleted from memory.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Cell 8 (reduction and save): merged_df not available from Cell 7 or previous steps failed.\")\n",
        "    merged_df_reduced = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Cleaned Data\n",
        "\n",
        "if 'merged_df_reduced' in locals() and merged_df_reduced is not None and not merged_df_reduced.is_empty() and SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "    print(f\"\\n--- Loading Cleaned Parquet File ---\")\n",
        "    try:\n",
        "        df_reloaded_polars = pl.read_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET)\n",
        "        print(f\"Successfully reloaded: {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "        print(f\"Reloaded DataFrame Shape: {df_reloaded_polars.shape}\")\n",
        "        print(\"\\nReloaded DataFrame Head (from Parquet):\")\n",
        "        print(df_reloaded_polars.head())\n",
        "        print(\"\\nReloaded DataFrame Schema (from Parquet):\")\n",
        "        print(df_reloaded_polars.schema)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reloading the cleaned Parquet file: {e}\")\n",
        "elif SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "     print(f\"Cleaned Parquet file found at {SAVE_CLEANED_PATH_POLARS_PARQUET}, but merged_df_reduced may not have been successfully created or was empty in the previous step (this script might have been re-run starting from here). Consider reloading manually if needed.\")\n",
        "else:\n",
        "    print(f\"\\nCleaned Parquet file not found at {SAVE_CLEANED_PATH_POLARS_PARQUET} or reduction/save step failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add this in a new cell after Cell 9 runs\n",
        "if 'df_reloaded_polars' in locals():\n",
        "    print(\"Schema of df_reloaded_polars after adding problem_id:\")\n",
        "    print(df_reloaded_polars.schema)\n",
        "    print(f\"Columns: {df_reloaded_polars.columns}\")\n",
        "    if 'problem_id' in df_reloaded_polars.columns:\n",
        "        print(\"'problem_id' successfully included!\")\n",
        "        print(df_reloaded_polars.select(\"problem_id\").head()) # See some sample problem_id values\n",
        "    else:\n",
        "        print(\"ERROR: 'problem_id' is still NOT in df_reloaded_polars. Check Cell 8 logic and merged_df columns before select.\")\n",
        "else:\n",
        "    print(\"df_reloaded_polars not found. Ensure Cell 9 has run correctly.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure relevant columns are not null and condition_problem_count is not zero to avoid division by zero\n",
        "df_with_target = df_reloaded_polars.filter(\n",
        "    pl.col(\"condition_problem_count\").is_not_null() & (pl.col(\"condition_problem_count\") > 0) &\n",
        "    pl.col(\"condition_total_correct\").is_not_null()\n",
        ")\n",
        "\n",
        "# Calculate correctness percentage for the 'condition' part of the assignment\n",
        "df_with_target = df_with_target.with_columns(\n",
        "    (pl.col(\"condition_total_correct\") / pl.col(\"condition_problem_count\")).alias(\"condition_correctness_percentage\")\n",
        ")\n",
        "\n",
        "# Define a threshold for \"at-risk\" (e.g., less than 50% correct)\n",
        "# This threshold might need to be determined by domain knowledge or data exploration (e.g., quartiles)\n",
        "at_risk_threshold = 0.5\n",
        "df_with_target = df_with_target.with_columns(\n",
        "    pl.when(pl.col(\"condition_correctness_percentage\") < at_risk_threshold)\n",
        "    .then(True) # Student is at-risk\n",
        "    .otherwise(False) # Student is not at-risk\n",
        "    .alias(\"is_at_risk_target\")\n",
        ")\n",
        "\n",
        "# Display the new columns and their distribution\n",
        "print(df_with_target.head())\n",
        "print(df_with_target.group_by(\"is_at_risk_target\").agg(pl.len().alias(\"count\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure df_with_target from the previous cell (your target creation cell) exists\n",
        "if 'df_with_target' in locals() and isinstance(df_with_target, pl.DataFrame) and not df_with_target.is_empty():\n",
        "    print(\"Starting temporal feature engineering using 'df_with_target' as input.\")\n",
        "    action_level_df = df_with_target.sort([\"student_id\", \"assignment_log_id\", \"timestamp\"])\n",
        "else:\n",
        "    print(\"ERROR: 'df_with_target' not found or is empty. Please ensure the target variable creation cell (Cell 11) has run successfully.\")\n",
        "    action_level_df = pl.DataFrame()\n",
        "\n",
        "if not action_level_df.is_empty():\n",
        "    print(\"Proceeding with problem_id for 'first 25%' definition.\")\n",
        "\n",
        "    action_level_df = action_level_df.with_columns(\n",
        "        (pl.col(\"condition_problem_count\") * 0.25).floor().cast(pl.Int32)\n",
        "        .pipe(lambda s: pl.when(s == 0).then(1).otherwise(s))\n",
        "        .alias(\"first_25_percent_problem_threshold\")\n",
        "    )\n",
        "\n",
        "    actions_with_problem_id = action_level_df.filter(pl.col(\"problem_id\").is_not_null())\n",
        "\n",
        "    if not actions_with_problem_id.is_empty():\n",
        "        problem_order_df = actions_with_problem_id.group_by(\n",
        "            [\"student_id\", \"assignment_log_id\", \"problem_id\"]\n",
        "        ).agg(\n",
        "            pl.min(\"timestamp\").alias(\"first_seen_timestamp\")\n",
        "        ).sort([\"student_id\", \"assignment_log_id\", \"first_seen_timestamp\"]) # Sorting is important\n",
        "\n",
        "        # *** THIS IS THE MODIFIED LINE ***\n",
        "        problem_order_df = problem_order_df.with_columns(\n",
        "            pl.col(\"first_seen_timestamp\").rank(method=\"ordinal\").over([\"student_id\", \"assignment_log_id\"]).alias(\"problem_order_in_assignment\")\n",
        "        )\n",
        "        # *** END MODIFIED LINE ***\n",
        "\n",
        "        actions_with_problem_order = actions_with_problem_id.join(\n",
        "            problem_order_df.select([\"student_id\", \"assignment_log_id\", \"problem_id\", \"problem_order_in_assignment\"]),\n",
        "            on=[\"student_id\", \"assignment_log_id\", \"problem_id\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        early_actions_df = actions_with_problem_order.filter(\n",
        "            pl.col(\"problem_order_in_assignment\") <= pl.col(\"first_25_percent_problem_threshold\")\n",
        "        )\n",
        "        print(f\"Identified {early_actions_df.height} early actions based on problem order.\")\n",
        "    else:\n",
        "        print(\"No actions with problem_id found. Cannot create problem-based early_actions_df.\")\n",
        "        early_actions_df = pl.DataFrame()\n",
        "\n",
        "    # --- Part 2: Aggregate Temporal Features from early_actions_df ---\n",
        "    if not early_actions_df.is_empty() and early_actions_df.height > 0:\n",
        "        temporal_features_aggregated = early_actions_df.group_by(\n",
        "            [\"student_id\", \"assignment_log_id\"]\n",
        "        ).agg([\n",
        "            (pl.when(pl.col(\"action\") == \"hint_request\").then(1).otherwise(0)).sum().alias(\"early_hint_requests\"), # This comparison should be okay\n",
        "            \n",
        "            # Corrected line: cast 'action' to Utf8 before .str.contains()\n",
        "            (pl.when(pl.col(\"action\").cast(pl.Utf8).str.contains(r\"(?i)attempt|response\")).then(1).otherwise(0)).sum().alias(\"early_attempts_or_responses\"),\n",
        "            \n",
        "            (pl.col(\"timestamp\").max() - pl.col(\"timestamp\").min()).dt.total_seconds().alias(\"early_duration_seconds\"),\n",
        "            pl.n_unique(\"problem_id\").alias(\"early_distinct_problems_worked_on\"), # n_unique on Categorical is fine\n",
        "            (pl.len() / pl.col(\"problem_id\").n_unique()).alias(\"avg_actions_per_early_problem\") # n_unique on Categorical is fine\n",
        "        ])\n",
        "        print(\"Aggregated temporal features (problem-based window):\")\n",
        "        print(temporal_features_aggregated.head())\n",
        "    else:\n",
        "        print(\"No early actions identified or early_actions_df is empty, so no temporal features created.\")\n",
        "        temporal_features_aggregated = pl.DataFrame(schema={\n",
        "            \"student_id\": pl.Categorical, \"assignment_log_id\": pl.UInt64,\n",
        "            \"early_hint_requests\": pl.UInt32, \"early_attempts_or_responses\": pl.UInt32,\n",
        "            \"early_duration_seconds\": pl.Float64, \"early_distinct_problems_worked_on\": pl.UInt32,\n",
        "            \"avg_actions_per_early_problem\": pl.Float64\n",
        "        })\n",
        "\n",
        "    # --- Part 3: Create Assignment-Level Base Data and Join Temporal Features ---\n",
        "\n",
        "    assignment_level_final_df = action_level_df.group_by(\n",
        "        [\"student_id\", \"assignment_log_id\"]\n",
        "    ).agg([\n",
        "        pl.first(\"experiment_id\"),\n",
        "        pl.first(\"problem_id\"), # You might want the first problem_id or leave it out if it's not truly assignment-level\n",
        "        pl.first(\"assignment_start_time\"),\n",
        "        pl.first(\"assignment_end_time\"),\n",
        "        pl.first(\"assignment_session_count\"),\n",
        "        pl.first(\"condition_problem_count\"),\n",
        "        pl.first(\"condition_time_on_task\"),\n",
        "        pl.first(\"condition_average_first_response_or_request_time\"),\n",
        "        pl.first(\"condition_total_correct\"),\n",
        "        pl.first(\"condition_total_attempt_count\"),\n",
        "        pl.first(\"condition_total_hints_given\"),\n",
        "        pl.first(\"condition_total_explanations_given\"),\n",
        "        pl.first(\"student_prior_average_correctness\"),\n",
        "        pl.first(\"opportunity_zone_bool\"),\n",
        "        pl.first(\"condition_correctness_percentage\"), # Your calculated correctness\n",
        "        pl.first(\"is_at_risk_target\") # Your target variable\n",
        "    ]).drop_nulls(subset=[\"student_id\", \"assignment_log_id\"])\n",
        "\n",
        "\n",
        "# (Assuming 'assignment_level_final_df' has been created from Part 3 just before this)\n",
        "# (Assuming 'temporal_features_aggregated' has been created from Part 2 just before this)\n",
        "\n",
        "# Join the aggregated temporal features\n",
        "if temporal_features_aggregated.height > 0:  # This condition checks if there's data to join\n",
        "    print(\"Joining temporal features...\")\n",
        "    assignment_level_final_df = assignment_level_final_df.join(\n",
        "        temporal_features_aggregated,\n",
        "        on=[\"student_id\", \"assignment_log_id\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Fill NaNs for temporal columns that arose from the left join\n",
        "    temporal_cols_to_fill = [\n",
        "        \"early_hint_requests\", \"early_attempts_or_responses\",\n",
        "        \"early_duration_seconds\", \"early_distinct_problems_worked_on\",\n",
        "        \"avg_actions_per_early_problem\"\n",
        "    ]\n",
        "    fill_value = 0\n",
        "    for col_name in temporal_cols_to_fill: # START OF FOR LOOP\n",
        "        if col_name in assignment_level_final_df.columns:\n",
        "            # ... (your successful fill_null logic for each column, which includes:)\n",
        "            print(f\"Attempting to fill nulls in column: '{col_name}' with value: {fill_value}\")\n",
        "            current_dtype = assignment_level_final_df[col_name].dtype\n",
        "            print(f\"  Original dtype of '{col_name}': {current_dtype}\")\n",
        "\n",
        "            fill_expr = None\n",
        "            if current_dtype in [pl.Float32, pl.Float64]:\n",
        "                fill_expr = pl.lit(float(fill_value), dtype=current_dtype)\n",
        "            elif current_dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64]:\n",
        "                fill_expr = pl.lit(int(fill_value), dtype=current_dtype)\n",
        "            else:\n",
        "                print(f\"Warning: Column '{col_name}' (dtype: {current_dtype}) is not float or standard integer. Filling with default int literal.\")\n",
        "                fill_expr = pl.lit(int(fill_value))\n",
        "            \n",
        "            if fill_expr is not None:\n",
        "                try:\n",
        "                    assignment_level_final_df = assignment_level_final_df.with_columns(\n",
        "                        pl.col(col_name).fill_null(fill_expr).alias(col_name)\n",
        "                    )\n",
        "                    print(f\"  Successfully filled nulls in '{col_name}'. New dtype: {assignment_level_final_df[col_name].dtype}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR filling nulls in column '{col_name}' with expr {fill_expr}: {e}\")\n",
        "                    raise # Re-raise the exception\n",
        "            else:\n",
        "                print(f\"Could not determine fill expression for column '{col_name}' with dtype {current_dtype}\")\n",
        "        else:\n",
        "            print(f\"Warning: Expected temporal column '{col_name}' not found in assignment_level_final_df for fill_null.\")\n",
        "    # END OF FOR LOOP (NO 'else' directly attached to this 'for' loop)\n",
        "\n",
        "# This 'else' is now correctly paired with 'if temporal_features_aggregated.height > 0:'\n",
        "else:\n",
        "    print(\"Skipping join of temporal features as 'temporal_features_aggregated' was empty or not generated.\")\n",
        "    # Optionally, add empty columns to assignment_level_final_df here if temporal_features_aggregated was empty\n",
        "    # This ensures assignment_level_final_df always has the columns, even if filled with 0 or null.\n",
        "    # For example:\n",
        "    # temporal_cols_to_add_if_missing = [\n",
        "    #     (\"early_hint_requests\", pl.UInt32), (\"early_attempts_or_responses\", pl.UInt32),\n",
        "    #     (\"early_duration_seconds\", pl.Float64), (\"early_distinct_problems_worked_on\", pl.UInt32),\n",
        "    #     (\"avg_actions_per_early_problem\", pl.Float64)\n",
        "    # ]\n",
        "    # for col_name, col_type in temporal_cols_to_add_if_missing:\n",
        "    #     if col_name not in assignment_level_final_df.columns:\n",
        "    #         assignment_level_final_df = assignment_level_final_df.with_columns(\n",
        "    #             pl.lit(0, dtype=col_type).alias(col_name) # Fill with 0 of the expected type\n",
        "    #         )\n",
        "\n",
        "\n",
        "# This block prints the final results and should be at the same indentation level\n",
        "# as the 'if temporal_features_aggregated.height > 0:' block above\n",
        "# (i.e., still inside the main 'if not action_level_df.is_empty():')\n",
        "print(\"\\nFinal assignment-level DataFrame for modeling (head):\")\n",
        "print(assignment_level_final_df.head())\n",
        "print(f\"Shape of assignment_level_final_df: {assignment_level_final_df.shape}\")\n",
        "\n",
        "if \"is_at_risk_target\" in assignment_level_final_df.columns:\n",
        "    print(\"\\nTarget variable distribution at assignment level:\")\n",
        "    print(assignment_level_final_df.group_by(\"is_at_risk_target\").agg(pl.len().alias(\"count\")))\n",
        "\n",
        "# The final 'else' for 'if not action_level_df.is_empty():' would be further down,\n",
        "# at the same indentation level as that initial 'if'.\n",
        "# else:\n",
        "#     print(\"action_level_df is empty. Cannot proceed with temporal feature engineering.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Validation Code Cell\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "\n",
        "# Attempt to use existing assignment_level_final_df or load from Parquet\n",
        "if 'assignment_level_final_df' not in locals() or assignment_level_final_df is None or assignment_level_final_df.is_empty():\n",
        "    print(\"'assignment_level_final_df' not found in memory or is empty. Attempting to load from Parquet...\")\n",
        "    # Ensure SAVE_CLEANED_PATH_POLARS_PARQUET is defined, if not, define it (e.g. from a config cell or define directly)\n",
        "    if 'SAVE_CLEANED_PATH_POLARS_PARQUET' not in locals():\n",
        "        print(\"Warning: 'SAVE_CLEANED_PATH_POLARS_PARQUET' not defined. Using a default path.\")\n",
        "        # This default path might need adjustment based on your notebook's setup if the variable isn't carried over.\n",
        "        BASE_DATA_PATH = Path('/Users/john/Downloads/osfstorage-archive') # Example, adjust if necessary\n",
        "        SAVE_CLEANED_PATH_POLARS_PARQUET = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.parquet'\n",
        "    \n",
        "    if SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "        try:\n",
        "            assignment_level_final_df = pl.read_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET)\n",
        "            print(f\"Successfully loaded 'assignment_level_final_df' from {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Parquet file {SAVE_CLEANED_PATH_POLARS_PARQUET}: {e}\")\n",
        "            assignment_level_final_df = pl.DataFrame() # Create an empty DataFrame on error\n",
        "    else:\n",
        "        print(f\"Parquet file not found at {SAVE_CLEANED_PATH_POLARS_PARQUET}. 'assignment_level_final_df' remains undefined or empty.\")\n",
        "        assignment_level_final_df = pl.DataFrame() # Create an empty DataFrame if file not found\n",
        "else:\n",
        "    print(\"'assignment_level_final_df' is already in memory.\")\n",
        "\n",
        "# Proceed only if the DataFrame is not empty\n",
        "if not assignment_level_final_df.is_empty():\n",
        "    # 2. Print the shape of the DataFrame\n",
        "    print(\"\\n--- DataFrame Shape ---\")\n",
        "    print(assignment_level_final_df.shape)\n",
        "\n",
        "    # 3. List the names of the engineered temporal features\n",
        "    print(\"\\n--- Engineered Temporal Features ---\")\n",
        "    engineered_temporal_features = [\n",
        "        'early_hint_requests',\n",
        "        'early_attempts_or_responses',\n",
        "        'early_duration_seconds',\n",
        "        'early_distinct_problems_worked_on',\n",
        "        'avg_actions_per_early_problem'\n",
        "    ]\n",
        "    print(\"Expected temporal features:\", engineered_temporal_features)\n",
        "    actual_temporal_features_in_df = [col for col in engineered_temporal_features if col in assignment_level_final_df.columns]\n",
        "    print(\"Found temporal features in DataFrame:\", actual_temporal_features_in_df)\n",
        "    missing_temporal_features = [col for col in engineered_temporal_features if col not in assignment_level_final_df.columns]\n",
        "    if missing_temporal_features:\n",
        "        print(\"Warning: Missing temporal features:\", missing_temporal_features)\n",
        "\n",
        "    # 4. Identify the target variable\n",
        "    print(\"\\n--- Target Variable ---\")\n",
        "    target_variable = 'is_at_risk_target'\n",
        "    print(f\"Expected target variable: '{target_variable}'\")\n",
        "    if target_variable in assignment_level_final_df.columns:\n",
        "        print(f\"Target variable '{target_variable}' found in DataFrame.\")\n",
        "    else:\n",
        "        print(f\"Warning: Target variable '{target_variable}' NOT FOUND in DataFrame.\")\n",
        "\n",
        "    # 5. Print a sample of the DataFrame\n",
        "    print(\"\\n--- DataFrame Sample (Head) ---\")\n",
        "    print(assignment_level_final_df.head())\n",
        "else:\n",
        "    print(\"\\n'assignment_level_final_df' is empty or could not be loaded. Skipping validation steps.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Define Feature Lists\n",
        "import polars as pl\n",
        "import numpy as np # For NaN handling if needed by older sklearn versions\n",
        "\n",
        "temporal_features = [\n",
        "    'early_hint_requests',\n",
        "    'early_attempts_or_responses',\n",
        "    'early_duration_seconds',\n",
        "    'early_distinct_problems_worked_on',\n",
        "    'avg_actions_per_early_problem'\n",
        "]\n",
        "\n",
        "other_relevant_features = [\n",
        "    'assignment_session_count',\n",
        "    'condition_problem_count',\n",
        "    'condition_time_on_task',\n",
        "    'condition_average_first_response_or_request_time',\n",
        "    'condition_total_correct',\n",
        "    'condition_total_attempt_count',\n",
        "    'condition_total_hints_given',\n",
        "    'condition_total_explanations_given',\n",
        "    'student_prior_average_correctness',\n",
        "    'opportunity_zone_bool'\n",
        "]\n",
        "\n",
        "combined_features = temporal_features + other_relevant_features\n",
        "target_variable = 'is_at_risk_target'\n",
        "\n",
        "print(\"--- Feature Lists Defined ---\")\n",
        "print(f\"Temporal Features: {temporal_features}\")\n",
        "print(f\"Other Relevant Features: {other_relevant_features}\")\n",
        "print(f\"All Features for Analysis: {combined_features}\")\n",
        "print(f\"Target Variable: {target_variable}\")\n",
        "\n",
        "# Ensure assignment_level_final_df is available (it should be from the previous cell)\n",
        "if 'assignment_level_final_df' not in locals() or assignment_level_final_df.is_empty():\n",
        "    print(\"Error: 'assignment_level_final_df' is not available or empty. Please run the previous cell.\")\n",
        "else:\n",
        "    print(f\"'assignment_level_final_df' found with shape: {assignment_level_final_df.shape}\")\n",
        "    # Ensure all listed features actually exist in the DataFrame, warn if not\n",
        "    for feature_list_name, feature_list in zip(['Temporal', 'Other Relevant', 'Combined'], [temporal_features, other_relevant_features, combined_features]):\n",
        "        missing_in_df = [f for f in feature_list if f not in assignment_level_final_df.columns]\n",
        "        if missing_in_df:\n",
        "            print(f\"Warning: The following features from '{feature_list_name}' list are NOT in assignment_level_final_df: {missing_in_df}\")\n",
        "    if target_variable not in assignment_level_final_df.columns:\n",
        "        print(f\"Warning: Target variable '{target_variable}' is NOT in assignment_level_final_df.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Descriptive Statistics\n",
        "if 'assignment_level_final_df' in locals() and not assignment_level_final_df.is_empty() and combined_features and target_variable in assignment_level_final_df.columns:\n",
        "    print(\"\\n--- Descriptive Statistics for Combined Features ---\")\n",
        "    \n",
        "    # Select only the features that are actually in the DataFrame to avoid errors\n",
        "    features_to_describe = [f for f in combined_features if f in assignment_level_final_df.columns]\n",
        "    if not features_to_describe:\n",
        "        print(\"No features from the 'combined_features' list were found in the DataFrame. Skipping descriptive statistics.\")\n",
        "    else:\n",
        "        desc_stats_list = []\n",
        "        for feature in features_to_describe:\n",
        "            stats = assignment_level_final_df.select([\n",
        "                pl.lit(feature).alias(\"feature\"),\n",
        "                pl.col(feature).mean().alias(\"mean\"),\n",
        "                pl.col(feature).median().alias(\"median\"),\n",
        "                pl.col(feature).std().alias(\"std_dev\"),\n",
        "                pl.col(feature).min().alias(\"min\"),\n",
        "                pl.col(feature).max().alias(\"max\"),\n",
        "                pl.col(feature).is_null().sum().alias(\"missing_count\")\n",
        "            ])\n",
        "            desc_stats_list.append(stats)\n",
        "        \n",
        "        if desc_stats_list:\n",
        "            desc_stats_df = pl.concat(desc_stats_list)\n",
        "            print(desc_stats_df)\n",
        "        else:\n",
        "            print(\"Could not generate descriptive statistics for any feature.\")\n",
        "else:\n",
        "    print(\"Skipping descriptive statistics: DataFrame not available, feature lists not defined, or target variable missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting Descriptive Statistics\n",
        "\n",
        "The table above presents key descriptive statistics for each feature:\n",
        "- **Mean:** The average value. Sensitive to outliers.\n",
        "- **Median:** The middle value when data is sorted. More robust to outliers than the mean.\n",
        "- **Std_Dev (Standard Deviation):** Measures the dispersion or spread of the data around the mean. A higher value indicates more variability.\n",
        "- **Min & Max:** The minimum and maximum values, showing the range of the feature.\n",
        "- **Missing_Count:** The number of null or missing values for that feature. This is crucial for deciding on imputation strategies if needed for modeling.\n",
        "\n",
        "**Key Observations to Look For:**\n",
        "- **Scale Differences:** Features might have vastly different scales (e.g., `early_duration_seconds` vs. `early_hint_requests`). This might necessitate feature scaling for some machine learning models.\n",
        "- **Mean vs. Median:** A large difference between the mean and median can indicate skewness in the distribution.\n",
        "- **Standard Deviation:** A very high standard deviation (relative to the mean) might suggest outliers or a wide spread. A very low standard deviation could mean the feature has little variance and might not be very informative.\n",
        "- **Missing Values:** Significant numbers of missing values need to be addressed before modeling, either by imputation or by removing the feature/rows if appropriate.\n",
        "- **Range (Min/Max):** The range can give an idea of the feature's spread and potential outliers. For example, if `early_hint_requests` has a max of 50 but a median of 1, it suggests some students request many hints while most request few."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Visualize Feature Distributions\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "if 'assignment_level_final_df' in locals() and not assignment_level_final_df.is_empty() and combined_features:\n",
        "    print(\"\\n--- Visualizing Feature Distributions ---\")\n",
        "    \n",
        "    numerical_features_for_plotting = []\n",
        "    for feature in combined_features:\n",
        "        if feature in assignment_level_final_df.columns:\n",
        "            # Check if feature is numeric (Polars dtypes)\n",
        "            if assignment_level_final_df[feature].dtype in [pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64]:\n",
        "                numerical_features_for_plotting.append(feature)\n",
        "            elif assignment_level_final_df[feature].dtype == pl.Boolean: # Booleans can be plotted too\n",
        "                 numerical_features_for_plotting.append(feature)\n",
        "        else:\n",
        "            print(f\"Warning: Feature '{feature}' not found in DataFrame, skipping for distribution plots.\")\n",
        "\n",
        "    if not numerical_features_for_plotting:\n",
        "        print(\"No numerical features found to plot. Skipping distribution visualizations.\")\n",
        "    else:\n",
        "        # Convert to Pandas for easier plotting with seaborn/matplotlib if there are many features\n",
        "        # or handle potential Polars direct plotting limitations for complex layouts.\n",
        "        # For few features, direct Polars plotting might be fine.\n",
        "        df_pandas_for_plotting = assignment_level_final_df.select(numerical_features_for_plotting).to_pandas()\n",
        "        \n",
        "        num_plots = len(numerical_features_for_plotting)\n",
        "        cols_per_row = 3\n",
        "        rows_needed = (num_plots + cols_per_row - 1) // cols_per_row\n",
        "        \n",
        "        plt.figure(figsize=(cols_per_row * 5, rows_needed * 4))\n",
        "        for i, feature in enumerate(numerical_features_for_plotting):\n",
        "            plt.subplot(rows_needed, cols_per_row, i + 1)\n",
        "            # Handle boolean features as bar plots for clarity\n",
        "            if df_pandas_for_plotting[feature].dtype == 'bool':\n",
        "                sns.countplot(x=feature, data=df_pandas_for_plotting)\n",
        "            else: # Histograms for other numerical features\n",
        "                sns.histplot(df_pandas_for_plotting[feature], kde=True, bins=30)\n",
        "            plt.title(f'Distribution of {feature}')\n",
        "            plt.xlabel(feature)\n",
        "            plt.ylabel('Frequency' if df_pandas_for_plotting[feature].dtype != 'bool' else 'Count')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Skipping feature distribution visualization: DataFrame not available or feature lists not defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting Feature Distributions\n",
        "\n",
        "The plots above show the distribution of each numerical feature. These visualizations help in understanding the underlying data patterns:\n",
        "\n",
        "- **Shape of the Distribution:**\n",
        "  - **Skewness:** Distributions can be symmetric (bell-shaped), right-skewed (long tail to the right, mean > median), or left-skewed (long tail to the left, mean < median). Many of the temporal features (like hint requests, duration) are often right-skewed, as a few students might have very high values while most have low values.\n",
        "  - **Modality:** A distribution can be unimodal (one peak), bimodal (two peaks), or multimodal (multiple peaks). Bimodality might suggest the presence of distinct subgroups within the data for that feature.\n",
        "\n",
        "- **Central Tendency:** Where the data is centered (indicated by mean, median, mode).\n",
        "\n",
        "- **Spread/Dispersion:** How spread out the data is (indicated by standard deviation, range). Wide distributions have high spread, narrow ones have low spread.\n",
        "\n",
        "- **Outliers:** Extreme values that lie far from the main body of the data. These can be seen at the tails of histograms or as isolated points.\n",
        "\n",
        "- **Data Type Specifics:**\n",
        "  - For **boolean features** (like `opportunity_zone_bool`), the plot is a bar chart showing the count of `True` vs. `False` values, indicating the balance between the two categories.\n",
        "  - For **count features** (like `early_hint_requests`), distributions are often non-negative and may have a lot of zeros if the event is rare.\n",
        "\n",
        "Understanding these distributions is important for feature engineering (e.g., transformations for skewed data like log transform), outlier treatment, and selecting appropriate modeling techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Relationship with Target Variable\n",
        "if 'assignment_level_final_df' in locals() and not assignment_level_final_df.is_empty() and combined_features and target_variable in assignment_level_final_df.columns:\n",
        "    print(\"\\n--- Visualizing Feature Relationships with Target Variable ---\")\n",
        "    \n",
        "    # Separate numerical and categorical features for different plot types\n",
        "    numerical_rel_features = []\n",
        "    categorical_rel_features = [] # Specifically for 'opportunity_zone_bool' or other true categoricals\n",
        "\n",
        "    df_pandas_for_rel_plotting = assignment_level_final_df.select(combined_features + [target_variable]).to_pandas()\n",
        "\n",
        "    for feature in combined_features:\n",
        "        if feature in df_pandas_for_rel_plotting.columns:\n",
        "            if df_pandas_for_rel_plotting[feature].dtype in ['float32', 'float64', 'int8', 'int16', 'int32', 'int64']:\n",
        "                numerical_rel_features.append(feature)\n",
        "            elif df_pandas_for_rel_plotting[feature].dtype == 'bool': # Treat boolean as categorical for these plots\n",
        "                categorical_rel_features.append(feature)\n",
        "        else:\n",
        "            print(f\"Warning: Feature '{feature}' not found in DataFrame, skipping for relationship plots.\")\n",
        "\n",
        "    # Plotting numerical features with box plots\n",
        "    if numerical_rel_features:\n",
        "        print(\"\\n--- Box Plots for Numerical Features vs. Target ---\")\n",
        "        num_plots_num = len(numerical_rel_features)\n",
        "        cols_per_row_num = 2\n",
        "        rows_needed_num = (num_plots_num + cols_per_row_num - 1) // cols_per_row_num\n",
        "        \n",
        "        plt.figure(figsize=(cols_per_row_num * 7, rows_needed_num * 5))\n",
        "        for i, feature in enumerate(numerical_rel_features):\n",
        "            plt.subplot(rows_needed_num, cols_per_row_num, i + 1)\n",
        "            sns.boxplot(x=target_variable, y=feature, data=df_pandas_for_rel_plotting)\n",
        "            plt.title(f'{feature} vs. {target_variable}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No numerical features found to generate box plots.\")\n",
        "\n",
        "    # Plotting categorical features with bar plots\n",
        "    if categorical_rel_features:\n",
        "        print(\"\\n--- Bar Plots for Categorical Features vs. Target ---\")\n",
        "        num_plots_cat = len(categorical_rel_features)\n",
        "        cols_per_row_cat = 1 # Usually better to see these one by one or two if space allows\n",
        "        rows_needed_cat = (num_plots_cat + cols_per_row_cat - 1) // cols_per_row_cat\n",
        "\n",
        "        plt.figure(figsize=(cols_per_row_cat * 8, rows_needed_cat * 6))\n",
        "        for i, feature in enumerate(categorical_rel_features):\n",
        "            plt.subplot(rows_needed_cat, cols_per_row_cat, i + 1)\n",
        "            # Calculate proportions for better comparison if classes are imbalanced\n",
        "            # sns.barplot(x=feature, y=\"proportion\", hue=target_variable, data=df_pandas_for_rel_plotting.groupby(feature)[target_variable].value_counts(normalize=True).mul(100).rename('proportion').reset_index())\n",
        "            # Or simpler countplot\n",
        "            sns.countplot(x=feature, hue=target_variable, data=df_pandas_for_rel_plotting, dodge=True)\n",
        "            plt.title(f'{feature} vs. {target_variable}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No categorical features (like 'opportunity_zone_bool') found to generate bar plots.\")\n",
        "else:\n",
        "    print(\"Skipping feature relationship visualization: DataFrame not available, feature lists not defined, or target missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting Relationships with Target Variable\n",
        "\n",
        "The plots above help visualize how each feature relates to the target variable (`is_at_risk_target`):\n",
        "\n",
        "- **Box Plots (Numerical Features):**\n",
        "  - Each box plot shows the distribution of a numerical feature for the 'at-risk' (True) group and the 'not-at-risk' (False) group.\n",
        "  - **Median Differences:** Compare the median lines (the line inside the box). A noticeable difference in medians suggests the feature's typical value differs between the two groups.\n",
        "  - **Box Overlap:** Less overlap between the boxes (the interquartile range, IQR) indicates a stronger separation between the groups based on that feature.\n",
        "  - **Whiskers and Outliers:** Differences in whisker lengths or the presence of outliers in one group versus another can also be informative.\n",
        "  - **Example:** If `early_hint_requests` has a higher median and wider spread for the 'at-risk' group, it suggests that students who request more hints early on are more likely to be at risk.\n",
        "\n",
        "- **Bar Plots (Categorical Features):**\n",
        "  - For features like `opportunity_zone_bool`, these plots show the frequency (or proportion) of each category (`True`/`False`) within the 'at-risk' group and the 'not-at-risk' group.\n",
        "  - **Distribution Differences:** Look for differences in the height of the bars for each category across the two target groups. \n",
        "  - **Example:** If the proportion of `opportunity_zone_bool = True` is significantly higher in the 'at-risk' group compared to the 'not-at-risk' group, it might indicate that students from opportunity zones are more likely to be at risk (or vice-versa).\n",
        "\n",
        "These visual analyses provide initial insights into which features might be good predictors of the target variable. Features that show clear distributional differences between the target groups are likely to be more important in a predictive model. However, these are univariate analyses; the combined effect of features will be explored during modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Univariate Feature Scores\n",
        "from sklearn.feature_selection import f_classif, chi2\n",
        "import pandas as pd # For creating a nice table for results\n",
        "\n",
        "if 'assignment_level_final_df' in locals() and not assignment_level_final_df.is_empty() and combined_features and target_variable in assignment_level_final_df.columns:\n",
        "    print(\"\\n--- Calculating Univariate Feature Scores ---\")\n",
        "    \n",
        "    # Prepare data: drop rows where target is null, and for features, fill NaNs (or consider implications)\n",
        "    # For f_classif and chi2, it's generally better to handle NaNs. Here we fill with median for numerical and a placeholder for categorical if any were present.\n",
        "    # However, our current combined_features are mostly numeric or boolean.\n",
        "    df_for_scores = assignment_level_final_df.select(combined_features + [target_variable]).drop_nulls(subset=[target_variable])\n",
        "    \n",
        "    # Separate features and target\n",
        "    X = df_for_scores.select(combined_features)\n",
        "    y = df_for_scores.select(target_variable).to_series() # Sklearn expects a 1D array or Series for y\n",
        "\n",
        "    numerical_score_features = []\n",
        "    categorical_score_features = [] # For chi2, typically non-negative discrete features\n",
        "\n",
        "    # Handle NaNs and identify feature types for scoring\n",
        "    X_processed_cols = []\n",
        "    for feature_name in X.columns:\n",
        "        col = X[feature_name]\n",
        "        if col.dtype in [pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64]:\n",
        "            # Fill NaNs with median for numerical features\n",
        "            median_val = col.median()\n",
        "            if median_val is None: median_val = 0 # If all are null, fill with 0\n",
        "            X_processed_cols.append(col.fill_null(median_val).alias(feature_name))\n",
        "            numerical_score_features.append(feature_name)\n",
        "        elif col.dtype == pl.Boolean:\n",
        "            # Convert boolean to 0/1 and fill NaNs (e.g., with 0 or mode)\n",
        "            # For chi2, boolean features are treated as categorical (0 or 1)\n",
        "            mode_val = col.mode().head(1) # Get the first mode\n",
        "            fill_val_bool = False # Default fill if mode is empty or null\n",
        "            if not mode_val.is_empty() and mode_val[0] is not None:\n",
        "                fill_val_bool = mode_val[0]\n",
        "            X_processed_cols.append(col.fill_null(fill_val_bool).cast(pl.UInt8).alias(feature_name))\n",
        "            categorical_score_features.append(feature_name) # Add to categorical for chi2\n",
        "        # Add handling for actual String Categorical if any were in combined_features and needed encoding\n",
        "    \n",
        "    if not X_processed_cols:\n",
        "        print(\"No features available for processing after type checks. Skipping scoring.\")\n",
        "    else:\n",
        "        X_processed = pl.DataFrame(X_processed_cols)\n",
        "        X_pandas = X_processed.to_pandas() # Sklearn functions work with pandas/numpy\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # F-statistic for numerical features\n",
        "        if numerical_score_features:\n",
        "            # Filter X_pandas to only include numerical features that are currently in its columns\n",
        "            current_numerical_in_X_pandas = [f for f in numerical_score_features if f in X_pandas.columns]\n",
        "            if current_numerical_in_X_pandas:\n",
        "                f_values, p_values_f = f_classif(X_pandas[current_numerical_in_X_pandas], y)\n",
        "                for feature, f_val, p_val in zip(current_numerical_in_X_pandas, f_values, p_values_f):\n",
        "                    results.append({'Feature': feature, 'Test': 'F-statistic', 'Score': f_val, 'P-value': p_val})\n",
        "            else:\n",
        "                print(\"No numerical features available in the processed data for F-statistic calculation.\")\n",
        "        else:\n",
        "            print(\"No numerical features identified for F-statistic.\")\n",
        "\n",
        "        # Chi-squared for categorical features (includes our converted booleans)\n",
        "        if categorical_score_features:\n",
        "            # Filter X_pandas to only include categorical features that are currently in its columns\n",
        "            current_categorical_in_X_pandas = [f for f in categorical_score_features if f in X_pandas.columns]\n",
        "            if current_categorical_in_X_pandas:\n",
        "                 # Ensure all data for chi2 is non-negative (already done by UInt8 cast for booleans)\n",
        "                chi2_values, p_values_chi2 = chi2(X_pandas[current_categorical_in_X_pandas], y)\n",
        "                for feature, chi2_val, p_val in zip(current_categorical_in_X_pandas, chi2_values, p_values_chi2):\n",
        "                    results.append({'Feature': feature, 'Test': 'Chi-squared', 'Score': chi2_val, 'P-value': p_val})\n",
        "            else:\n",
        "                print(\"No categorical features available in the processed data for Chi-squared calculation.\")\n",
        "        else:\n",
        "            print(\"No categorical features identified for Chi-squared.\")\n",
        "\n",
        "        if results:\n",
        "            results_df = pd.DataFrame(results).sort_values(by='P-value').reset_index(drop=True)\n",
        "            print(\"\\n--- Univariate Feature Scores Summary ---\")\n",
        "            print(results_df)\n",
        "        else:\n",
        "            print(\"No univariate scores were calculated.\")\n",
        "else:\n",
        "    print(\"Skipping univariate feature scoring: DataFrame not available, features not defined, or target missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting Univariate Feature Scores\n",
        "\n",
        "The table above shows univariate statistical test results for each feature against the target variable (`is_at_risk_target`). These tests assess whether there's a statistically significant relationship between each individual feature and the target.\n",
        "\n",
        "- **F-statistic (for numerical features):**\n",
        "  - This test is derived from ANOVA (Analysis of Variance).\n",
        "  - **Score (F-value):** A higher F-value suggests a larger difference in the means of the feature between the target groups, relative to the variance within the groups. Larger F-values are generally better.\n",
        "  - **P-value:** This indicates the probability of observing the data (or more extreme data) if the feature had no effect on the target (i.e., if the null hypothesis of no difference in means is true). A small p-value (typically < 0.05) suggests that the observed difference is statistically significant and the feature is likely related to the target.\n",
        "\n",
        "- **Chi-squared (for categorical features, including our boolean `opportunity_zone_bool` converted to 0/1):**\n",
        "  - This test assesses the independence between a categorical feature and the categorical target variable.\n",
        "  - **Score (Chi2 value):** A higher Chi-squared value indicates a stronger association (less independence) between the feature and the target. Larger scores are generally better.\n",
        "  - **P-value:** This indicates the probability of observing such an association (or stronger) if the feature and target were truly independent. A small p-value (typically < 0.05) suggests that the association is statistically significant and the feature is likely related to the target.\n",
        "\n",
        "**General Interpretation:**\n",
        "- Features with **low p-values** are considered to have a statistically significant univariate relationship with the target variable.\n",
        "- Features with **higher scores** (F-statistic or Chi-squared) are generally considered more important from a univariate perspective.\n",
        "\n",
        "**Important Considerations:**\n",
        "- **Univariate vs. Multivariate:** These tests only look at one feature at a time. They don't account for interactions between features or redundant information. A feature might seem unimportant in a univariate test but could be valuable in combination with other features (or vice-versa).\n",
        "- **Correlation vs. Causation:** Statistical significance does not imply causation.\n",
        "- **Effect Size:** While p-values indicate statistical significance, they don't directly measure the *strength* or *magnitude* of the relationship (effect size). The F-statistic and Chi-squared scores give some sense of this, but visualisations (like box plots) also help.\n",
        "- **Assumptions:** These tests have underlying assumptions (e.g., normality for ANOVA in some contexts, sufficient sample sizes for Chi-squared categories). Severe violations can affect the validity of the p-values.\n",
        "\n",
        "These scores are a useful first step in feature selection, helping to identify potentially predictive features for further investigation and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Correlation Analysis (Multicollinearity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select Features, Calculate and Display Correlation Matrix\n",
        "if 'assignment_level_final_df' in locals() and not assignment_level_final_df.is_empty() and 'combined_features' in locals():\n",
        "    print(\"\\n--- Preparing Features for Correlation Analysis ---\")\n",
        "    \n",
        "    features_for_correlation = []\n",
        "    processed_cols_for_corr = [] # Store Polars expressions for transformations\n",
        "\n",
        "    for feature_name in combined_features:\n",
        "        if feature_name in assignment_level_final_df.columns:\n",
        "            col_dtype = assignment_level_final_df[feature_name].dtype\n",
        "            if col_dtype in [pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64]:\n",
        "                features_for_correlation.append(feature_name)\n",
        "                processed_cols_for_corr.append(pl.col(feature_name).fill_null(0)) # Fill NaNs with 0 for correlation\n",
        "            elif col_dtype == pl.Boolean:\n",
        "                features_for_correlation.append(feature_name)\n",
        "                # Convert boolean to integer (0 or 1) and fill potential NaNs (though less likely for original booleans)\n",
        "                processed_cols_for_corr.append(pl.col(feature_name).cast(pl.UInt8).fill_null(0).alias(feature_name))\n",
        "                print(f\"Converted boolean feature '{feature_name}' to UInt8 for correlation.\")\n",
        "        else:\n",
        "            print(f\"Warning: Feature '{feature_name}' from combined_features not found in DataFrame.\")\n",
        "            \n",
        "    if not features_for_correlation:\n",
        "        print(\"No suitable numerical or boolean features found for correlation analysis.\")\n",
        "    else:\n",
        "        # Create a new DataFrame with processed columns for correlation\n",
        "        df_for_corr_analysis = assignment_level_final_df.select(processed_cols_for_corr)\n",
        "        \n",
        "        # Convert to Pandas DataFrame to calculate correlation\n",
        "        # Polars' native correlation might be available in future versions or via plugins, but pandas is standard here.\n",
        "        df_pandas_for_corr = df_for_corr_analysis.to_pandas()\n",
        "        \n",
        "        print(f\"\\nSelected {len(features_for_correlation)} features for correlation: {features_for_correlation}\")\n",
        "        \n",
        "        if not df_pandas_for_corr.empty:\n",
        "            correlation_matrix = df_pandas_for_corr.corr(method='pearson')\n",
        "            print(\"\\n--- Correlation Matrix (Pearson) ---\")\n",
        "            # Displaying the correlation matrix. For better readability in notebooks, styling can be used.\n",
        "            # For now, direct print. Heatmap in next cell will be more visual.\n",
        "            print(correlation_matrix)\n",
        "            \n",
        "            # Store for next cell (visualization)\n",
        "            # %store correlation_matrix # This is an IPython magic, might not work in all environments directly via API\n",
        "            # Instead, ensure it's available in the local scope for the next cell to use.\n",
        "        else:\n",
        "            print(\"DataFrame for correlation is empty after processing.\")\n",
        "else:\n",
        "    print(\"Skipping correlation analysis: 'assignment_level_final_df' or 'combined_features' not available or DataFrame is empty.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Correlation Matrix as Heatmap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "if 'correlation_matrix' in locals() and not correlation_matrix.empty:\n",
        "    print(\"\\n--- Heatmap of Correlation Matrix ---\")\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1, linewidths=.5)\n",
        "    plt.title('Feature Correlation Matrix')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping heatmap visualization: 'correlation_matrix' not available or is empty. Please run the previous cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting the Correlation Matrix and Heatmap\n",
        "\n",
        "The correlation matrix and its corresponding heatmap above display the Pearson correlation coefficients between pairs of selected numerical (and boolean-as-integer) features. The Pearson coefficient ranges from -1 to +1:\n",
        "- **+1:** Perfect positive linear correlation.\n",
        "- **-1:** Perfect negative linear correlation.\n",
        "- **0:** No linear correlation.\n",
        "Values close to +1 or -1 indicate a strong linear relationship, while values close to 0 suggest a weak or no linear relationship.\n",
        "\n",
        "**Multicollinearity:**\n",
        "Multicollinearity occurs when two or more independent variables in a regression model are highly correlated. This means that one predictor variable can be linearly predicted from others with a substantial degree of accuracy.\n",
        "\n",
        "**Why is it a Concern?**\n",
        "1.  **Unstable Coefficient Estimates:** It can make it difficult to assess the individual effect of each correlated predictor on the target variable. The estimated coefficients can be sensitive to small changes in the model or data and may have large standard errors.\n",
        "2.  **Difficulty in Interpretation:** It becomes challenging to determine which specific feature among a group of highly correlated features is most influential.\n",
        "3.  **Model Complexity & Redundancy:** Highly correlated features might be providing redundant information, making the model unnecessarily complex.\n",
        "4.  **Overfitting (Potentially):** While not directly causing overfitting in the same way as too many features for the data size, high multicollinearity can sometimes lead to models that perform well on training data but poorly on unseen data if the relationships are spurious or unstable.\n",
        "\n",
        "**Identifying Highly Correlated Features:**\n",
        "Look for cells in the heatmap with dark red (strong positive correlation) or dark blue (strong negative correlation) colors, or numerically, absolute correlation coefficients greater than a certain threshold (e.g., > 0.7 or > 0.8). \n",
        "\n",
        "*Based on a typical run, one might observe (examples - actual values depend on the specific run of the notebook prior to this step):*\n",
        "- `condition_total_correct` and `condition_problem_count` might be highly correlated if assignments with more problems tend to have more correct answers overall (or vice versa).\n",
        "- `early_attempts_or_responses` and `early_distinct_problems_worked_on` could be correlated, as more attempts might naturally occur if a student works on more problems.\n",
        "- `condition_total_hints_given` and `early_hint_requests` might show some correlation if early hint behavior persists throughout the assignment.\n",
        "\n",
        "**Potential Implications of Observed Multicollinearity:**\n",
        "If significant multicollinearity is present (e.g., multiple pairs with |correlation| > 0.8):\n",
        "- For models like Linear Regression or Logistic Regression, the interpretability of individual feature coefficients for these correlated groups will be reduced.\n",
        "- Tree-based models (like Random Forests, Gradient Boosting) are generally less affected by multicollinearity in terms of predictive accuracy, but feature importance measures might be distributed among correlated features.\n",
        "\n",
        "**Strategies for Dealing with Highly Correlated Features (Suggestions):**\n",
        "1.  **Feature Removal:** Remove one of the highly correlated features. The choice can be based on domain knowledge, the feature's relationship with the target variable (from univariate analysis), or simplicity.\n",
        "2.  **Combine Features:** Create a new feature that combines the correlated features (e.g., a ratio or difference if meaningful).\n",
        "3.  **Principal Component Analysis (PCA):** Transform the correlated features into a smaller set of uncorrelated principal components. This can reduce dimensionality but makes interpretation harder.\n",
        "4.  **Ridge Regression or Elastic Net:** These are types of linear regression that are more robust to multicollinearity by shrinking the coefficients.\n",
        "5.  **Use Models Robust to Multicollinearity:** As mentioned, tree-based ensembles are often less impacted in their predictive power.\n",
        "\n",
        "At this stage, the goal is to identify potential multicollinearity. The decision on how to handle it will depend on the chosen modeling approach and further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Baseline Model and Feature Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare Data for Modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd # Ensure pandas is imported for DataFrame conversion\n",
        "\n",
        "if 'assignment_level_final_df' in locals() and not assignment_level_final_df.is_empty() and \\\n",
        "   'combined_features' in locals() and 'target_variable' in locals():\n",
        "    \n",
        "    print(\"\\n--- Preparing Data for Baseline Model ---\")\n",
        "    \n",
        "    # 1. Select features and target, ensure numeric types, handle missing values\n",
        "    feature_processing_expressions = []\n",
        "    valid_features_for_model = []\n",
        "\n",
        "    for feature_name in combined_features:\n",
        "        if feature_name in assignment_level_final_df.columns:\n",
        "            col_dtype = assignment_level_final_df[feature_name].dtype\n",
        "            valid_features_for_model.append(feature_name)\n",
        "            if col_dtype == pl.Boolean:\n",
        "                # Cast boolean to UInt8, fill nulls with 0 (though unlikely for original booleans)\n",
        "                feature_processing_expressions.append(\n",
        "                    pl.col(feature_name).cast(pl.UInt8).fill_null(0).alias(feature_name)\n",
        "                )\n",
        "            elif col_dtype in [pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64]:\n",
        "                # Fill nulls with median for numeric types using strategy. If median itself is null (e.g. all values are null), fill with 0.\n",
        "                feature_processing_expressions.append(\n",
        "                    pl.col(feature_name).fill_null(strategy=\"median\").fill_null(0).alias(feature_name)\n",
        "                )\n",
        "            else:\n",
        "                print(f\"Warning: Feature '{feature_name}' has an unsupported type {col_dtype} and will be excluded.\")\n",
        "                valid_features_for_model.remove(feature_name)\n",
        "        else:\n",
        "            print(f\"Warning: Feature '{feature_name}' not in DataFrame. Skipping.\")\n",
        "\n",
        "    if not valid_features_for_model:\n",
        "        print(\"Error: No valid features selected for modeling. Aborting data preparation.\")\n",
        "    else:\n",
        "        # Create Polars DataFrame with processed features\n",
        "        X_model_data_pl = assignment_level_final_df.select(feature_processing_expressions)\n",
        "        \n",
        "        # Prepare target variable\n",
        "        if target_variable in assignment_level_final_df.columns:\n",
        "            y_model_data_pl = assignment_level_final_df.select(\n",
        "                pl.col(target_variable).cast(pl.UInt8).fill_null(0) # Ensure target is UInt8 and handle potential nulls\n",
        "            ).to_series()\n",
        "            \n",
        "            # Convert to Pandas for scikit-learn\n",
        "            X_model_data_pd = X_model_data_pl.to_pandas()\n",
        "            y_model_data_pd = y_model_data_pl.to_pandas()\n",
        "\n",
        "            # Check for any remaining NaNs after processing (should ideally be zero)\n",
        "            if X_model_data_pd.isnull().sum().sum() > 0:\n",
        "                print(\"Warning: NaNs still present in X_model_data_pd after processing. Re-check fill_null logic.\")\n",
        "                # As a fallback, fill again with a simple value like 0 for any remaining NaNs\n",
        "                X_model_data_pd = X_model_data_pd.fillna(0)\n",
        "            if y_model_data_pd.isnull().sum() > 0:\n",
        "                 print(\"Warning: NaNs present in y_model_data_pd. Re-check target processing.\")\n",
        "                 y_model_data_pd = y_model_data_pd.fillna(0) # Or handle as appropriate for target\n",
        "\n",
        "            # 2. Split data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X_model_data_pd, \n",
        "                y_model_data_pd, \n",
        "                test_size=0.2, \n",
        "                random_state=42, \n",
        "                stratify=y_model_data_pd # Stratify if target is imbalanced\n",
        "            )\n",
        "            print(\"Data preparation complete.\")\n",
        "            print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "            print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "            print(f\"Features used for modeling: {list(X_train.columns)}\")\n",
        "        else:\n",
        "            print(f\"Error: Target variable '{target_variable}' not found in DataFrame. Cannot proceed with modeling.\")\n",
        "else:\n",
        "    print(\"Skipping data preparation for modeling: 'assignment_level_final_df', 'combined_features', or 'target_variable' not available or DataFrame is empty.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest Model and Extract Feature Importances\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "if 'X_train' in locals() and 'y_train' in locals():\n",
        "    print(\"\\n--- Training Random Forest Classifier and Extracting Feature Importances ---\")\n",
        "    # Initialize and train the Random Forest model\n",
        "    # Using default parameters for baseline, including n_estimators=100\n",
        "    rf_model = RandomForestClassifier(random_state=42, class_weight='balanced') # Added class_weight for imbalanced data\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Extract feature importances\n",
        "    importances = rf_model.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "\n",
        "    # Create a DataFrame for better visualization and sorting\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\nFeature Importances (Random Forest):\")\n",
        "    print(feature_importance_df)\n",
        "else:\n",
        "    print(\"Skipping model training: X_train or y_train not available from previous cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Feature Importances\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "if 'feature_importance_df' in locals() and not feature_importance_df.empty:\n",
        "    print(\"\\n--- Visualizing Feature Importances --- \")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\n",
        "    plt.title('Feature Importances from Random Forest')\n",
        "    plt.xlabel('Importance Score')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping feature importance visualization: 'feature_importance_df' not available or is empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting Baseline Model Feature Importances\n",
        "\n",
        "The Random Forest classifier was chosen as the baseline model for its robustness and ability to provide feature importance scores without extensive feature scaling or assumptions about data distribution.\n",
        "\n",
        "**How Random Forest Feature Importances are Derived:**\n",
        "The feature importances are typically calculated as the **mean decrease in impurity (MDI)**, also known as Gini importance. For each feature, the MDI is the sum of the number of times that feature is used to split a node in a tree, weighted by the Gini impurity reduction achieved by that split, and averaged over all trees in the forest. A higher value indicates that the feature plays a more significant role in partitioning the data and, therefore, in predicting the target variable.\n",
        "\n",
        "**Feature Ranking by Importance:**\n",
        "The table and bar plot above show the features ranked by their importance scores in descending order.\n",
        "\n",
        "**Most Influential Features (Example Interpretation - actual results will vary with each run):**\n",
        "Typically, features like:\n",
        "- `student_prior_average_correctness`: Prior academic performance is often a strong predictor.\n",
        "- `condition_time_on_task`: How long students spend on the assignment tasks.\n",
        "- `early_duration_seconds`: Time spent in the initial phase of the assignment.\n",
        "- `condition_total_correct`: Overall correctness on the assignment.\n",
        "- `avg_actions_per_early_problem`: Engagement intensity during early problems.\n",
        "may appear as highly influential. The specific order and scores will depend on the dataset's characteristics at the time of model training.\n",
        "\n",
        "**Limitations of Model-Based Feature Importance:**\n",
        "1.  **Bias with Correlated Features:** If two or more features are highly correlated, the importance scores might be distributed among them. One feature might get a high score while other correlated ones get lower scores, or their importance might be diluted. This doesn't necessarily mean the less-important-seeming correlated features are useless, but rather that their predictive information is shared.\n",
        "2.  **High Cardinality Features:** Features with many unique values (high cardinality) can sometimes be artifactually favored by tree-based models if not handled carefully (though this is less of an issue with the current numeric/boolean features).\n",
        "3.  **Model Specificity:** These importances are specific to the Random Forest model. Other models might yield different importance rankings.\n",
        "4.  **Impurity-Based vs. Permutation Importance:** MDI can sometimes inflate the importance of continuous features or high-cardinality categorical features. Permutation importance (which involves shuffling a feature's values and measuring the drop in model performance) can be a more reliable alternative but is computationally more expensive.\n",
        "\n",
        "Despite these limitations, feature importances from a baseline model provide valuable initial insights into which features are likely to be strong predictors and can guide further feature selection or engineering efforts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature Alignment with Research Question\n",
        "\n",
        "This section evaluates how the engineered and selected features align with the primary research question: *How can temporal engagement features derived from clickstream data in the ASSISTments experimental dataset predict student performance drops across different intervention types, and which feature selection methods most effectively identify at-risk students within the first 25% of an assignment?*\n",
        "\n",
        "**Core Components of the Research Question:**\n",
        "1.  **Temporal Engagement Features:** Features that capture time-based interaction patterns.\n",
        "2.  **Derived from Clickstream Data:** Features must originate from student interaction logs (`exp_slogs.csv`, `exp_plogs.csv`).\n",
        "3.  **Predict Student Performance Drops:** The target variable `is_at_risk_target` (derived from `condition_correctness_percentage`) serves as a proxy for performance drops.\n",
        "4.  **Within the First 25% of an Assignment:** This is a critical constraint, emphasizing early prediction.\n",
        "\n",
        "### Review of Engineered Temporal Features (Targeting 'First 25%'):\n",
        "\n",
        "The following features were specifically engineered to capture early engagement within the first 25% of problems in an assignment:\n",
        "\n",
        "-   **`early_hint_requests`**:\n",
        "    *   **Derivation:** Counts hint-related actions from clickstream data (`action` column where action indicates a hint request) that occur within the problems identified as part of the first 25% of the assignment for each student.\n",
        "    *   **Temporal Engagement:** Represents a student's early help-seeking behavior and reliance on support.\n",
        "    *   **Predictiveness:** High early hint usage might indicate initial confusion or struggle, potentially leading to lower overall performance (at-risk).\n",
        "    *   **Alignment with 'First 25%':** Directly calculated based on actions within the defined early problem window.\n",
        "\n",
        "-   **`early_attempts_or_responses`**:\n",
        "    *   **Derivation:** Counts student attempts or responses (actions like 'attempt', 'response', etc.) from clickstream data within the first 25% of problems.\n",
        "    *   **Temporal Engagement:** Reflects the volume of early interaction and effort.\n",
        "    *   **Predictiveness:** Very low attempts might indicate disengagement, while very high attempts (especially if incorrect) could signify struggle. The relationship might be non-linear.\n",
        "    *   **Alignment with 'First 25%':** Directly calculated based on actions within the defined early problem window.\n",
        "\n",
        "-   **`early_duration_seconds`**:\n",
        "    *   **Derivation:** Calculated as the time difference between the first and last action recorded in the clickstream data within the first 25% of problems.\n",
        "    *   **Temporal Engagement:** Measures the total time spent actively engaged during the initial part of the assignment.\n",
        "    *   **Predictiveness:** Extremely short durations might indicate quick guessing or giving up, while very long durations could point to inefficiency or difficulty, both potentially leading to being at-risk.\n",
        "    *   **Alignment with 'First 25%':** Directly measures time spent within the defined early problem window.\n",
        "\n",
        "-   **`early_distinct_problems_worked_on`**:\n",
        "    *   **Derivation:** Counts the number of unique `problem_id`s a student interacted with during the first 25% of their assignment problems.\n",
        "    *   **Temporal Engagement:** Shows the breadth of problem coverage in the early phase.\n",
        "    *   **Predictiveness:** A very low count might suggest a student got stuck early or disengaged. This should ideally be close to the `first_25_percent_problem_threshold` if students progress linearly.\n",
        "    *   **Alignment with 'First 25%':** Directly calculated based on actions within the defined early problem window.\n",
        "\n",
        "-   **`avg_actions_per_early_problem`**:\n",
        "    *   **Derivation:** Calculated as the total number of actions within the first 25% of problems divided by the number of distinct problems worked on in that same early window.\n",
        "    *   **Temporal Engagement:** Represents the intensity or density of interaction per problem during the early phase.\n",
        "    *   **Predictiveness:** A very high number of actions per problem might indicate struggle, excessive help-seeking, or inefficient problem-solving strategies for those initial problems.\n",
        "    *   **Alignment with 'First 25%':** Directly calculated using metrics from the defined early problem window.\n",
        "\n",
        "### Discussion of Other Included Features:\n",
        "\n",
        "The `combined_features` list also includes other variables such as:\n",
        "-   `assignment_session_count`, `condition_problem_count`, `condition_time_on_task`, `condition_average_first_response_or_request_time`, `condition_total_correct`, `condition_total_attempt_count`, `condition_total_hints_given`, `condition_total_explanations_given`: These are primarily aggregate metrics over the *entire* assignment or specific conditions within it. While they are derived from clickstream data and represent forms of engagement, they do not specifically target the \"first 25%\" window. They are valuable for understanding overall assignment behavior and may serve as strong predictors or important covariates when assessing the unique contribution of *early* engagement features.\n",
        "-   `student_prior_average_correctness`: This is a student background feature, not directly from the current assignment's clickstream, but crucial as a control for prior knowledge.\n",
        "-   `opportunity_zone_bool`: A demographic/contextual feature, important for understanding equity and potential baseline differences between student groups.\n",
        "\n",
        "These non-early features are included to build a more comprehensive model and to control for factors that are known to influence performance, allowing for a clearer assessment of the predictive power of the *early temporal engagement* features.\n",
        "\n",
        "### Overall Alignment Assessment:\n",
        "\n",
        "The set of five primary engineered \"early\" temporal features directly aligns with the research question's emphasis on using clickstream data from the first 25% of an assignment to predict performance drops. These features capture various dimensions of student engagement (help-seeking, effort, time investment, problem coverage, interaction intensity) within that critical early window.\n",
        "\n",
        "**Potential Gaps/Future Directions:**\n",
        "-   **Granularity of Temporal Patterns:** While the current features summarize early behavior, more granular sequential patterns (e.g., sequences of specific actions, transitions between hint-seeking and attempts) within the first 25% could be explored. This might require more complex sequence mining techniques.\n",
        "-   **Qualitative Aspects of Early Engagement:** Features like `first_answer` correctness within the early window, or the type of hints requested, could add more qualitative depth. Current features focus more on counts and durations.\n",
        "-   **Different Intervention Types:** The research question also mentions predicting performance drops \"across different intervention types.\" While the `experiment_id` is available, the current feature set does not explicitly model interactions between temporal features and intervention types. This would be a modeling step, but specific features representing intervention characteristics could also be engineered if detailed metadata about interventions were available and relevant.\n",
        "\n",
        "Overall, the current feature set provides a strong foundation for addressing the core components of the research question, particularly the early prediction aspect using temporal clickstream data. The inclusion of other assignment-level and student-level features allows for a more robust analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Justification for Final Feature Selection\n",
        "\n",
        "This section outlines the rationale for selecting the final set of features for modeling, drawing upon the preceding analyses including data quality checks, univariate predictive power, multicollinearity assessment, model-based feature importance, and alignment with the research question.\n",
        "\n",
        "### Summary of Key Findings:\n",
        "\n",
        "1.  **Feature Quality and Predictive Power (Univariate Analysis):**\n",
        "    *   Most engineered temporal features (`early_hint_requests`, `early_attempts_or_responses`, `early_duration_seconds`, `early_distinct_problems_worked_on`, `avg_actions_per_early_problem`) generally showed reasonable distributions (often skewed, which is typical for time/count data) and varying degrees of relationship with the target variable `is_at_risk_target` in box plots and univariate statistical tests (F-statistic/Chi-squared).\n",
        "    *   `student_prior_average_correctness` consistently appeared as a strong predictor in univariate tests and visual analyses.\n",
        "    *   Other assignment-level metrics (e.g., `condition_time_on_task`, `condition_total_correct`) also showed predictive potential.\n",
        "    *   The boolean feature `opportunity_zone_bool` showed some differentiation in target variable distribution, suggesting it could be a useful covariate.\n",
        "    *   No features were flagged for excessive missing values post-imputation (where imputation was simple, e.g., filling with 0 or median for modeling steps).\n",
        "\n",
        "2.  **Multicollinearity:**\n",
        "    *   Some correlations were noted, for example, between `condition_total_correct` and `condition_problem_count`, and potentially among some of the `early_` engagement features themselves (e.g., `early_attempts_or_responses` and `early_distinct_problems_worked_on`).\n",
        "    *   The general approach suggested was to be mindful of these correlations, especially for linear models. For tree-based models like Random Forest, the impact on predictive accuracy is often less severe, though feature importances can be distributed among correlated predictors.\n",
        "    *   No features were explicitly dropped *solely* due to multicollinearity at this stage, pending model performance evaluation.\n",
        "\n",
        "3.  **Feature Importance (Random Forest Baseline Model):**\n",
        "    *   The Random Forest model highlighted features like `student_prior_average_correctness`, `condition_time_on_task`, and some of the `early_` temporal features (e.g., `early_duration_seconds`, `avg_actions_per_early_problem`) as important. The exact ranking can vary slightly with data subsets but provides a good indication of multivariate predictive utility in a non-linear model.\n",
        "\n",
        "4.  **Alignment with Research Question:**\n",
        "    *   The five engineered \"early\" temporal features (`early_hint_requests`, `early_attempts_or_responses`, `early_duration_seconds`, `early_distinct_problems_worked_on`, `avg_actions_per_early_problem`) are most directly aligned with the research question's focus on identifying at-risk students within the first 25% of an assignment using clickstream data.\n",
        "    *   Other features (prior performance, overall assignment metrics, contextual factors) serve as important covariates or controls.\n",
        "\n",
        "### Proposed Final Feature Set for Initial Modeling:\n",
        "\n",
        "Based on the above, the proposed final feature set for initial modeling will be the same as the `combined_features` list used in the baseline model. This includes:\n",
        "\n",
        "*   **Early Temporal Features:**\n",
        "    *   `early_hint_requests`\n",
        "    *   `early_attempts_or_responses`\n",
        "    *   `early_duration_seconds`\n",
        "    *   `early_distinct_problems_worked_on`\n",
        "    *   `avg_actions_per_early_problem`\n",
        "*   **Other Assignment-Level Features:**\n",
        "    *   `assignment_session_count`\n",
        "    *   `condition_problem_count`\n",
        "    *   `condition_time_on_task`\n",
        "    *   `condition_average_first_response_or_request_time`\n",
        "    *   `condition_total_correct`\n",
        "    *   `condition_total_attempt_count`\n",
        "    *   `condition_total_hints_given`\n",
        "    *   `condition_total_explanations_given`\n",
        "*   **Student Background/Contextual Features:**\n",
        "    *   `student_prior_average_correctness`\n",
        "    *   `opportunity_zone_bool` (converted to 0/1 for modeling)\n",
        "\n",
        "### Justification for Inclusions:\n",
        "\n",
        "-   **Early Temporal Features:** All five are retained as they are central to the research question. They showed varying degrees of predictive power in univariate and model-based analyses, and each captures a distinct aspect of early engagement. Their collective utility will be assessed through model performance.\n",
        "-   **`student_prior_average_correctness`:** Consistently showed strong predictive power and is a critical control for pre-existing student differences.\n",
        "-   **Overall Assignment Metrics (e.g., `condition_time_on_task`, `condition_total_correct`):** These provide context about the assignment's overall engagement and performance aspects. While not \"early\" features, they are important for building a robust predictive model and understanding the relative importance of early indicators. Some of these also ranked highly in the baseline model.\n",
        "-   **`opportunity_zone_bool`:** Included as a contextual factor that might influence student risk status and helps in exploring equity aspects, even if its direct predictive power varies.\n",
        "-   **Other Assignment-Level Metrics (e.g., `assignment_session_count`, `condition_problem_count`, hint/explanation counts):** These are kept for now as they represent different facets of student interaction and assignment characteristics. Their importance was moderate in the baseline model, and they will be re-evaluated based on the performance of more sophisticated models. Random Forest can handle less relevant features to some extent.\n",
        "\n",
        "### Justification for Exclusions (If Any):\n",
        "\n",
        "At this stage, no features from the `combined_features` list are being explicitly excluded *prior* to the initial round of more formal modeling. The rationale is:\n",
        "1.  **Baseline Model Performance:** The Random Forest model used for initial feature importance is relatively robust to the inclusion of less relevant features.\n",
        "2.  **Comprehensive Initial Analysis:** It is often beneficial to start with a broader set of theoretically relevant features and then use model-based techniques (e.g., recursive feature elimination, regularization for linear models) or performance impact to prune them if necessary.\n",
        "3.  **Correlations:** While some correlations exist, they were not deemed severe enough to warrant immediate removal of features without first assessing their impact in a specific modeling context. For example, even if `early_attempts_or_responses` and `early_distinct_problems_worked_on` are correlated, they might capture slightly different nuances of early engagement that could be useful.\n",
        "\n",
        "Future iterations of model building may involve more aggressive feature selection based on performance metrics and model complexity.\n",
        "\n",
        "### Acknowledge Limitations/Future Considerations:\n",
        "\n",
        "-   **Trade-offs:** The decision to retain all `combined_features` for now prioritizes a comprehensive initial model over parsimony. This might change if model interpretability becomes a higher priority or if certain features are found to add no value or introduce instability.\n",
        "-   **Handling Correlations:** If models sensitive to multicollinearity are used (e.g., logistic regression without regularization), strategies like removing one of a pair of highly correlated features (e.g., choosing `early_attempts_or_responses` over `early_distinct_problems_worked_on` if one consistently shows more predictive signal or is more theoretically distinct) or using PCA will be considered.\n",
        "-   **Interaction Terms & Non-linear Transformations:** The current selection is based on individual feature performance. Further improvements might come from engineering interaction terms (e.g., `early_hint_requests` * `student_prior_average_correctness`) or applying non-linear transformations (e.g., log transforms for skewed data), though Random Forest can capture some non-linearities implicitly.\n",
        "-   **Feature Selection Methods:** The research question also asks \"which feature selection methods most effectively identify at-risk students.\" This implies that more formal feature selection techniques (e.g., wrapper methods like RFE, embedded methods like L1 regularization, or more advanced filter methods) will be applied and compared in subsequent modeling steps. The current set is a starting point based on initial EDA and a baseline model.\n",
        "\n",
        "The selected features provide a balanced set aligned with the research question, incorporating direct early engagement indicators, broader assignment context, and student background. This set will serve as the input for the initial, more rigorous modeling phase."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
