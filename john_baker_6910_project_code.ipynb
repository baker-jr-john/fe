{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ9f09tc34r0"
      },
      "source": [
        "# Load, Merge, and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polars version: 1.29.0\n",
            "Base data path: /Users/john/Downloads/osfstorage-archive\n",
            "Experiment IDs path: /Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23\n",
            "Global String Cache enabled: True\n"
          ]
        }
      ],
      "source": [
        "# Setup and Configuration\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# --- Enable Global String Cache for Categoricals ---\n",
        "pl.enable_string_cache()\n",
        "# --- ---\\\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_DATA_PATH = Path('/Users/john/Downloads/osfstorage-archive')\n",
        "EXPERIMENT_IDS_PATH = BASE_DATA_PATH / 'experiment_dataset_2021-09-23'\n",
        "\n",
        "# Output path for the cleaned data\n",
        "SAVE_CLEANED_PATH_POLARS_PARQUET = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.parquet'\n",
        "SAVE_CLEANED_PATH_POLARS_CSV = BASE_DATA_PATH / 'merged_experiment_data_cleaned_polars.csv'\n",
        "\n",
        "print(f\"Polars version: {pl.__version__}\")\n",
        "print(f\"Base data path: {BASE_DATA_PATH}\")\n",
        "print(f\"Experiment IDs path: {EXPERIMENT_IDS_PATH}\")\n",
        "print(f\"Global String Cache enabled: {pl.using_string_cache()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 88 experiment ID directories.\n",
            "Sample performance file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_alogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_alogs.csv']\n",
            "Sample problems file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_plogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_plogs.csv']\n",
            "Sample actions file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/exp_slogs.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/exp_slogs.csv']\n",
            "Sample metrics file paths: ['/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAU85Y/priors.csv', '/Users/john/Downloads/osfstorage-archive/experiment_dataset_2021-09-23/PSAXD6K/priors.csv']\n"
          ]
        }
      ],
      "source": [
        "# Generate File Paths\n",
        "try:\n",
        "    if not EXPERIMENT_IDS_PATH.is_dir():\n",
        "        raise FileNotFoundError(f\"Error: Directory not found at {EXPERIMENT_IDS_PATH}\")\n",
        "    experiment_ids = [d.name for d in EXPERIMENT_IDS_PATH.iterdir() if d.is_dir()]\n",
        "    print(f\"Found {len(experiment_ids)} experiment ID directories.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    experiment_ids = []\n",
        "\n",
        "performance_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_alogs.csv') for exp_id in experiment_ids]\n",
        "problems_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_plogs.csv') for exp_id in experiment_ids]\n",
        "actions_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'exp_slogs.csv') for exp_id in experiment_ids]\n",
        "metrics_file_paths = [str(EXPERIMENT_IDS_PATH / exp_id / 'priors.csv') for exp_id in experiment_ids]\n",
        "\n",
        "print(\"Sample performance file paths:\", performance_file_paths[:2])\n",
        "print(\"Sample problems file paths:\", problems_file_paths[:2])\n",
        "print(\"Sample actions file paths:\", actions_file_paths[:2])\n",
        "print(\"Sample metrics file paths:\", metrics_file_paths[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Schemas and Date Parsing Information\n",
        "\n",
        "actions_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'problem_id': pl.Categorical,\n",
        "    'problem_part': pl.Categorical, \n",
        "    'scaffold_id': pl.Categorical,  \n",
        "    'experiment_tag_path': pl.Utf8,\n",
        "    'action': pl.Categorical,\n",
        "    'timestamp': pl.Utf8,\n",
        "    'assistments_reference_action_log_id': pl.UInt64\n",
        "}\n",
        "actions_parse_dates = ['timestamp']\n",
        "\n",
        "problems_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'problem_id': pl.Categorical,\n",
        "    'problem_part': pl.Categorical, \n",
        "    'scaffold_id': pl.Categorical,  \n",
        "    'problem_condition': pl.Categorical,\n",
        "    'start_time': pl.Utf8,\n",
        "    'end_time': pl.Utf8,\n",
        "    'session_count': pl.UInt16,\n",
        "    'time_on_task': pl.Float32,\n",
        "    'first_response_or_request_time': pl.Float32,\n",
        "    'first_answer': pl.Utf8,\n",
        "    'correct': pl.Boolean,\n",
        "    'reported_score': pl.Float32,\n",
        "    'answer_before_tutoring': pl.Boolean,\n",
        "    'attempt_count': pl.UInt16,\n",
        "    'hints_available': pl.UInt16,\n",
        "    'hints_given': pl.UInt16,\n",
        "    'scaffold_problems_available': pl.UInt16,\n",
        "    'scaffold_problems_given': pl.UInt16,\n",
        "    'explanation_available': pl.Boolean,\n",
        "    'explanation_given': pl.Boolean,\n",
        "    'answer_given': pl.Boolean,\n",
        "    'assistments_reference_problem_log_id': pl.UInt64\n",
        "}\n",
        "problems_parse_dates = ['start_time', 'end_time']\n",
        "\n",
        "performance_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'release_date': pl.Utf8,\n",
        "    'due_date': pl.Utf8,\n",
        "    'start_time': pl.Utf8,\n",
        "    'end_time': pl.Utf8,\n",
        "    'assignment_session_count': pl.Float32,\n",
        "    'pretest_problem_count': pl.Float32,\n",
        "    'pretest_correct': pl.Float32,\n",
        "    'pretest_time_on_task': pl.Float32,\n",
        "    'pretest_average_first_response_time': pl.Float32,\n",
        "    'pretest_session_count': pl.Float32,\n",
        "    'assigned_condition': pl.Categorical,\n",
        "    'condition_time_on_task': pl.Float32,\n",
        "    'condition_average_first_response_or_request_time': pl.Float32,\n",
        "    'condition_problem_count': pl.Float32,\n",
        "    'condition_total_correct': pl.Float32,\n",
        "    'condition_total_correct_after_wrong_response': pl.Float32,\n",
        "    'condition_total_correct_after_tutoring': pl.Float32,\n",
        "    'condition_total_answers_before_tutoring': pl.Float32,\n",
        "    'condition_total_attempt_count': pl.Float32,\n",
        "    'condition_total_hints_available': pl.Float32,\n",
        "    'condition_total_hints_given': pl.Float32,\n",
        "    'condition_total_scaffold_problems_available': pl.Float32,\n",
        "    'condition_total_scaffold_problems_given': pl.Float32,\n",
        "    'condition_total_explanations_available': pl.Float32,\n",
        "    'condition_total_explanations_given': pl.Float32,\n",
        "    'condition_total_answers_given': pl.Float32,\n",
        "    'condition_session_count': pl.Float32,\n",
        "    'posttest_problem_count': pl.Float32,\n",
        "    'posttest_correct': pl.Float32,\n",
        "    'posttest_time_on_task': pl.Float32,\n",
        "    'posttest_average_first_response_time': pl.Float32,\n",
        "    'posttest_session_count': pl.Float32,\n",
        "    'assistments_reference_assignment_log_id': pl.UInt64\n",
        "}\n",
        "performance_parse_dates = ['release_date', 'due_date', 'start_time', 'end_time']\n",
        "\n",
        "metrics_schema = {\n",
        "    'experiment_id': pl.Categorical,\n",
        "    'student_id': pl.Categorical,\n",
        "    'student_prior_started_skill_builder_count': pl.UInt32,\n",
        "    'student_prior_completed_skill_builder_count': pl.UInt32,\n",
        "    'student_prior_started_problem_set_count': pl.UInt32,\n",
        "    'student_prior_completed_problem_set_count': pl.UInt32,\n",
        "    'student_prior_completed_problem_count': pl.UInt32,\n",
        "    'student_prior_median_first_response_time': pl.Float32,\n",
        "    'student_prior_median_time_on_task': pl.Float32,\n",
        "    'student_prior_average_correctness': pl.Float32,\n",
        "    'student_prior_average_attempt_count': pl.Float32,\n",
        "    'class_id': pl.Categorical,\n",
        "    'class_creation_date': pl.Utf8,\n",
        "    'class_student_count': pl.UInt16,\n",
        "    'class_prior_skill_builder_count': pl.UInt32,\n",
        "    'class_prior_problem_set_count': pl.UInt32,\n",
        "    'class_prior_skill_builder_percent_started': pl.Float32,\n",
        "    'class_prior_skill_builder_percent_completed': pl.Float32,\n",
        "    'class_prior_problem_set_percent_started': pl.Float32,\n",
        "    'class_prior_problem_set_percent_completed': pl.Float32,\n",
        "    'class_prior_completed_problem_count': pl.UInt32,\n",
        "    'class_prior_median_time_on_task': pl.Float32,\n",
        "    'class_prior_median_first_response_time': pl.Float32,\n",
        "    'class_prior_average_correctness': pl.Float32,\n",
        "    'class_prior_average_attempt_count': pl.Float32,\n",
        "    'teacher id': pl.Categorical, \n",
        "    'teacher_account_creation_date': pl.Utf8,\n",
        "    'district_id': pl.Categorical,\n",
        "    'location': pl.Categorical,\n",
        "    'opportunity_zone': pl.Categorical,\n",
        "    'locale_description': pl.Categorical\n",
        "}\n",
        "metrics_parse_dates = ['class_creation_date', 'teacher_account_creation_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper Function for Memory-Efficient CSV Concatenation\n",
        "\n",
        "def combine_polars_csvs(file_paths, schema=None, parse_dates_list=None,\n",
        "                        known_date_format_str: str = None,\n",
        "                        date_time_unit='us'):\n",
        "    lazy_frames = []\n",
        "    print(f\"\\nScanning {len(file_paths)} files...\")\n",
        "\n",
        "    common_columns_from_first_file = None\n",
        "    if file_paths and schema:\n",
        "        try:\n",
        "            common_columns_from_first_file = pl.scan_csv(\n",
        "                file_paths[0], infer_schema_length=100, n_rows=10\n",
        "            ).collect_schema().names()\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not determine common columns from first file {file_paths[0]}: {e}\")\n",
        "            common_columns_from_first_file = list(schema.keys())\n",
        "\n",
        "    problematic_file_for_date_parse = None\n",
        "    current_col_for_date_parse = \"unknown\"\n",
        "\n",
        "    for i, file_path_str in enumerate(file_paths):\n",
        "        file_path = Path(file_path_str)\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Scanning file {i+1}/{len(file_paths)}: {file_path.parent.name}/{file_path.name}\")\n",
        "\n",
        "        try:\n",
        "            lf = pl.scan_csv(file_path,\n",
        "                             schema=schema,\n",
        "                             infer_schema_length=100,\n",
        "                             null_values=[\"\", \"NA\", \"NaN\", \"null\"])\n",
        "\n",
        "            if parse_dates_list:\n",
        "                date_parsing_expressions = []\n",
        "                columns_to_check_for_dates = common_columns_from_first_file if common_columns_from_first_file else lf.collect_schema().names()\n",
        "\n",
        "                for col_name in parse_dates_list:\n",
        "                    current_col_for_date_parse = col_name\n",
        "                    if col_name in columns_to_check_for_dates:\n",
        "                        if col_name not in lf.collect_schema().names():\n",
        "                            continue\n",
        "\n",
        "                        date_expr = pl.col(col_name).cast(pl.Utf8, strict=False)\n",
        "\n",
        "                        if known_date_format_str:\n",
        "                            date_expr = date_expr.str.to_datetime(\n",
        "                                format=known_date_format_str,\n",
        "                                strict=False,\n",
        "                                time_unit=date_time_unit\n",
        "                            )\n",
        "                        else:\n",
        "                            date_expr = date_expr.str.to_datetime(\n",
        "                                strict=False,\n",
        "                                time_unit=date_time_unit\n",
        "                            )\n",
        "                        date_parsing_expressions.append(\n",
        "                            date_expr.dt.convert_time_zone(\"UTC\").alias(col_name)\n",
        "                        )\n",
        "                if date_parsing_expressions:\n",
        "                    lf = lf.with_columns(date_parsing_expressions)\n",
        "\n",
        "            lazy_frames.append(lf)\n",
        "            problematic_file_for_date_parse = None\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  Warning: File not found, skipping: {file_path}\")\n",
        "        except pl.exceptions.NoDataError:\n",
        "             print(f\"  Warning: File is empty, skipping: {file_path}\")\n",
        "        except Exception as e:\n",
        "            problematic_file_for_date_parse = file_path\n",
        "            if \"strptime\" in str(e).lower() or \"conversion\" in str(e).lower() or \"datetime\" in str(e).lower():\n",
        "                 print(f\"  Potential date parsing error for {problematic_file_for_date_parse} (column likely '{current_col_for_date_parse}'): {e}\")\n",
        "            else:\n",
        "                print(f\"  Error scanning {file_path} or applying initial transforms: {e}\")\n",
        "\n",
        "\n",
        "    if not lazy_frames:\n",
        "        print(\"  No lazy frames were created from scanning files.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Concatenating {len(lazy_frames)} lazy frames...\")\n",
        "    try:\n",
        "        combined_lf = pl.concat(lazy_frames, how=\"vertical_relaxed\")\n",
        "        print(\"Collecting data into DataFrame (streaming enabled)...\")\n",
        "        # Reverted to engine=\"streaming\" as per deprecation warning for Polars 1.29.0\n",
        "        collected_df = combined_lf.collect(engine=\"streaming\")\n",
        "        print(\"Concatenation and collection complete.\")\n",
        "        return collected_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error during lazy concatenation or collection: {e}\")\n",
        "        if problematic_file_for_date_parse:\n",
        "            print(f\"  This might be related to an earlier issue in file: {problematic_file_for_date_parse}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combining Actions Data (exp_slogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_slogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_slogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_slogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_slogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_slogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_slogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_slogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_slogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_slogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Actions DataFrame shape: (3708299, 9)\n",
            "\n",
            "Combining Problems Data (exp_plogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_plogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_plogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_plogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_plogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_plogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_plogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_plogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_plogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_plogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Problems DataFrame shape: (771386, 24)\n",
            "\n",
            "Combining Performance Data (exp_alogs)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/exp_alogs.csv\n",
            "  Scanning file 11/88: PSAYCFH/exp_alogs.csv\n",
            "  Scanning file 21/88: PSAZ2G4/exp_alogs.csv\n",
            "  Scanning file 31/88: PSAQJFP/exp_alogs.csv\n",
            "  Scanning file 41/88: PSAJVP8/exp_alogs.csv\n",
            "  Scanning file 51/88: PSA9XWV/exp_alogs.csv\n",
            "  Scanning file 61/88: PSAM4NK/exp_alogs.csv\n",
            "  Scanning file 71/88: PSATP2Z/exp_alogs.csv\n",
            "  Scanning file 81/88: PSASDZY/exp_alogs.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Performance DataFrame shape: (95990, 35)\n",
            "\n",
            "Combining Metrics Data (priors)...\n",
            "\n",
            "Scanning 88 files...\n",
            "  Scanning file 1/88: PSAU85Y/priors.csv\n",
            "  Scanning file 11/88: PSAYCFH/priors.csv\n",
            "  Scanning file 21/88: PSAZ2G4/priors.csv\n",
            "  Scanning file 31/88: PSAQJFP/priors.csv\n",
            "  Scanning file 41/88: PSAJVP8/priors.csv\n",
            "  Scanning file 51/88: PSA9XWV/priors.csv\n",
            "  Scanning file 61/88: PSAM4NK/priors.csv\n",
            "  Scanning file 71/88: PSATP2Z/priors.csv\n",
            "  Scanning file 81/88: PSASDZY/priors.csv\n",
            "Concatenating 88 lazy frames...\n",
            "Collecting data into DataFrame (streaming enabled)...\n",
            "Concatenation and collection complete.\n",
            "Metrics DataFrame shape: (95979, 31)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load DataFrames\n",
        "\n",
        "COMMON_DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S%.f%z\" \n",
        "\n",
        "print(\"Combining Actions Data (exp_slogs)...\")\n",
        "actions_df = combine_polars_csvs(\n",
        "    actions_file_paths, \n",
        "    schema=actions_schema, \n",
        "    parse_dates_list=actions_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT \n",
        ")\n",
        "if actions_df is not None:\n",
        "    print(f\"Actions DataFrame shape: {actions_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Problems Data (exp_plogs)...\")\n",
        "problems_df = combine_polars_csvs(\n",
        "    problems_file_paths, \n",
        "    schema=problems_schema, \n",
        "    parse_dates_list=problems_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT \n",
        ")\n",
        "if problems_df is not None:\n",
        "    print(f\"Problems DataFrame shape: {problems_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Performance Data (exp_alogs)...\")\n",
        "performance_df = combine_polars_csvs(\n",
        "    performance_file_paths, \n",
        "    schema=performance_schema, \n",
        "    parse_dates_list=performance_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT\n",
        ")\n",
        "if performance_df is not None:\n",
        "    print(f\"Performance DataFrame shape: {performance_df.shape}\")\n",
        "\n",
        "print(\"\\nCombining Metrics Data (priors)...\")\n",
        "if 'teacher id' in metrics_schema: \n",
        "    metrics_schema_corrected = metrics_schema.copy()\n",
        "    metrics_schema_corrected['teacher_id'] = metrics_schema_corrected.pop('teacher id')\n",
        "else:\n",
        "    metrics_schema_corrected = metrics_schema\n",
        "\n",
        "metrics_df = combine_polars_csvs(\n",
        "    metrics_file_paths, \n",
        "    schema=metrics_schema_corrected, \n",
        "    parse_dates_list=metrics_parse_dates,\n",
        "    known_date_format_str=COMMON_DATETIME_FORMAT\n",
        ")\n",
        "if metrics_df is not None:\n",
        "    if 'teacher id' in metrics_df.columns: \n",
        "        metrics_df = metrics_df.rename({'teacher id': 'teacher_id'})\n",
        "    print(f\"Metrics DataFrame shape: {metrics_df.shape}\")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Merge Operations ---\n",
            "Starting with actions_df: (3708299, 9)\n",
            "Merging problems_df...\n",
            "After merging problems_df: (3708299, 28)\n",
            "Merging performance_df...\n",
            "After merging performance_df: (3711215, 61)\n",
            "Merging metrics_df...\n",
            "After merging metrics_df: (3711215, 90)\n",
            "\n",
            "--- Merge Complete ---\n",
            "Final Merged DataFrame Info:\n",
            "Shape: (3711215, 90)\n",
            "Columns in merged_df: ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id', 'experiment_tag_path', 'action', 'timestamp', 'assistments_reference_action_log_id', 'problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task', 'first_response_or_request_time', 'first_answer', 'correct', 'reported_score', 'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given', 'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available', 'explanation_given', 'answer_given', 'assistments_reference_problem_log_id', 'release_date', 'due_date', 'start_time_perf', 'end_time_perf', 'assignment_session_count', 'pretest_problem_count', 'pretest_correct', 'pretest_time_on_task', 'pretest_average_first_response_time', 'pretest_session_count', 'assigned_condition', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_problem_count', 'condition_total_correct', 'condition_total_correct_after_wrong_response', 'condition_total_correct_after_tutoring', 'condition_total_answers_before_tutoring', 'condition_total_attempt_count', 'condition_total_hints_available', 'condition_total_hints_given', 'condition_total_scaffold_problems_available', 'condition_total_scaffold_problems_given', 'condition_total_explanations_available', 'condition_total_explanations_given', 'condition_total_answers_given', 'condition_session_count', 'posttest_problem_count', 'posttest_correct', 'posttest_time_on_task', 'posttest_average_first_response_time', 'posttest_session_count', 'assistments_reference_assignment_log_id', 'student_prior_started_skill_builder_count', 'student_prior_completed_skill_builder_count', 'student_prior_started_problem_set_count', 'student_prior_completed_problem_set_count', 'student_prior_completed_problem_count', 'student_prior_median_first_response_time', 'student_prior_median_time_on_task', 'student_prior_average_correctness', 'student_prior_average_attempt_count', 'class_id', 'class_creation_date', 'class_student_count', 'class_prior_skill_builder_count', 'class_prior_problem_set_count', 'class_prior_skill_builder_percent_started', 'class_prior_skill_builder_percent_completed', 'class_prior_problem_set_percent_started', 'class_prior_problem_set_percent_completed', 'class_prior_completed_problem_count', 'class_prior_median_time_on_task', 'class_prior_median_first_response_time', 'class_prior_average_correctness', 'class_prior_average_attempt_count', 'teacher_account_creation_date', 'district_id', 'location', 'opportunity_zone', 'locale_description', 'teacher_id']\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'problem_id': Categorical(ordering='physical'), 'problem_part': Categorical(ordering='physical'), 'scaffold_id': Categorical(ordering='physical'), 'experiment_tag_path': String, 'action': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'assistments_reference_action_log_id': UInt64, 'problem_condition': Categorical(ordering='physical'), 'start_time': Datetime(time_unit='us', time_zone='UTC'), 'end_time': Datetime(time_unit='us', time_zone='UTC'), 'session_count': UInt16, 'time_on_task': Float32, 'first_response_or_request_time': Float32, 'first_answer': String, 'correct': Boolean, 'reported_score': Float32, 'answer_before_tutoring': Boolean, 'attempt_count': UInt16, 'hints_available': UInt16, 'hints_given': UInt16, 'scaffold_problems_available': UInt16, 'scaffold_problems_given': UInt16, 'explanation_available': Boolean, 'explanation_given': Boolean, 'answer_given': Boolean, 'assistments_reference_problem_log_id': UInt64, 'release_date': Datetime(time_unit='us', time_zone='UTC'), 'due_date': Datetime(time_unit='us', time_zone='UTC'), 'start_time_perf': Datetime(time_unit='us', time_zone='UTC'), 'end_time_perf': Datetime(time_unit='us', time_zone='UTC'), 'assignment_session_count': Float32, 'pretest_problem_count': Float32, 'pretest_correct': Float32, 'pretest_time_on_task': Float32, 'pretest_average_first_response_time': Float32, 'pretest_session_count': Float32, 'assigned_condition': Categorical(ordering='physical'), 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_problem_count': Float32, 'condition_total_correct': Float32, 'condition_total_correct_after_wrong_response': Float32, 'condition_total_correct_after_tutoring': Float32, 'condition_total_answers_before_tutoring': Float32, 'condition_total_attempt_count': Float32, 'condition_total_hints_available': Float32, 'condition_total_hints_given': Float32, 'condition_total_scaffold_problems_available': Float32, 'condition_total_scaffold_problems_given': Float32, 'condition_total_explanations_available': Float32, 'condition_total_explanations_given': Float32, 'condition_total_answers_given': Float32, 'condition_session_count': Float32, 'posttest_problem_count': Float32, 'posttest_correct': Float32, 'posttest_time_on_task': Float32, 'posttest_average_first_response_time': Float32, 'posttest_session_count': Float32, 'assistments_reference_assignment_log_id': UInt64, 'student_prior_started_skill_builder_count': UInt32, 'student_prior_completed_skill_builder_count': UInt32, 'student_prior_started_problem_set_count': UInt32, 'student_prior_completed_problem_set_count': UInt32, 'student_prior_completed_problem_count': UInt32, 'student_prior_median_first_response_time': Float32, 'student_prior_median_time_on_task': Float32, 'student_prior_average_correctness': Float32, 'student_prior_average_attempt_count': Float32, 'class_id': Categorical(ordering='physical'), 'class_creation_date': Datetime(time_unit='us', time_zone='UTC'), 'class_student_count': UInt16, 'class_prior_skill_builder_count': UInt32, 'class_prior_problem_set_count': UInt32, 'class_prior_skill_builder_percent_started': Float32, 'class_prior_skill_builder_percent_completed': Float32, 'class_prior_problem_set_percent_started': Float32, 'class_prior_problem_set_percent_completed': Float32, 'class_prior_completed_problem_count': UInt32, 'class_prior_median_time_on_task': Float32, 'class_prior_median_first_response_time': Float32, 'class_prior_average_correctness': Float32, 'class_prior_average_attempt_count': Float32, 'teacher_account_creation_date': Datetime(time_unit='us', time_zone='UTC'), 'district_id': Categorical(ordering='physical'), 'location': Categorical(ordering='physical'), 'opportunity_zone': Categorical(ordering='physical'), 'locale_description': Categorical(ordering='physical'), 'teacher_id': Categorical(ordering='physical')})\n"
          ]
        }
      ],
      "source": [
        "# Merge DataFrames into One\n",
        "\n",
        "merged_df = None\n",
        "merge_successful = True\n",
        "\n",
        "print(\"\\n--- Starting Merge Operations ---\")\n",
        "\n",
        "if actions_df is None or actions_df.is_empty():\n",
        "    print(\"Actions DataFrame is empty or None. Cannot proceed with merge.\")\n",
        "    merge_successful = False\n",
        "else:\n",
        "    merged_df = actions_df.clone()\n",
        "    print(f\"Starting with actions_df: {merged_df.shape}\")\n",
        "\n",
        "    # Merge Problems Data\n",
        "    if problems_df is not None and not problems_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging problems_df...\")\n",
        "            problem_keys = ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id']\n",
        "            merged_df = merged_df.join(problems_df, on=problem_keys, how=\"left\", suffix=\"_problem\")\n",
        "            print(f\"After merging problems_df: {merged_df.shape}\")\n",
        "            del problems_df \n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging problems_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful: \n",
        "        print(\"Skipping problems_df merge (not loaded or empty).\")\n",
        "\n",
        "    # Merge Performance Data\n",
        "    if performance_df is not None and not performance_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging performance_df...\")\n",
        "            perf_keys = ['experiment_id', 'student_id']\n",
        "            merged_df = merged_df.join(performance_df, on=perf_keys, how=\"left\", suffix=\"_perf\")\n",
        "            print(f\"After merging performance_df: {merged_df.shape}\")\n",
        "            del performance_df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging performance_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful:\n",
        "        print(\"Skipping performance_df merge (not loaded or empty).\")\n",
        "\n",
        "    # Merge Metrics Data\n",
        "    if metrics_df is not None and not metrics_df.is_empty() and merge_successful:\n",
        "        try:\n",
        "            print(\"Merging metrics_df...\")\n",
        "            metrics_keys = ['experiment_id', 'student_id']\n",
        "            merged_df = merged_df.join(metrics_df, on=metrics_keys, how=\"left\", suffix=\"_metrics\")\n",
        "            print(f\"After merging metrics_df: {merged_df.shape}\")\n",
        "            del metrics_df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging metrics_df: {e}\")\n",
        "            merge_successful = False\n",
        "    elif merge_successful:\n",
        "        print(\"Skipping metrics_df merge (not loaded or empty).\")\n",
        "\n",
        "    if merged_df is not None and merge_successful:\n",
        "        print(\"\\n--- Merge Complete ---\")\n",
        "        print(\"Final Merged DataFrame Info:\")\n",
        "        print(f\"Shape: {merged_df.shape}\")\n",
        "        print(\"Columns in merged_df:\", merged_df.columns)\n",
        "        print(merged_df.head)\n",
        "        print(merged_df.schema)\n",
        "        \n",
        "        if 'actions_df' in locals() and actions_df is not merged_df: \n",
        "            del actions_df\n",
        "            gc.collect()\n",
        "            \n",
        "    elif merged_df is not None: \n",
        "        print(\"\\n--- Merge Partially Complete or Some DataFrames Skipped ---\")\n",
        "        print(\"Columns in partially merged_df:\", merged_df.columns)\n",
        "    else: \n",
        "        print(\"\\n--- Merge Failed or Base DataFrame (actions_df) was not suitable ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8NxlbW3ynlT"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Data Cleaning ---\n",
            "Initial merged_df shape for cleaning: (3711215, 90)\n",
            "\n",
            "--- Renaming Columns ---\n",
            "Applying renames: {'assistments_reference_action_log_id': 'action_log_id', 'start_time_perf': 'assignment_start_time', 'end_time_perf': 'assignment_end_time', 'assistments_reference_assignment_log_id': 'assignment_log_id'}\n",
            "Scheduled for categorical conversion: experiment_tag_path\n",
            "Scheduled 'assignment_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'pretest_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct_after_wrong_response' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_correct_after_tutoring' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_answers_before_tutoring' for Float32 to UInt16 conversion.\n",
            "Scheduled 'condition_total_attempt_count' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_hints_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_hints_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_scaffold_problems_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_scaffold_problems_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_explanations_available' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_explanations_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_total_answers_given' for Float32 to UInt32 conversion.\n",
            "Scheduled 'condition_session_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_problem_count' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_correct' for Float32 to UInt16 conversion.\n",
            "Scheduled 'posttest_session_count' for Float32 to UInt16 conversion.\n",
            "\n",
            "Applying column type transformations...\n",
            "Type transformations applied.\n",
            "\n",
            "--- Specific Value Cleaning ---\n",
            "Scheduled 'opportunity_zone' to boolean 'opportunity_zone_bool' conversion.\n",
            "Scheduled fill_null for categorical district_id with 'Unknown_District'.\n",
            "Scheduled fill_null for categorical location with 'Unknown_Location'.\n",
            "Scheduled fill_null for categorical locale_description with 'Unknown_Locale'.\n",
            "\n",
            "Applying specific value cleaning expressions...\n",
            "Specific value cleaning applied.\n",
            "\n",
            "Dropping fully empty columns: ['problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task', 'first_response_or_request_time', 'first_answer', 'correct', 'reported_score', 'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given', 'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available', 'explanation_given', 'answer_given', 'assistments_reference_problem_log_id']\n",
            "Dropping original opportunity zone column: 'opportunity_zone'\n",
            "\n",
            "Shape after Cleaning: (3711215, 71)\n",
            "Columns after cleaning: ['experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id', 'experiment_tag_path', 'action', 'timestamp', 'action_log_id', 'release_date', 'due_date', 'assignment_start_time', 'assignment_end_time', 'assignment_session_count', 'pretest_problem_count', 'pretest_correct', 'pretest_time_on_task', 'pretest_average_first_response_time', 'pretest_session_count', 'assigned_condition', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_problem_count', 'condition_total_correct', 'condition_total_correct_after_wrong_response', 'condition_total_correct_after_tutoring', 'condition_total_answers_before_tutoring', 'condition_total_attempt_count', 'condition_total_hints_available', 'condition_total_hints_given', 'condition_total_scaffold_problems_available', 'condition_total_scaffold_problems_given', 'condition_total_explanations_available', 'condition_total_explanations_given', 'condition_total_answers_given', 'condition_session_count', 'posttest_problem_count', 'posttest_correct', 'posttest_time_on_task', 'posttest_average_first_response_time', 'posttest_session_count', 'assignment_log_id', 'student_prior_started_skill_builder_count', 'student_prior_completed_skill_builder_count', 'student_prior_started_problem_set_count', 'student_prior_completed_problem_set_count', 'student_prior_completed_problem_count', 'student_prior_median_first_response_time', 'student_prior_median_time_on_task', 'student_prior_average_correctness', 'student_prior_average_attempt_count', 'class_id', 'class_creation_date', 'class_student_count', 'class_prior_skill_builder_count', 'class_prior_problem_set_count', 'class_prior_skill_builder_percent_started', 'class_prior_skill_builder_percent_completed', 'class_prior_problem_set_percent_started', 'class_prior_problem_set_percent_completed', 'class_prior_completed_problem_count', 'class_prior_median_time_on_task', 'class_prior_median_first_response_time', 'class_prior_average_correctness', 'class_prior_average_attempt_count', 'teacher_account_creation_date', 'district_id', 'location', 'locale_description', 'teacher_id', 'opportunity_zone_bool']\n"
          ]
        }
      ],
      "source": [
        "# Data Cleaning\n",
        "\n",
        "if 'merged_df' in locals() and merged_df is not None and merge_successful: \n",
        "    print(\"\\n--- Starting Data Cleaning ---\")\n",
        "    print(f\"Initial merged_df shape for cleaning: {merged_df.shape}\")\n",
        "\n",
        "    print(\"\\n--- Renaming Columns ---\")\n",
        "    rename_map = {\n",
        "        'assistments_reference_action_log_id': 'action_log_id',\n",
        "        'start_time_perf': 'assignment_start_time',\n",
        "        'end_time_perf': 'assignment_end_time',\n",
        "        'assistments_reference_assignment_log_id': 'assignment_log_id'\n",
        "    }\n",
        "    actual_renames = {k: v for k, v in rename_map.items() if k in merged_df.columns}\n",
        "    if actual_renames:\n",
        "        print(f\"Applying renames: {actual_renames}\")\n",
        "        merged_df = merged_df.rename(actual_renames)\n",
        "    else:\n",
        "        print(\"No columns matched for renaming based on the current rename_map.\")\n",
        "\n",
        "    column_transformations = []\n",
        "\n",
        "    datetime_cols_final_check = [\n",
        "        'timestamp', 'start_time', 'end_time', 'release_date', 'due_date',\n",
        "        'assignment_start_time', 'assignment_end_time',\n",
        "        'class_creation_date', 'teacher_account_creation_date'\n",
        "    ]\n",
        "    for col_name in datetime_cols_final_check:\n",
        "        if col_name in merged_df.columns:\n",
        "            current_dtype = merged_df[col_name].dtype\n",
        "            if current_dtype == pl.Utf8:\n",
        "                print(f\"Scheduled for datetime re-parsing (UTF8 found): {col_name}\")\n",
        "                column_transformations.append(\n",
        "                    pl.col(col_name).str.to_datetime(format=COMMON_DATETIME_FORMAT, strict=False, time_unit='us')\n",
        "                    .dt.convert_time_zone(\"UTC\")\n",
        "                    .alias(col_name)\n",
        "                )\n",
        "            elif isinstance(current_dtype, pl.Datetime):\n",
        "                current_tz = current_dtype.time_zone\n",
        "                if current_tz is None:\n",
        "                    print(f\"Info: Datetime column '{col_name}' is naive. Localizing to UTC.\")\n",
        "                    column_transformations.append(\n",
        "                        pl.col(col_name).dt.replace_time_zone(\"UTC\", ambiguous='earliest').alias(col_name)\n",
        "                    )\n",
        "                elif current_tz != \"UTC\":\n",
        "                    print(f\"Scheduled for UTC conversion (already datetime, was '{current_tz}'): {col_name}\")\n",
        "                    column_transformations.append(\n",
        "                        pl.col(col_name).dt.convert_time_zone(\"UTC\").alias(col_name)\n",
        "                    )\n",
        "\n",
        "    cols_to_category_polars = [\n",
        "        'experiment_id', 'student_id', 'problem_id', 'problem_part', 'scaffold_id',\n",
        "        'experiment_tag_path', 'action', 'problem_condition', 'assigned_condition',\n",
        "        'class_id', 'district_id', 'location', 'opportunity_zone',\n",
        "        'locale_description', 'teacher_id'\n",
        "    ]\n",
        "    for col_name in cols_to_category_polars:\n",
        "        if col_name in merged_df.columns and merged_df[col_name].dtype != pl.Categorical:\n",
        "             column_transformations.append(pl.col(col_name).cast(pl.Categorical).alias(col_name))\n",
        "             print(f\"Scheduled for categorical conversion: {col_name}\")\n",
        "\n",
        "    float_to_int_casts = {\n",
        "        'assignment_session_count': pl.UInt16, 'pretest_problem_count': pl.UInt16,\n",
        "        'pretest_correct': pl.UInt16, 'pretest_session_count': pl.UInt16,\n",
        "        'condition_problem_count': pl.UInt16, 'condition_total_correct': pl.UInt16,\n",
        "        'condition_total_correct_after_wrong_response': pl.UInt16,\n",
        "        'condition_total_correct_after_tutoring': pl.UInt16,\n",
        "        'condition_total_answers_before_tutoring': pl.UInt16,\n",
        "        'condition_total_attempt_count': pl.UInt32,\n",
        "        'condition_total_hints_available': pl.UInt32, 'condition_total_hints_given': pl.UInt32,\n",
        "        'condition_total_scaffold_problems_available': pl.UInt32,\n",
        "        'condition_total_scaffold_problems_given': pl.UInt32,\n",
        "        'condition_total_explanations_available': pl.UInt32,\n",
        "        'condition_total_explanations_given': pl.UInt32,\n",
        "        'condition_total_answers_given': pl.UInt32,\n",
        "        'condition_session_count': pl.UInt16, 'posttest_problem_count': pl.UInt16,\n",
        "        'posttest_correct': pl.UInt16, 'posttest_session_count': pl.UInt16,\n",
        "    }\n",
        "    for col_name, target_int_type in float_to_int_casts.items():\n",
        "        if col_name in merged_df.columns:\n",
        "            if merged_df[col_name].dtype == pl.Float32:\n",
        "                column_transformations.append(\n",
        "                    pl.col(col_name)\n",
        "                      .fill_null(0)\n",
        "                      .cast(target_int_type, strict=False)\n",
        "                      .alias(col_name)\n",
        "                )\n",
        "                print(f\"Scheduled '{col_name}' for Float32 to {target_int_type} conversion.\")\n",
        "            elif merged_df[col_name].dtype != target_int_type:\n",
        "                print(f\"Warning: Column '{col_name}' was expected to be Float32 for int conversion, but found {merged_df[col_name].dtype}. Skipping specific int cast.\")\n",
        "\n",
        "    # General Float64 to Float32 pass\n",
        "    float64_cols = [col_name for col_name, dtype in merged_df.schema.items() if dtype == pl.Float64]\n",
        "    for col_name in float64_cols:\n",
        "        if col_name in merged_df.columns:\n",
        "            column_transformations.append(pl.col(col_name).cast(pl.Float32).alias(col_name))\n",
        "            print(f\"Scheduled for Float64 to Float32 conversion: {col_name}\")\n",
        "\n",
        "    if column_transformations:\n",
        "        print(\"\\nApplying column type transformations...\")\n",
        "        merged_df = merged_df.with_columns(column_transformations)\n",
        "        print(\"Type transformations applied.\")\n",
        "\n",
        "    print(\"\\n--- Specific Value Cleaning ---\")\n",
        "    specific_value_cleaning_expressions = []\n",
        "\n",
        "    if 'opportunity_zone' in merged_df.columns:\n",
        "        if merged_df['opportunity_zone'].dtype != pl.Categorical:\n",
        "             merged_df = merged_df.with_columns(pl.col('opportunity_zone').cast(pl.Categorical))\n",
        "        specific_value_cleaning_expressions.append(\n",
        "            pl.when(pl.col('opportunity_zone').cast(pl.Utf8) == \"Yes\").then(True)\n",
        "              .when(pl.col('opportunity_zone').cast(pl.Utf8) == \"No\").then(False)\n",
        "              .otherwise(None)\n",
        "              .cast(pl.Boolean)\n",
        "              .alias('opportunity_zone_bool')\n",
        "        )\n",
        "        print(\"Scheduled 'opportunity_zone' to boolean 'opportunity_zone_bool' conversion.\")\n",
        "\n",
        "    cat_cols_to_fill_info = {\n",
        "        'district_id': 'Unknown_District',\n",
        "        'location': 'Unknown_Location',\n",
        "        'locale_description': 'Unknown_Locale'\n",
        "    }\n",
        "    for col_name, fill_val in cat_cols_to_fill_info.items():\n",
        "        if col_name in merged_df.columns:\n",
        "            if merged_df[col_name].dtype != pl.Categorical:\n",
        "                merged_df = merged_df.with_columns(pl.col(col_name).cast(pl.Categorical))\n",
        "                print(f\"Casted '{col_name}' to Categorical before fill_null.\")\n",
        "            specific_value_cleaning_expressions.append(pl.col(col_name).fill_null(pl.lit(fill_val).cast(pl.Categorical)).alias(col_name))\n",
        "            print(f\"Scheduled fill_null for categorical {col_name} with '{fill_val}'.\")\n",
        "\n",
        "    if specific_value_cleaning_expressions:\n",
        "        print(\"\\nApplying specific value cleaning expressions...\")\n",
        "        merged_df = merged_df.with_columns(specific_value_cleaning_expressions)\n",
        "        print(\"Specific value cleaning applied.\")\n",
        "\n",
        "    pandas_identified_empty_cols = [\n",
        "         'problem_condition', 'start_time', 'end_time', 'session_count', 'time_on_task',\n",
        "         'first_response_or_request_time', 'first_answer', 'correct', 'reported_score',\n",
        "         'answer_before_tutoring', 'attempt_count', 'hints_available', 'hints_given',\n",
        "         'scaffold_problems_available', 'scaffold_problems_given', 'explanation_available',\n",
        "         'explanation_given', 'answer_given',\n",
        "         'assistments_reference_problem_log_id'\n",
        "    ]\n",
        "    actual_empty_cols_to_drop = []\n",
        "    if not merged_df.is_empty():\n",
        "        for col_name in pandas_identified_empty_cols:\n",
        "            if col_name in merged_df.columns and merged_df[col_name].is_null().all():\n",
        "                actual_empty_cols_to_drop.append(col_name)\n",
        "            elif col_name in merged_df.columns:\n",
        "                null_count = merged_df[col_name].is_null().sum()\n",
        "                if null_count > 0 :\n",
        "                    print(f\"Info: Column '{col_name}' (candidate for empty drop) was not fully null. Nulls: {null_count}/{merged_df.height}\")\n",
        "\n",
        "    if actual_empty_cols_to_drop:\n",
        "        print(f\"\\nDropping fully empty columns: {actual_empty_cols_to_drop}\")\n",
        "        merged_df = merged_df.drop(actual_empty_cols_to_drop)\n",
        "    else:\n",
        "        print(\"\\nNo fully empty columns (from the predefined list) identified for dropping.\")\n",
        "\n",
        "    if 'opportunity_zone' in merged_df.columns and 'opportunity_zone_bool' in merged_df.columns:\n",
        "        print(\"Dropping original opportunity zone column: 'opportunity_zone'\")\n",
        "        merged_df = merged_df.drop('opportunity_zone')\n",
        "\n",
        "    print(f\"\\nShape after Cleaning: {merged_df.shape}\")\n",
        "    print(\"Columns after cleaning:\", merged_df.columns)\n",
        "    gc.collect()\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Cell 7 cleaning: merged_df not available, previous merge failed, or merge_successful flag is False.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Reducing DataFrame to Essential Columns ---\n",
            "Attempting to select these 18 essential columns: ['experiment_id', 'student_id', 'timestamp', 'action', 'action_log_id', 'assignment_start_time', 'assignment_end_time', 'assignment_log_id', 'assignment_session_count', 'condition_problem_count', 'condition_time_on_task', 'condition_average_first_response_or_request_time', 'condition_total_correct', 'condition_total_attempt_count', 'condition_total_hints_given', 'condition_total_explanations_given', 'student_prior_average_correctness', 'opportunity_zone_bool']\n",
            "\n",
            "Reduced DataFrame Info: Shape (3711215, 18)\n",
            "shape: (5, 18)\n",
            "\n",
            " experimen  student_i  timestamp  action       condition  condition  student_p  opportun \n",
            " t_id       d          ---        ---           _total_hi  _total_ex  rior_aver  ity_zone \n",
            " ---        ---        datetime[  cat           nts_given  planation  age_corre  _bool    \n",
            " cat        cat        s, UTC]                 ---        s_g       ctn       ---      \n",
            "                                                u32        ---        ---        bool     \n",
            "                                                           u32        f32                 \n",
            "\n",
            " PSAU85Y    10408      2021-03-2  assignmen    1          0          0.738739   null     \n",
            "                       4 16:41:4  t_started                                               \n",
            "                       6.393 UTC                                                          \n",
            " PSAU85Y    10408      2021-03-2  problem_s    1          0          0.738739   null     \n",
            "                       4 16:41:4  tarted                                                  \n",
            "                       7.437 UTC                                                          \n",
            " PSAU85Y    10408      2021-03-2  correct_r    1          0          0.738739   null     \n",
            "                       4 16:42:0  esponse                                                 \n",
            "                       3.665 UTC                                                          \n",
            " PSAU85Y    10408      2021-03-2  problem_f    1          0          0.738739   null     \n",
            "                       4 16:42:0  inished                                                 \n",
            "                       3.675 UTC                                                          \n",
            " PSAU85Y    10408      2021-03-2  continue_    1          0          0.738739   null     \n",
            "                       4 16:42:0  selected                                                \n",
            "                       5.295 UTC                                                          \n",
            "\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'action': Categorical(ordering='physical'), 'action_log_id': UInt64, 'assignment_start_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_end_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_log_id': UInt64, 'assignment_session_count': UInt16, 'condition_problem_count': UInt16, 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_total_correct': UInt16, 'condition_total_attempt_count': UInt32, 'condition_total_hints_given': UInt32, 'condition_total_explanations_given': UInt32, 'student_prior_average_correctness': Float32, 'opportunity_zone_bool': Boolean})\n",
            "\n",
            "Attempting to save cleaned and reduced Polars DataFrame to: /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "Successfully saved to /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "\n",
            "Full cleaned merged_df deleted from memory.\n"
          ]
        }
      ],
      "source": [
        "# Create Reduced DataFrame and Save\n",
        "\n",
        "if 'merged_df' in locals() and merged_df is not None and merge_successful:\n",
        "    print(\"\\n--- Reducing DataFrame to Essential Columns ---\")\n",
        "\n",
        "    essential_cols_to_keep_polars = [\n",
        "        # Keys / Base Info from actions_df\n",
        "        'experiment_id', \n",
        "        'student_id', \n",
        "        'timestamp', \n",
        "        'action', \n",
        "        'action_log_id',\n",
        "        \n",
        "        # From performance_df \n",
        "        'assignment_start_time', \n",
        "        'assignment_end_time',   \n",
        "        'assignment_log_id',     \n",
        "        'assignment_session_count', \n",
        "        'condition_problem_count',  \n",
        "        'condition_time_on_task',   \n",
        "        'condition_average_first_response_or_request_time', \n",
        "        'condition_total_correct',  \n",
        "        'condition_total_attempt_count', \n",
        "        'condition_total_hints_given', \n",
        "        'condition_total_explanations_given', \n",
        "        \n",
        "        # From metrics_df \n",
        "        'student_prior_average_correctness', \n",
        "        \n",
        "        'opportunity_zone_bool', \n",
        "    ]\n",
        "    \n",
        "    # Filter to only include columns that actually exist in the cleaned merged_df\n",
        "    final_essential_columns = [col for col in essential_cols_to_keep_polars if col in merged_df.columns]\n",
        "    \n",
        "    print(f\"Attempting to select these {len(final_essential_columns)} essential columns: {final_essential_columns}\")\n",
        "    missing_essentials_for_reduction = [col for col in essential_cols_to_keep_polars if col not in final_essential_columns]\n",
        "\n",
        "    if missing_essentials_for_reduction:\n",
        "        print(f\"Warning: The following conceptual essential columns were NOT FOUND in merged_df for reduction: {missing_essentials_for_reduction}\")\n",
        "        print(\"Please ensure their names are correct in the 'essential_cols_to_keep_polars' list and they exist in the output of Cell 7.\")\n",
        "    \n",
        "    if not final_essential_columns:\n",
        "        print(\"Error: No essential columns available for selection based on your list. Cannot create reduced DataFrame.\")\n",
        "        merged_df_reduced = None\n",
        "    else:\n",
        "        try:\n",
        "            merged_df_reduced = merged_df.select(final_essential_columns)\n",
        "            print(f\"\\nReduced DataFrame Info: Shape {merged_df_reduced.shape}\")\n",
        "            print(merged_df_reduced.head())\n",
        "            print(merged_df_reduced.schema)\n",
        "\n",
        "            print(f\"\\nAttempting to save cleaned and reduced DataFrame to: {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "            merged_df_reduced.write_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET) \n",
        "            print(f\"Successfully saved to {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error during final select or save: {e}\")\n",
        "            merged_df_reduced = None\n",
        "            \n",
        "    if 'merged_df' in locals(): \n",
        "        del merged_df \n",
        "        gc.collect()\n",
        "        print(\"\\nFull cleaned merged_df deleted from memory.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Cell 8 (reduction and save): merged_df not available from Cell 7 or previous steps failed.\")\n",
        "    merged_df_reduced = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading Cleaned Parquet File ---\n",
            "Successfully reloaded: /Users/john/Downloads/osfstorage-archive/merged_experiment_data_cleaned_polars.parquet\n",
            "Reloaded DataFrame Shape: (3711215, 18)\n",
            "\n",
            "Reloaded DataFrame Head (from Parquet):\n",
            "shape: (5, 18)\n",
            "\n",
            " experimen  student_i  timestamp  action       condition  condition  student_p  opportun \n",
            " t_id       d          ---        ---           _total_hi  _total_ex  rior_aver  ity_zone \n",
            " ---        ---        datetime[  cat           nts_given  planation  age_corre  _bool    \n",
            " cat        cat        s, UTC]                 ---        s_g       ctn       ---      \n",
            "                                                u32        ---        ---        bool     \n",
            "                                                           u32        f32                 \n",
            "\n",
            " PSAU85Y    10408      2021-03-2  assignmen    1          0          0.738739   null     \n",
            "                       4 16:41:4  t_started                                               \n",
            "                       6.393 UTC                                                          \n",
            " PSAU85Y    10408      2021-03-2  problem_s    1          0          0.738739   null     \n",
            "                       4 16:41:4  tarted                                                  \n",
            "                       7.437 UTC                                                          \n",
            " PSAU85Y    10408      2021-03-2  correct_r    1          0          0.738739   null     \n",
            "                       4 16:42:0  esponse                                                 \n",
            "                       3.665 UTC                                                          \n",
            " PSAU85Y    10408      2021-03-2  problem_f    1          0          0.738739   null     \n",
            "                       4 16:42:0  inished                                                 \n",
            "                       3.675 UTC                                                          \n",
            " PSAU85Y    10408      2021-03-2  continue_    1          0          0.738739   null     \n",
            "                       4 16:42:0  selected                                                \n",
            "                       5.295 UTC                                                          \n",
            "\n",
            "\n",
            "Reloaded DataFrame Schema (from Parquet):\n",
            "Schema({'experiment_id': Categorical(ordering='physical'), 'student_id': Categorical(ordering='physical'), 'timestamp': Datetime(time_unit='us', time_zone='UTC'), 'action': Categorical(ordering='physical'), 'action_log_id': UInt64, 'assignment_start_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_end_time': Datetime(time_unit='us', time_zone='UTC'), 'assignment_log_id': UInt64, 'assignment_session_count': UInt16, 'condition_problem_count': UInt16, 'condition_time_on_task': Float32, 'condition_average_first_response_or_request_time': Float32, 'condition_total_correct': UInt16, 'condition_total_attempt_count': UInt32, 'condition_total_hints_given': UInt32, 'condition_total_explanations_given': UInt32, 'student_prior_average_correctness': Float32, 'opportunity_zone_bool': Boolean})\n"
          ]
        }
      ],
      "source": [
        "# Load Cleaned Data\n",
        "\n",
        "if 'merged_df_reduced' in locals() and merged_df_reduced is not None and not merged_df_reduced.is_empty() and SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "    print(f\"\\n--- Loading Cleaned Parquet File ---\")\n",
        "    try:\n",
        "        df_reloaded_polars = pl.read_parquet(SAVE_CLEANED_PATH_POLARS_PARQUET)\n",
        "        print(f\"Successfully reloaded: {SAVE_CLEANED_PATH_POLARS_PARQUET}\")\n",
        "        print(f\"Reloaded DataFrame Shape: {df_reloaded_polars.shape}\")\n",
        "        print(\"\\nReloaded DataFrame Head (from Parquet):\")\n",
        "        print(df_reloaded_polars.head())\n",
        "        print(\"\\nReloaded DataFrame Schema (from Parquet):\")\n",
        "        print(df_reloaded_polars.schema)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reloading the cleaned Parquet file: {e}\")\n",
        "elif SAVE_CLEANED_PATH_POLARS_PARQUET.exists():\n",
        "     print(f\"Cleaned Parquet file found at {SAVE_CLEANED_PATH_POLARS_PARQUET}, but merged_df_reduced may not have been successfully created or was empty in the previous step (this script might have been re-run starting from here). Consider reloading manually if needed.\")\n",
        "else:\n",
        "    print(f\"\\nCleaned Parquet file not found at {SAVE_CLEANED_PATH_POLARS_PARQUET} or reduction/save step failed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
